<!-- Chunk 916: bytes 3243841-3315783, type=function -->
def test_capability():
    # Test code here
    assert expected_behavior
```

## See Also

- [Related Capability](capability-name.md)
- [Parent SKILL.md](../SKILL.md)

```

### Examples Format

Each example should be in its own directory under `examples/`:

```

examples/
‚îú‚îÄ‚îÄ 01-example-name/
‚îÇ ‚îú‚îÄ‚îÄ README.md # Explanation of the example
‚îÇ ‚îú‚îÄ‚îÄ example.py # The example code
‚îÇ ‚îî‚îÄ‚îÄ test_example.py # Test for the example
‚îú‚îÄ‚îÄ 02-another-example/
‚îÇ ‚îú‚îÄ‚îÄ README.md
‚îÇ ‚îú‚îÄ‚îÄ example.py
‚îÇ ‚îî‚îÄ‚îÄ test_example.py

````

**README.md format:**
```markdown
# Example Name

## What This Demonstrates
Brief description of what this example shows.

## Running the Example
```bash
# Commands to run the example
python example.py
````

## Key Concepts

- Concept 1
- Concept 2
- Concept 3

## Expected Output

```
# What you should see when running it
```

## See Also

- [Related Capability](../../references/capability-name.md)

````

**Numbering:** Use `01-`, `02-`, etc. for ordering.

**Self-contained:** Each example should run independently without needing other examples.

---

## 5. Content Guidelines

### Documentation Style

#### Problem ‚Üí Solution Format

Structure documentation to match how developers think:

1. **State the problem** - What issue are we solving?
2. **Show the solution** - How do we solve it?
3. **Provide an example** - Concrete code or usage
4. **Explain why** - Why this solution works

**Example:**
```markdown
## Connection Pool Exhaustion

**Problem:** Under load, applications run out of database connections and fail with "too many connections" errors.

**Solution:** Create the database engine in the lifespan context manager with proper pool parameters.

**Example:**
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    engine = create_async_engine(
        DATABASE_URL,
        pool_size=10,
        max_overflow=20,
    )
    yield
    await engine.dispose()
````

**Why this works:** The engine is created once at startup, properly configured with pool limits, and disposed of at shutdown, preventing connection leaks.

````

#### ‚ùå/‚úÖ Patterns

Show both wrong and right approaches:

```markdown
### ‚ùå Common Mistake
```python
# Engine created at import time - never closes!
engine = create_async_engine(DATABASE_URL)
````

### ‚úÖ Production Pattern

```python
# Engine created in lifespan, properly disposed
@asynccontextmanager
async def lifespan(app: FastAPI):
    engine = create_async_engine(DATABASE_URL)
    yield
    await engine.dispose()
```

````

**Why?** Visual distinction makes patterns memorable. Shows what to avoid AND what to do.

#### Quick Reference Tables

Include tables for fast lookup:

```markdown
## Quick Reference

| Problem | Solution | Keywords |
|---------|----------|----------|
| Connections not closing | Use `lifespan` with `engine.dispose()` | connection leak, pool |
| Pool exhaustion | Set `pool_size` and `max_overflow` | concurrent, load |
| Tests fail together | Use async fixtures with rollback | isolation, flaky |
````

**Why?** Enables quick scanning and problem-solving.

#### Code Examples

Follow these guidelines for code examples:

1. **Include type hints:**

```python
 async def get_user(user_id: int) -> Optional[User]:
```

2. **Add explanatory comments:**

```python
 # CRITICAL: Only update provided fields
 update_data = update.model_dump(exclude_unset=True)
```

3. **Show imports:**

```python
 from typing import Optional
 from pydantic import BaseModel
```

4. **Use meaningful variable names:**

```python
 # Good
 async_connection_pool = create_async_engine(url)

 # Bad
 x = create_async_engine(url)
```

5. **Handle errors:**

```python
 try:
     result = await operation()
 except SpecificError as e:
     logger.error(f"Operation failed: {e}")
     raise
```

#### Cross-References

Link to related skills and capabilities:

```markdown
## See Also

- [Database Lifecycle Management](database-lifecycle-management.md)
- [Async Testing](async-testing.md)
- [Dependency Injection](dependency-injection.md)
- [Python Language](../../languages/python/SKILL.md)
```

**Why?** Enables navigation and shows relationships.

### Writing Principles

1. **Be Specific**
   ‚ùå "Handle errors properly"
   ‚úÖ "Return HTTPException with status_code 404 and detail message"
2. **Show, Don't Just Tell**
   ‚ùå "Use the lifespan context manager"
   ‚úÖ "`python  @asynccontextmanager  async def lifespan(app: FastAPI):      engine = create_async_engine(DATABASE_URL)      yield      await engine.dispose()` "
3. **Explain Why**
   Don't just say what to do - explain why it matters:
   ‚ùå "Create the engine in lifespan"
   ‚úÖ "Create the engine in lifespan so it's disposed on shutdown, preventing connection leaks"
4. **Be Concise**
   Respect the reader's time. Remove fluff:
   ‚ùå "In this section, we're going to talk about how you can go about creating a database engine..."
   ‚úÖ "Create the database engine in lifespan:"
5. **Be Complete**
   Cover edge cases and common pitfalls:
   ‚úÖ "Note: If you're using multiple workers, stagger startup to prevent connection pool exhaustion."

### Version & Dependency Management

#### Specify Versions

Always specify versions for external dependencies:

```json
{
  "core_libraries": [
    "fastapi>=0.128.0",
    "uvicorn[standard]>=0.23.0",
    "pydantic>=2.0.0"
  ]
}
```

**Why?** Prevents breaking changes when dependencies update.

#### Document Minimum Requirements

```markdown
## Requirements

- Python 3.8+
- FastAPI 0.128.0+
- SQLAlchemy 2.0+
```

#### Note Breaking Changes

```markdown
## Version Notes

### 0.128.0 Breaking Changes

- `@app.on_event` is deprecated, use `lifespan` instead
- New CLI: `fastapi dev` replaces `uvicorn --reload`
```

#### Update Examples When Versions Change

When a dependency updates:

1. Test all examples with new version
2. Update version specifications
3. Document any breaking changes
4. Add migration notes if needed

---

## 6. Process Workflow

Follow this 6-step process to create a new skill from concept to validated artifact.

### Step 0: Example Gathering

**Goal:** Collect concrete usage examples and build domain understanding before diving into technical planning.

**Purpose:**

This step grounds the skill in real use cases, not assumptions. By collecting examples early, you ensure the skill addresses actual needs and build domain-specific vocabulary for consistent documentation.

**Actions:**

1. **Answer clarifying questions** about:
   - Specific use cases for the skill
   - Triggering conditions (when is this skill needed?)
   - Edge cases and special scenarios
   - Expected inputs and outputs

2. **Provide concrete examples** with:
   - **Input description**: What's the situation or problem?
   - **Expected output**: What should happen?
   - **Context**: Additional details (environment, constraints, etc.)

3. **Build domain terminology**:
   - Key terms and their definitions
   - Abbreviations and acronyms
   - Domain-specific language

4. **Review readiness score** (0.0-1.0):
   - **Diversity (40%)**: Are examples varied enough?
   - **Clarity (30%)**: Are requirements well-defined?
   - **Coverage (30%)**: Are edge cases addressed?

**Configuration:**

| Setting | Default | Description |
|---------|---------|-------------|
| `min_examples` | 3 | Minimum examples before proceeding |
| `readiness_threshold` | 0.8 | Score to advance (0.0-1.0) |
| `max_questions` | 10 | Maximum clarifying questions |
| `max_rounds` | 3 | Maximum feedback rounds |

**Example gathering in practice:**

```
‚ùå Too vague:
   "I need a skill for FastAPI databases"

‚úÖ Concrete examples:
   Example 1:
   - Input: FastAPI app with PostgreSQL, need async queries
   - Output: Proper async database connection with lifecycle management
   - Context: Production app, 1000 concurrent users

   Example 2:
   - Input: Testing async endpoints that hit database
   - Output: Isolated tests with rollback between tests
   - Context: CI/CD pipeline, needs to run in < 30 seconds
```

**Outputs:**

- **Refined task**: Updated task description incorporating examples
- **Collected examples**: 3+ concrete usage examples
- **Domain terminology**: Dictionary of key terms
- **Readiness score**: Confidence that requirements are understood (‚â• 0.8 to proceed)

**Checkpoint:** Can you answer "yes" to these?

- Collected at least 3 concrete examples
- Examples cover different scenarios (not just variations of the same case)
- Triggering conditions are clear
- Edge cases have been identified
- Domain terminology is documented
- Readiness score ‚â• 0.8 (or max questions reached)

**What happens next:**

The refined task and examples feed into Step 1 (Discovery), where they inform the interrogation questions and help determine the skill's scope and boundaries.

### Step 1: Discovery

**Goal:** Understand what problem you're solving and ensure it doesn't duplicate existing work.

**Actions:**

1. **Ask interrogation questions** (see [Section 3](#3-skill-creation-interrogations-discovery-questions))

- Phase 1: Understanding the Need
- Phase 2: Scope & Boundaries
- Phase 3: Capability Breakdown

2. **Search existing taxonomy:**

```bash
 # Search for related terms
 find skills -name "*.md" | xargs grep -l "search_term"

 # Check metadata for related skills
 find skills -name "metadata.json" | xargs grep -l "keyword"
```

3. **Define scope and boundaries:**

- Determine skill type, weight, load priority
- List dependencies
- Break down into capabilities (3-7)

**Outputs:**

- Skill concept document
- Taxonomy path
- Type, weight, priority assignments
- Capability list
- Dependency analysis

**Checkpoint:** Can you answer "yes" to these?

- Problem statement is clear and specific
- No existing skill covers this
- Capabilities are atomic and testable
- Dependencies are identified
- Taxonomy path is determined

### Step 2: Planning

**Goal:** Create a complete plan for the skill structure and metadata.

**Actions:**

1. **Determine metadata fields:**

- Finalize `skill_id`, `name`, `description`
- Choose `type`, `weight`, `load_priority`
- List `dependencies` and `capabilities`

2. **Plan directory structure:**

```
 skill-name/
 ‚îú‚îÄ‚îÄ metadata.json
 ‚îú‚îÄ‚îÄ SKILL.md
 ‚îú‚îÄ‚îÄ references/
 ‚îÇ   ‚îú‚îÄ‚îÄ capability-1.md
 ‚îÇ   ‚îú‚îÄ‚îÄ capability-2.md
 ‚îÇ   ‚îî‚îÄ‚îÄ ...
 ‚îú‚îÄ‚îÄ examples/
 ‚îÇ   ‚îú‚îÄ‚îÄ 01-example-1/
 ‚îÇ   ‚îî‚îÄ‚îÄ 02-example-2/
 ‚îú‚îÄ‚îÄ tests/
 ‚îÇ   ‚îî‚îÄ‚îÄ test_skill.py
 ‚îî‚îÄ‚îÄ guides/
     ‚îú‚îÄ‚îÄ requirements.json
     ‚îî‚îÄ‚îÄ reference.md
```

3. **Plan content for each file:**

- SKILL.md sections and examples
- Each capability's content
- Examples to demonstrate each capability
- Tests for validation

**Outputs:**

- Complete `metadata.json` draft
- Directory structure plan
- Content outline for each file
- Test plan

**Checkpoint:** Can you answer "yes" to these?

- metadata.json is complete and valid
- Directory structure is planned
- All capabilities have content outlines
- Examples are planned for each capability
- Tests are planned

### Step 3: Creation

**Goal:** Create the skill directory and all required files.

**Actions:**

1. **Create directory structure:**

```bash
 mkdir -p skills/path/to/skill-name
 mkdir -p skills/path/to/skill-name/capabilities
 mkdir -p skills/path/to/skill-name/examples/01-first-example
 mkdir -p skills/path/to/skill-name/tests
 mkdir -p skills/path/to/skill-name/resources
```

2. **Write metadata.json:**

- Use the draft from Step 2
- Ensure all required fields are present
- Validate JSON syntax

3. **Write SKILL.md:**

- Add YAML frontmatter (agentskills.io compliant)
- Write Overview section
- Write When to Use section
- Write Quick Reference table
- Write Core Patterns/Capabilities sections
- Write Common Mistakes table
- Write Real-World Impact section
- Add See Also links

4. **Create capability files:**

- One file per capability in `references/`
- Follow the [Capability File Format](#capability-files-format)
- Include problem statement, pattern, examples
- Add cross-references

5. **Add examples:**

- Create at least one example per capability
- Include README.md, example code, test
- Ensure examples are runnable
- Test each example

6. **Add resources:**

- requirements.json or equivalent
- Config files if applicable
- Reference materials
- Troubleshooting guide

**Outputs:**

- Complete skill directory
- All required files present and valid
- Examples tested and working
- Resources documented

**Checkpoint:** Can you answer "yes" to these?

- All required directories exist
- metadata.json is valid JSON
- SKILL.md has YAML frontmatter
- All capabilities are documented
- At least one example per capability
- Examples run successfully
- Tests pass

### Step 4: Validation

**Goal:** Ensure the skill meets all quality and compliance standards.

**Actions:**

1. **Check agentskills.io compliance:**

```bash
 # Verify YAML frontmatter
 # Check required fields
 # Validate naming conventions
```

2. **Run validation CLI:**

```bash
 uv run skill-fleet validate-skill skills/path/to/skill-name
```

3. **Test examples:**

```bash
 # Run each example
 for example in examples/*; do
     cd $example
     python example.py
     python test_example.py
 done
```

4. **Verify cross-references:**

- Check all links work
- Verify referenced skills exist
- Ensure no circular dependencies

5. **Run tests:**

```bash
 cd tests/
 pytest test_skill.py -v
```

**Outputs:**

- Validation report
- List of any issues found
- Fixes applied

**Checkpoint:** Can you answer "yes" to these?

- agentskills.io compliance verified
- Validation CLI passes
- All examples run successfully
- All cross-references work
- All tests pass

### Step 5: Review & Evolution

**Goal:** Final review and documentation of the skill's evolution.

**Actions:**

1. **Self-review using checklist:**

- Use the [Validation Checklist](#9-validation-checklist)
- Verify all sections are complete
- Check for consistency

2. **Peer review (if applicable):**

- Submit for review
- Address feedback
- Update documentation

3. **Update evolution metadata:**

```json
{
  "evolution": {
    "version": "1.0.0",
    "parent_id": null,
    "evolution_path": "initial_release",
    "change_log": "Initial skill creation with N capabilities",
    "validation_score": 1.0,
    "integrity_hash": "sha256_hash"
  }
}
```

4. **Update taxonomy_meta.json:**

- Add skill to taxonomy
- Update dependencies
- Increment version

5. **Commit to version control:**

```bash
 git add skills/path/to/skill-name
 git commit -m "Add skill-name: description of capabilities"
```

**Outputs:**

- Validated skill
- Evolution record updated
- Taxonomy metadata updated
- Version control commit

**Checkpoint:** Can you answer "yes" to these?

- Self-review complete
- Peer review complete (if applicable)
- Evolution metadata updated
- Taxonomy metadata updated
- Committed to version control

---

## 7. Common Patterns & Anti-Patterns

### Always Do

These practices ensure quality and consistency:

1. **Use kebab-case for names**

- Directories: `fastapi-production-patterns`
- Capabilities: `database-lifecycle-management`
- Examples: `01-partial-updates`

2. **Include all required directories**

- `references/`, `examples/`, `tests/`, `guides/`

3. **Specify versions in dependencies**

- `"fastapi>=0.128.0"` not `"fastapi"`

4. **Write testable examples**

- Each example should run independently
- Include test files

5. **Cross-reference related skills**

- Link to related capabilities
- Link to parent/child skills

6. **Document breaking changes**

- Note version requirements
- Explain migration paths

7. **Update taxonomy_meta.json**

- Keep taxonomy in sync
- Track dependencies

8. **Show both wrong and right approaches**

- Use ‚ùå/‚úÖ pattern
- Explain why

9. **Include quick reference tables**

- Problem ‚Üí Solution mappings
- Common mistakes and fixes

10. **Write descriptive commit messages**

- `Add skill-name: brief description`
  - Include capability list

### Never Do

These practices cause problems:

1. **Create circular dependencies**

- A depends on B, B depends on A
- Use shared abstractions instead

2. **Use camelCase or snake_case for names**

- ‚ùå `FastAPI_Patterns`, `fastapi_patterns`
- ‚úÖ `fastapi-patterns`

3. **Forget to update metadata**

- Always update `metadata.json`
- Always update `last_modified`

4. **Skip validation**

- Always run `skill-fleet validate-skill`
- Fix issues before committing

5. **Duplicate existing capabilities**

- Search existing skills first
- Enhance existing skill if appropriate

6. **Make capabilities too broad**

- > 7 capabilities ‚Üí split into multiple skills
- Each capability should be atomic

7. **Mix multiple domains in one skill**

- Keep skills focused on one domain
- Use dependencies to compose

8. **Write vague descriptions**

- ‚ùå "Database utilities"
- ‚úÖ "Async database connection lifecycle management for FastAPI"

9. **Forget to dispose resources**

- Always clean up in shutdown
- Use context managers

10. **Hardcode paths or configurations**

- Use environment variables
  - Provide config templates

### Red Flags

Watch for these warning signs:

| Red Flag                     | Why It's a Problem          | What to Do                          |
| ---------------------------- | --------------------------- | ----------------------------------- |
| Skill has >10 capabilities   | Too broad, hard to navigate | Split into multiple skills          |
| Skill has 1-2 capabilities   | Too narrow, low value       | Merge with related skill or expand  |
| No dependencies but complex  | Missing abstractions        | Extract common patterns             |
| Circular dependency detected | Breaks composition          | Refactor to use shared abstractions |
| Examples don't run           | Invalid skill               | Fix examples before committing      |
| No version specified         | Breaking changes risk       | Always specify versions             |
| Duplicate capability names   | Confusion                   | Ensure unique names                 |
| Long capability names        | Hard to reference           | Keep names concise                  |
| Missing tests                | Unknown validity            | Add tests for all capabilities      |
| No See Also sections         | Poor discoverability        | Add cross-references                |

---

## 8. Examples

### Example 1: Creating a Technical Skill

**Task:** Create a skill for Python decorators.

#### Step 1: Discovery

**Questions:**

- What problem? Understanding and implementing Python decorators
- New or existing? New (no existing decorator skill)
- Domain? Technical ‚Üí Programming ‚Üí Languages ‚Üí Python
- Type? `technical`
- Weight? `medium` (multiple decorator patterns)
- Capabilities?
  1. Basic function decorators
  2. Class decorators
  3. Decorators with arguments
  4. Property decorators
  5. Decorator composition

#### Step 2: Planning

**Metadata:**

```json
{
  "skill_id": "technical/programming/languages/python/decorators",
  "name": "python-decorators",
  "description": "Create and apply Python decorators for functions and classes, including decorators with arguments, property decorators, and decorator composition patterns.",
  "version": "1.0.0",
  "type": "technical",
  "weight": "medium",
  "load_priority": "task_specific",
  "dependencies": ["technical/programming/languages/python"],
  "capabilities": [
    "basic-function-decorators",
    "class-decorators",
    "decorators-with-arguments",
    "property-decorators",
    "decorator-composition"
  ]
}
```

#### Step 3: Creation

Create directory structure and files:

```
python-decorators/
‚îú‚îÄ‚îÄ metadata.json
‚îú‚îÄ‚îÄ SKILL.md
‚îú‚îÄ‚îÄ references/
‚îÇ   ‚îú‚îÄ‚îÄ basic-function-decorators.md
‚îÇ   ‚îú‚îÄ‚îÄ class-decorators.md
‚îÇ   ‚îú‚îÄ‚îÄ decorators-with-arguments.md
‚îÇ   ‚îú‚îÄ‚îÄ property-decorators.md
‚îÇ   ‚îî‚îÄ‚îÄ decorator-composition.md
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ 01-basic-logging-decorator/
‚îÇ   ‚îú‚îÄ‚îÄ 02-timing-decorator/
‚îÇ   ‚îî‚îÄ‚îÄ 03-property-decorators/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_decorators.py
‚îî‚îÄ‚îÄ guides/
    ‚îî‚îÄ‚îÄ requirements.json
```

#### Step 4: Validation

```bash
uv run skill-fleet validate-skill skills/technical/programming/languages/python/decorators
```

#### Step 5: Review

Commit to version control:

```bash
git add skills/technical/programming/languages/python/decorators
git commit -m "Add python-decorators: basic and class decorators, decorators with arguments, property decorators, and composition patterns"
```

### Example 2: Creating a Domain Knowledge Skill

**Task:** Create a skill for medical terminology.

#### Step 1: Discovery

**Questions:**

- What problem? Understanding medical terminology and abbreviations
- New or existing? New
- Domain? Domain Knowledge ‚Üí Medical
- Type? `domain`
- Weight? `heavyweight` (large terminology set)
- Capabilities?
  1. Common medical abbreviations
  2. Anatomy terminology
  3. Pharmacology terms
  4. Clinical terminology
  5. Medical coding systems

#### Step 2: Planning

**Metadata:**

```json
{
  "skill_id": "domain_knowledge/medical/terminology",
  "name": "medical-terminology",
  "description": "Understand and use medical terminology including common abbreviations, anatomical terms, pharmacology vocabulary, clinical terminology, and medical coding systems.",
  "version": "1.0.0",
  "type": "domain",
  "weight": "heavyweight",
  "load_priority": "on_demand",
  "dependencies": [],
  "capabilities": [
    "medical-abbreviations",
    "anatomy-terminology",
    "pharmacology-terms",
    "clinical-terminology",
    "medical-coding-systems"
  ]
}
```

#### Step 3-5: Follow same process as Example 1

### Example 3: Adding a Capability to Existing Skill

**Task:** Add "Rate Limiting" capability to FastAPI skill.

#### Step 1: Discovery

**Questions:**

- What problem? Rate limiting for FastAPI endpoints
- New or existing? Add to existing `fastapi-production-patterns`
- Fit? Yes, related to production patterns
- Atomic? Yes, rate limiting is standalone

#### Step 2: Planning

**Capability structure:**

- Name: `rate-limiting`
- Content: Problem statement, pattern, examples
- Tests: Rate limit tests

#### Step 3: Creation

Create file:

```
fastapi-production-patterns/references/rate-limiting.md
```

Add example:

```
fastapi-production-patterns/examples/09-rate-limiting/
```

Update metadata.json:

```json
{
  "capabilities": [
    ...existing...,
    "rate-limiting"
  ]
}
```

#### Step 4: Validation

```bash
uv run skill-fleet validate-skill skills/technical/programming/web-frameworks/python/fastapi
```

#### Step 5: Review

Commit:

```bash
git add skills/technical/programming/web-frameworks/python/fastapi
git commit -m "Add rate-limiting capability to fastapi-production-patterns"
```

---

## 9. Validation Checklist

Use this checklist before committing any skill.

### Structure

- All required directories exist (`references/`, `examples/`, `tests/`, `guides/`)
- `metadata.json` present and valid JSON
- `SKILL.md` present with YAML frontmatter
- At least one capability file in `references/`
- At least one example in `examples/`
- At least one test file in `tests/`
- Naming conventions followed (kebab-case)

### Content

- agentskills.io compliant (YAML frontmatter complete)
- Version specified in `metadata.json`
- Description is 1-1024 characters
- All capabilities listed in `metadata.json` are documented
- All examples are runnable
- All tests pass
- No broken cross-references

### Metadata

- `skill_id` uses path format with slashes
- `name` matches directory name
- `type` is one of: cognitive, technical, domain, tool, mcp, specialization, task_focus, memory
- `weight` is one of: lightweight, medium, heavyweight
- `load_priority` is one of: always, task_specific, on_demand, dormant
- `dependencies` list is valid (no circular dependencies)
- `capabilities` list matches actual capabilities
- `created_at` and `last_modified` are ISO-8601 timestamps
- `evolution` object is complete

### Cross-References

- No circular dependencies in dependency graph
- All referenced skills exist
- `taxonomy_meta.json` updated
- Related skills linked in `See Also` sections
- Internal links work

### Quality

- Code examples have type hints
- Anti-patterns documented (‚ùå/‚úÖ format)
- Quick reference table included
- `See Also` sections complete
- Examples are self-contained
- Tests are comprehensive

### Compliance

- `uv run skill-fleet validate-skill` passes
- agentskills.io validation passes
- No deprecated patterns used
- Version requirements specified

---

## 10. Troubleshooting

### Common Issues

#### Validation Fails

**Symptoms:**

```
‚ùå Validation failed: Missing required field 'type'
‚ùå Validation failed: Invalid naming convention
```

**Causes:**

- Missing required fields in `metadata.json`
- Invalid naming (not kebab-case)
- Missing directories
- Invalid JSON syntax

**Fixes:**

1. Check `metadata.json` has all required fields
2. Verify naming uses kebab-case
3. Ensure all required directories exist
4. Validate JSON syntax: `python -m json.tool metadata.json`

#### Examples Don't Run

**Symptoms:**

```
ModuleNotFoundError: No module named 'fastapi'
ImportError: cannot import name 'X'
```

**Causes:**

- Missing dependencies
- Wrong versions
- Missing imports in examples

**Fixes:**

1. Check `guides/requirements.json` for dependencies
2. Install dependencies: `uv sync`
3. Verify imports in example files
4. Check version specifications

#### Circular Dependency Detected

**Symptoms:**

```
‚ùå Validation failed: Circular dependency detected
A ‚Üí B ‚Üí A
```

**Causes:**

- Skill A depends on B, B depends on A
- Usually indicates missing abstraction

**Fixes:**

1. Identify the circular dependency
2. Extract common functionality into shared skill
3. Update both skills to depend on shared skill
4. Remove circular references

#### Taxonomy Conflicts

**Symptoms:**

```
‚ùå Validation failed: Duplicate skill_id found
‚ùå Validation failed: Taxonomy path already exists
```

**Causes:**

- Skill with same `skill_id` already exists
- Taxonomy path conflicts

**Fixes:**

1. Search for existing skill: `find skills -name "metadata.json" | xargs grep "skill_id"`
2. If duplicate, merge or choose different path
3. If conflict, reorganize taxonomy

#### Capability Mismatch

**Symptoms:**

```
‚ùå Validation failed: Capability 'X' listed but not found
‚ùå Validation failed: Capability file 'Y' not in metadata
```

**Causes:**

- Capability listed in `metadata.json` but no file
- Capability file exists but not listed in `metadata.json`

**Fixes:**

1. List capabilities in `references/` directory
2. Compare with `metadata.json` capabilities list
3. Ensure exact match (kebab-case)

### Getting Help

#### CLI Help

```bash
# General help
uv run skill-fleet --help

# Command-specific help
uv run skill-fleet validate-skill --help
uv run skill-fleet create-skill --help
```

#### Documentation

- [Overview](overview.md) - System architecture
- [Getting Started Guide](getting-started/index.md) - Installation & CLI workflow
- [Skill Creator Guide](skill-creator-guide.md) - DSPy workflow
- [CLI Reference](cli-reference.md) - Command reference
- [agentskills.io Compliance](agentskills-compliance.md) - Specification

#### Validation

```bash
# Validate a skill
uv run skill-fleet validate-skill path/to/skill

# Validate all skills
find skills -name "metadata.json" | while read meta; do
  uv run skill-fleet validate-skill "$(dirname "$meta")"
done
```

#### Debug Mode

```bash
# Enable debug logging
export LOG_LEVEL="DEBUG"
uv run skill-fleet validate-skill path/to/skill
```

---

## Appendix A: Quick Reference

### Skill Types

| Type             | Description                   | Examples                             |
| ---------------- | ----------------------------- | ------------------------------------ |
| `cognitive`      | Thinking patterns, reasoning  | Logical reasoning, Creative thinking |
| `technical`      | Programming, frameworks       | Python, FastAPI, Git                 |
| `domain`         | Subject matter expertise      | Medical, Legal, Financial            |
| `tool`           | Software/platform proficiency | Docker, AWS, VS Code                 |
| `mcp`            | Model Context Protocol        | MCP servers, protocol handlers       |
| `specialization` | Advanced applications         | Advanced ML, Security auditing       |
| `task_focus`     | Problem-solving methodologies | Debug-fix, Code review               |
| `memory`         | Memory management             | Context management, Retrieval        |

### Weight Guidelines

| Weight        | Capabilities | Documentation  | Examples |
| ------------- | ------------ | -------------- | -------- |
| `lightweight` | 1-3          | < 500 lines    | 1-2      |
| `medium`      | 4-7          | 500-2000 lines | 3-5      |
| `heavyweight` | 8+           | > 2000 lines   | 6+       |

### Load Priority

| Priority        | Description                 | Examples               |
| --------------- | --------------------------- | ---------------------- |
| `always`        | Core skills, always loaded  | Python, Git, Reasoning |
| `task_specific` | Loaded when task matches    | FastAPI, Docker        |
| `on_demand`     | Loaded only when referenced | Medical terminology    |
| `dormant`       | Archived/experimental       | Deprecated skills      |

### Naming Examples

| Type       | ‚úÖ Correct                               | ‚ùå Incorrect                   |
| ---------- | ---------------------------------------- | ------------------------------ |
| Directory  | `fastapi-production-patterns`            | `FastAPI_Patterns`             |
| Capability | `database-lifecycle-management`          | `DB_Lifecycle`                 |
| skill_id   | `technical/programming/languages/python` | `technical.programming.python` |

### File Templates

#### metadata.json Template

```json
{
  "skill_id": "path/to/skill",
  "name": "skill-name",
  "description": "1-1024 character description",
  "version": "1.0.0",
  "type": "technical",
  "weight": "medium",
  "load_priority": "task_specific",
  "dependencies": [],
  "capabilities": [],
  "category": "category",
  "tags": [],
  "created_at": "2026-01-09T00:00:00.000000+00:00",
  "last_modified": "2026-01-09T00:00:00.000000+00:00",
  "evolution": {
    "version": "1.0.0",
    "parent_id": null,
    "evolution_path": "initial_release",
    "change_log": "Initial skill creation",
    "validation_score": 1.0,
    "integrity_hash": "sha256"
  }
}
```

#### SKILL.md Template

```markdown
---
name: skill-name
description: 1-1024 character description
license: MIT
compatibility: Requirements
metadata:
  skill_id: path/to/skill
  version: 1.0.0
  type: technical
  weight: medium
  load_priority: task_specific
---

# Skill Title

## Overview

Description of what this skill does.

## When to Use

**When to use:**

- Condition 1
- Condition 2

**When NOT to use:**

- Condition 1

## Quick Reference

| Problem   | Solution   | Keywords |
| --------- | ---------- | -------- |
| Problem 1 | Solution 1 | keywords |

## Core Patterns/Capabilities

### Capability 1

**Problem:** What problem does it solve?
**Solution:** How to solve it

## Common Mistakes

| Mistake   | Fix   |
| --------- | ----- |
| Mistake 1 | Fix 1 |

## Real-World Impact

- **Metric**: Description ‚Üí outcome

## See Also

- [Related Skill](../related-skill/SKILL.md)
```

---

**Next Steps:**

1. Review the [Getting Started Guide](getting-started/index.md) for CLI usage
2. Read the [Skill Creator Guide](skill-creator-guide.md) for DSPy workflow
3. Explore existing skills to see patterns in action
4. Start creating your first skill!

**For questions or issues:**

- Use `uv run skill-fleet --help` for CLI assistance
- Run `uv run skill-fleet validate-skill` to validate your work
- Check existing skills for examples and patterns

Happy skill building! üöÄ


============================================================
END FILE: docs/getting-started/skill-creation-guidelines.md
============================================================

============================================================
FILE: docs/guides/api.md
============================================================

# API Guide

Last updated: 2026-01-27

## Summary

This guide describes the Skills Fleet HTTP API and recommends how to use it.

- Production API: v2 (base path `/api/v2`) ‚Äî job-based, stable
- Experimental chat API: v1 (base path `/api/v1`) ‚Äî streaming/chat features

If you are integrating with Skills Fleet for production use, prefer the v2 endpoints.

## Quick start

Start the server locally:

```bash
uv run skill-fleet serve --port 8000
# or
uvicorn skill_fleet.app.main:app --reload --port 8000
```

OpenAPI: http://localhost:8000/docs

Base URL (local): `http://localhost:8000/api/v2`

## Design overview

v2 uses a job-based pattern for long-running workflows: create a job, poll for status, and respond to HITL checkpoints when requested. This prevents HTTP timeouts and supports interactive human-in-the-loop review.

Key patterns:

- Asynchronous job creation (202 Accepted + job_id)
- Polling job status and HITL prompts
- Draft-first flow: generated drafts are saved under `skills/_drafts/<job_id>/...` and must be promoted to the taxonomy

## Core endpoints (high-level)

- POST /api/v2/skills/create ‚Äî create a skill (async)
- GET /api/v2/skills/{path} ‚Äî retrieve a skill by path or id
- GET /api/v2/hitl/{job_id}/prompt ‚Äî get current HITL prompt for a job
- POST /api/v2/hitl/{job_id}/response ‚Äî submit HITL response
- GET /api/v2/taxonomy ‚Äî list taxonomy
- GET /api/v2/taxonomy/xml ‚Äî generate agentskills.io `<available_skills>` XML
- POST /api/v2/validation/skill ‚Äî validate a skill directory
- POST /api/v2/evaluation/evaluate ‚Äî quality evaluation (DSPy metrics)
- POST /api/v2/optimization/start ‚Äî start DSPy optimization job
- GET /api/v2/jobs/{job_id} ‚Äî get job status
- POST /api/v2/drafts/{job_id}/promote ‚Äî promote draft to taxonomy

For a complete reference see: `docs/api/V2_ENDPOINTS.md` (detailed) and `docs/api/V2_SCHEMAS.md` (schemas).

## Simple example (create + poll HITL)

```python
import requests, time

BASE = "http://localhost:8000/api/v2"

# Create job
resp = requests.post(f"{BASE}/skills/create", json={"task_description": "Create a Python decorators skill"})
job_id = resp.json()["job_id"]

# Poll for HITL or completion
while True:
    status = requests.get(f"{BASE}/jobs/{job_id}").json()
    if status["status"] == "pending_hitl":
        prompt = requests.get(f"{BASE}/hitl/{job_id}/prompt").json()
        # respond with 'proceed' or other actions
        requests.post(f"{BASE}/hitl/{job_id}/response", json={"action": "proceed", "response": "intermediate"})
    elif status["status"] == "completed":
        print("Job complete")
        break
    elif status["status"] == "failed":
        raise RuntimeError(status.get("error"))
    time.sleep(2)
```

## Versioning guidance

- Use v2 for production integrations.
- Use v1 only for experimental streaming/chat clients ‚Äî v1 is intentionally separate and may change.

## Authentication & security

The default dev setup has no auth. For production add API key or OAuth/JWT middleware. See `docs/api/middleware.md` for recommended patterns and example middleware snippets.

## Links & references

- Detailed v2 endpoints: `docs/api/V2_ENDPOINTS.md`
- Schemas: `docs/api/V2_SCHEMAS.md` and `src/skill_fleet/api/schemas/`
- Job system & HITL: `docs/api/jobs.md` and `docs/guides/hitl.md`
- Taxonomy & agentskills.io: `docs/concepts/agentskills-compliance.md`


============================================================
END FILE: docs/guides/api.md
============================================================

============================================================
FILE: docs/guides/cli.md
============================================================

# CLI Guide

Last updated: 2026-01-27

This guide covers the most common `skill-fleet` CLI workflows and flags.

## Run commands with `uv run`

All Python CLI commands should be run with the `uv run` prefix to ensure the correct virtual environment:

```bash
uv run skill-fleet --help
uv run skill-fleet serve
uv run skill-fleet chat
```

## Common commands

- `uv run skill-fleet serve` ‚Äî start the FastAPI server (dev)
- `uv run skill-fleet chat` ‚Äî interactive chat mode (streaming/CLI)
- `uv run skill-fleet create "Task description"` ‚Äî start a create job
- `uv run skill-fleet promote <job_id>` ‚Äî promote a draft into taxonomy
- `uv run skill-fleet validate path/to/skill` ‚Äî validate a skill
- `uv run skill-fleet migrate` ‚Äî migrate skills to agentskills.io format
- `uv run skill-fleet generate-xml -o available_skills.xml` ‚Äî export XML

## Interactive chat

Use `uv run skill-fleet chat` for a guided conversation that drives the three-phase workflow. Use `--auto-approve` to skip HITL prompts in automation.

Flags of note:

- `--auto-approve` ‚Äî skip HITL prompts (useful for CI)
- `--show-thinking/--no-show-thinking` ‚Äî enable/disable rationale panels
- `--force-plain-text` ‚Äî fallback to plain-text prompts (useful in CI/minimal terminals)

## Best practices

- Use `--auto-approve` only in controlled automation.
- Always validate drafts before promoting them to the taxonomy.
- For heavy workloads, run the API server and call the v2 job endpoints instead of local blocking commands.

## See also

- `docs/guides/api.md` ‚Äî API examples for the same workflows
- `docs/cli/` ‚Äî original CLI reference (archived; see `docs/archive/LEGACY_README.md`)


============================================================
END FILE: docs/guides/cli.md
============================================================

============================================================
FILE: docs/guides/dspy.md
============================================================

# DSPy Guide

Last updated: 2026-01-27

This guide orients readers to the DSPy integration used by Skills Fleet. For full module-level details see `docs/dspy/`.

## What is DSPy here?

DSPy is the internal program orchestration framework used to build the three-phase skill creation programs. It provides signatures, modules, and program composition primitives used by the TaskAnalysis, ContentGeneration, and QualityAssurance orchestrators.

## Where to configure LMs

Centralized DSPy/LM configuration lives in `src/skill_fleet/llm/dspy_config.py`. Default model settings and task-specific mappings are defined in `config/config.yaml`.

## Optimization

Skill Fleet supports optimization workflows (MIPROv2, GEPA) to tune prompts and few-shot examples. Use the API or CLI `optimization` endpoints/commands to start optimization jobs.

## See also

- `docs/dspy/index.md` ‚Äî detailed DSPy architecture and modules
- `docs/dspy/optimization.md` ‚Äî optimizer guides
- `src/skill_fleet/core/dspy/` ‚Äî code-level implementations


============================================================
END FILE: docs/guides/dspy.md
============================================================

============================================================
FILE: docs/guides/hitl.md
============================================================

# Human-in-the-Loop (HITL) Guide

Last updated: 2026-01-27

HITL is central to Skills Fleet: long-running jobs pause for human confirmation, clarifications, previews, or validation.

## Key concepts

- HITL types: `clarify`, `confirm`, `preview`, `validate`.
- Jobs enter `pending_hitl` state and expose prompts at `GET /api/v2/hitl/{job_id}/prompt`.
- Clients submit responses to `POST /api/v2/hitl/{job_id}/response` with actions: `proceed`, `revise`, `refine`, `cancel`.

## Typical flow

1. Create job (POST /api/v2/skills/create) ‚Äî returns `job_id`.
2. Poll job status (/api/v2/jobs/{job_id}). If `pending_hitl`, GET the prompt.
3. Display prompt to human reviewer and collect input.
4. POST response to `/api/v2/hitl/{job_id}/response`.
5. Job resumes and continues to next phase.

## Client considerations

- Use polling or webhooks to surface HITL prompts to humans.
- Keep default HITL timeout in mind (configurable in server); default implementation waits up to 1 hour.
- Validate inputs before submitting; use structured options when available.

## Best practices

- Provide concise, actionable HITL prompts to reviewers.
- Use `preview` checkpoints to show a short excerpt instead of full content when appropriate.
- Record reviewer decisions (audit trail) and log request IDs for tracing.

## See also

- API job docs: `docs/api/jobs.md`
- CLI interactive chat: `uv run skill-fleet chat` (shows HITL prompts in the terminal)


============================================================
END FILE: docs/guides/hitl.md
============================================================

============================================================
FILE: docs/guides/workflows.md
============================================================

# Workflows Guide

Last updated: 2026-01-27

This guide explains the three-phase skill creation workflow used by Skills Fleet and the common orchestrators you will interact with.

## Overview

The skill creation workflow is organized in three high-level phases:

1. Phase 1 ‚Äî Task Analysis: analyze the user intent and produce a plan and taxonomy recommendation.
2. Phase 2 ‚Äî Content Generation: generate SKILL.md and supporting artifacts based on the plan.
3. Phase 3 ‚Äî Quality Assurance: validate and refine content until it meets quality thresholds.

Each phase has an orchestrator with async and sync entrypoints and integrates with MLflow for observability.

## Cross-phase components

- HITLCheckpointManager ‚Äî manages human-in-the-loop checkpoints across phases
- ConversationalOrchestrator ‚Äî state-machine driven multi-turn interaction for chat flows
- SignatureTuningOrchestrator ‚Äî iterative signature optimization for DSPy programs

## Quick usage (async)

```python
from skill_fleet.workflows import (
    TaskAnalysisOrchestrator,
    ContentGenerationOrchestrator,
    QualityAssuranceOrchestrator,
)

task = TaskAnalysisOrchestrator()
content = ContentGenerationOrchestrator()
qa = QualityAssuranceOrchestrator()

analysis = await task.analyze(task_description="Create a Python decorators skill")
generated = await content.generate(understanding=analysis["understanding"], plan=analysis["plan"], skill_style="comprehensive")
report = await qa.validate_and_refine(skill_content=generated["skill_content"], skill_metadata=analysis["plan"].get("skill_metadata", {}))
```

## When to use the ConversationalOrchestrator

Use the conversational flow when you need a multi-turn, interactive experience (CLI `chat` mode or streaming clients). The conversational orchestrator maps user messages to intent, asks clarifying questions, and can trigger the three-phase flow once the plan is confirmed.

## Implementation pointers

- Orchestrators live under `src/skill_fleet/workflows/` and are designed to be composable with DSPy programs in `src/skill_fleet/core/dspy/`.
- For long-running jobs use the API job endpoints (see `docs/guides/api.md`).

## See also

- Detailed orchestrator reference: `src/skill_fleet/workflows/` (code)
- DSPy modules and optimization: `docs/dspy/`
- HITL: `docs/guides/hitl.md`


============================================================
END FILE: docs/guides/workflows.md
============================================================

============================================================
FILE: docs/hitl/callbacks.md
============================================================

# HITL Callbacks Reference

**Last Updated**: 2026-01-12

## Overview

HITL callbacks are the interface between the skill creation workflow and user interaction systems (CLI, API, webhooks). The callback function is invoked at each checkpoint to collect user input.

`‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`
The callback interface is **universal**‚Äîit works with CLI, API, and webhook integrations. By defining a standard callback signature, the workflow remains agnostic to the frontend implementation.
`‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`

## Callback Signature

```python
async def hitl_callback(
    checkpoint: str,
    payload: dict[str, Any]
) -> dict[str, Any]:
    """Handle Human-in-the-Loop interactions during skill creation.

    Args:
        checkpoint: Checkpoint type ('clarify', 'confirm', 'preview', 'validate')
        payload: Checkpoint-specific data

    Returns:
        Dict with:
            - 'action': 'proceed' | 'revise' | 'refine' | 'cancel'
            - 'response': User response (for clarify)
            - 'feedback': User feedback (for revise/refine)
            - 'answers': Structured answers (for clarify)

    Raises:
        TimeoutError: If user takes too long to respond
    """
```

## Checkpoint Types

### clarify

Ask clarifying questions to resolve ambiguities.

**Payload:**
```python
{
    "questions": [
        {
            "question": "What level of detail?",
            "options": ["beginner", "intermediate", "advanced"],
            "rationale": "Need to understand target audience"
        }
    ],
    "rationale": "Why we're asking these questions"
}
```

**Response:**
```python
{
    "answers": {
        "response": "Intermediate level with practical examples"
    }
    # OR structured
    "answers": {
        "level": "intermediate",
        "focus": "practical"
    }
}
```

---

### confirm

Confirm understanding before proceeding to generation.

**Payload:**
```python
{
    "summary": "‚Ä¢ Skill: Python async programming\n‚Ä¢ Target: Intermediate developers\n‚Ä¢ Path: technical_skills/python/async",
    "path": "technical_skills/python/async",
    "key_assumptions": ["Users know basic Python", "Focus on async/await syntax"]
}
```

**Response:**
```python
{
    "action": "proceed",  # or "revise" or "cancel"
    "feedback": "Make it more advanced"  # Only if action="revise"
}
```

---

### preview

Show generated content preview for feedback.

**Payload:**
```python
{
    "content": "## Overview\nPython async/await provides...\n\n## Key Concepts\n- Coroutines\n- Event loops",
    "highlights": [
        "Clear explanations of async/await",
        "Practical examples throughout",
        "Common pitfalls covered"
    ]
}
```

**Response:**
```python
{
    "action": "proceed",  # or "refine" or "cancel"
    "feedback": "Add more error handling examples"
}
```

---

### validate

Show validation results and ask for acceptance.

**Payload:**
```python
{
    "report": "Status: PASSED\n\nAll checks passed:\n‚úì YAML frontmatter valid\n‚úì Documentation complete",
    "passed": true,
    "skill_content": "Full skill content...",
    "saved_path": "/path/to/skill"
}
```

**Response:**
```python
{
    "action": "proceed",  # or "refine" or "cancel"
    "feedback": "Add one more example to meet the minimum"
}
```

## Action Types

| Action | Description | Used In |
|--------|-------------|---------|
| `proceed` | Continue to next phase | All checkpoints |
| `revise` | Restart current phase with feedback | confirm |
| `refine` | Apply changes and continue | preview, validate |
| `cancel` | Cancel skill creation | All checkpoints |

## Implementation Examples

### CLI Implementation

```python
async def cli_hitl_callback(checkpoint: str, payload: dict) -> dict:
    """CLI HITL callback using Rich prompts."""
    from rich.prompt import Prompt
    from rich.console import Console

    console = Console()

    if checkpoint == "clarify":
        # Display questions
        for i, q in enumerate(payload["questions"], 1):
            console.print(f"{i}. {q['question']}")

        # Collect response
        response = Prompt.ask("Your answers", default="")
        return {"answers": {"response": response}}

    elif checkpoint == "confirm":
        # Display summary
        console.print(payload["summary"])
        console.print(f"Path: {payload['path']}")

        # Ask for action
        action = Prompt.ask(
            "Proceed?",
            choices=["proceed", "revise", "cancel"],
            default="proceed"
        )

        result = {"action": action}
        if action == "revise":
            result["feedback"] = Prompt.ask("What should change?")
        return result
```

### API Implementation

```python
async def api_hitl_callback(checkpoint: str, payload: dict) -> dict:
    """API HITL callback using polling."""
    job.hitl_type = checkpoint
    job.hitl_data = payload

    # Wait for client to POST response
    try:
        response = await wait_for_hitl_response(job.job_id, timeout=3600)
        return response
    except TimeoutError:
        job.status = "failed"
        job.error = "HITL interaction timed out"
        raise
```

### Webhook Implementation

```python
async def webhook_hitl_callback(checkpoint: str, payload: dict) -> dict:
    """Webhook HITL callback."""
    # Send webhook to user's endpoint
    async with httpx.AsyncClient() as client:
        webhook_response = await client.post(
            user.webhook_url,
            json={
                "checkpoint": checkpoint,
                "payload": payload,
                "job_id": job.job_id
            },
            timeout=30.0
        )

        if webhook_response.status_code != 200:
            raise Exception("Webhook delivery failed")

        # Wait for webhook response (separate endpoint)
        response = await wait_for_webhook_response(job.job_id)
        return response
```

## Best Practices

1. **Timeout Handling**: Always set reasonable timeouts (1 hour default)
2. **Error Handling**: Catch and handle user cancellations gracefully
3. **Validation**: Validate user responses before submitting
4. **Progress Feedback**: Show users what's happening
5. **State Management**: Clear responses after processing to avoid confusion

## Testing HITL Callbacks

```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_clarify_callback():
    """Test clarify checkpoint handling."""
    callback = AsyncMock(return_value={
        "answers": {"response": "intermediate level"}
    })

    result = await callback("clarify", {
        "questions": [{"question": "What level?"}]
    })

    assert result["answers"]["response"] == "intermediate level"

@pytest.mark.asyncio
async def test_confirm_revise():
    """Test confirm with revision."""
    callback = AsyncMock(return_value={
        "action": "revise",
        "feedback": "Make it more advanced"
    })

    result = await callback("confirm", {"summary": "..."})

    assert result["action"] == "revise"
    assert result["feedback"] == "Make it more advanced"
```

## See Also

- **[HITL Overview](index.md)** - System overview
- **[Interactions Documentation](interactions.md)** - Interaction types
- **[Runner Documentation](runner.md)** - Runner implementation
- **[DSPy HITL Signatures](../dspy/signatures.md#hitl-signatures)** - DSPy signatures


============================================================
END FILE: docs/hitl/callbacks.md
============================================================

============================================================
FILE: docs/hitl/index.md
============================================================

# HITL System Documentation

**Last Updated**: 2026-01-12
**Location**: `src/skill_fleet/core/signatures/hitl.py`, `src/skill_fleet/cli/hitl/runner.py`

## Overview

The Human-in-the-Loop (HITL) system enables interactive skill creation by pausing the workflow at key checkpoints to collect user feedback. This ensures the generated skills meet user expectations before finalization.

`‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`
HITL transforms skill creation from a black-box process into a collaborative experience. By pausing at critical decision points (requirements confirmation, content preview, validation results), users can guide the AI toward better outcomes.
`‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`

## HITL Architecture

```mermaid
flowchart LR
    WF[3-Phase Workflow] --> CP{Checkpoint Reached?}
    CP -->|Yes| HITL[HITL Handler]
    CP -->|No| WF

    HITL --> Type[Interaction Type]
    Type --> Display[Display Prompt]
    Display --> Input[Collect User Input]
    Input --> Response[Submit Response]
    Response --> WF

    style HITL fill:#f9f,stroke:#333
```

## Checkpoint Locations

| Phase | Checkpoint | Purpose | Interaction |
|-------|-----------|---------|--------------|
| **Phase 1** | Clarify | Resolve ambiguities | Questions |
| **Phase 1** | Confirm | Verify understanding | Summary + approve/revise |
| **Phase 2** | Preview | Review generated content | Content preview + feedback |
| **Phase 3** | Validate | Review validation results | Report + accept/refine |

## Next Steps

- **[Callbacks Documentation](callbacks.md)** - Callback interface
- **[Interactions Documentation](interactions.md)** - Interaction types
- **[Runner Documentation](runner.md)** - HITL runner implementation

## Related Documentation

- **[DSPy Signatures - HITL](../dspy/signatures.md#hitl-signatures)** - HITL DSPy signatures
- **[CLI Interactive Chat](../cli/interactive-chat.md)** - Chat mode HITL
- **[API Jobs](../api/jobs.md)** - Background job HITL


============================================================
END FILE: docs/hitl/index.md
============================================================

============================================================
FILE: docs/hitl/interactions.md
============================================================

# HITL Interactions Reference

**Last Updated**: 2026-01-13

## Overview

This document describes all HITL interaction types in detail, including their purpose, payload structure, user actions, and workflow integration.

`‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`
Each interaction type serves a specific purpose in the skill creation workflow. Clarification resolves ambiguities early, confirmation validates understanding, preview enables content feedback, and validation ensures quality before completion.
`‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`

## Interaction Types

| Type                   | Phase | Purpose                   | Input                | Actions                 |
| ---------------------- | ----- | ------------------------- | -------------------- | ----------------------- |
| **ClarifyingQuestion** | 1     | Resolve ambiguities       | Free text or options | proceed                 |
| **Confirmation**       | 1     | Verify understanding      | Summary + path       | proceed, revise, cancel |
| **Preview**            | 2     | Review generated content  | Content preview      | proceed, refine, cancel |
| **Validation**         | 3     | Review validation results | Validation report    | proceed, refine, cancel |

---

## 1. ClarifyingQuestion

### Purpose

Generate focused clarifying questions to better understand user intent before expensive generation begins.

### When It Happens

- After initial requirements gathering
- When ambiguities are detected
- Before moving to detailed planning

### Payload Structure

```python
{
    "questions": [
        {
            "question": "What level of detail should this skill cover?",
            "options": ["beginner", "intermediate", "advanced"],
            "rationale": "Need to understand target audience"
        }
    ],
    "rationale": "Why we're asking these questions"
}
```

Note: Some workflows may return `questions` as a single markdown string with a numbered list.
The CLI normalizes both formats and prompts one question at a time.

### Display Format (CLI)

The CLI displays one question at a time, and when options are present it uses
prompt-toolkit arrow-key selection (with an **Other (type my own)** free-text option).

```
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Question 1/1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ What level of detail should this skill cover?                             ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

Select one option:  (arrow keys)
```

### User Response

**Free-form:**

```python
{
    "answers": {
        "response": "Intermediate level with practical examples"
    }
}
```

**Structured:**

```python
{
    "answers": {
        "level": "intermediate",
        "focus": "practical"
    }
}
```

### Workflow Integration

```mermaid
sequenceDiagram
    participant W as Workflow
    participant U as User
    participant C as Callback

    W->>C: clarify(questions, rationale)
    C->>U: Display questions
    U->>C: User answers
    C->>W: Return answers
    W->>W: Incorporate into understanding
```

---

## 2. Confirmation

### Purpose

Summarize understanding of user intent and get confirmation before proceeding to generation.

### When It Happens

- After Phase 1 analysis is complete
- Before moving to Phase 2 generation
- After clarifying questions (if any)

### Payload Structure

```python
{
    "summary": "‚Ä¢ Skill: Python async programming\n‚Ä¢ Target: Intermediate developers\n‚Ä¢ Topics: Coroutines, event loops, async/await syntax\n‚Ä¢ Prerequisites: Basic Python knowledge",
    "path": "technical_skills/python/async",
    "key_assumptions": [
        "Users know basic Python syntax",
        "Focus on async/await not asyncio library",
        "Practical examples over theory"
    ],
    "confidence": 0.85
}
```

### Display Format (CLI)

```
‚ï≠‚îÄ üìã Understanding Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ ‚Ä¢ Skill: Python async programming    ‚îÇ
‚îÇ ‚Ä¢ Target: Intermediate developers     ‚îÇ
‚îÇ ‚Ä¢ Topics:                             ‚îÇ
‚îÇ   - Coroutines                        ‚îÇ
‚îÇ   - Event loops                       ‚îÇ
‚îÇ   - async/await syntax                ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ Proposed path:                        ‚îÇ
‚îÇ technical_skills/python/async         ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

Proceed? (proceed/revise/cancel) [proceed]:
```

### User Actions

| Action    | Description         | Follow-up                         |
| --------- | ------------------- | --------------------------------- |
| `proceed` | Accept and continue | Proceed to Phase 2                |
| `revise`  | Request changes     | Ask for feedback, restart Phase 1 |
| `cancel`  | Cancel job          | Terminate workflow                |

### Response Format

**Proceed:**

```python
{
    "action": "proceed"
}
```

**Revise:**

```python
{
    "action": "revise",
    "feedback": "Make it more advanced, targeting expert developers"
}
```

**Cancel:**

```python
{
    "action": "cancel"
}
```

### Workflow Integration

```mermaid
sequenceDiagram
    participant W as Workflow
    participant U as User
    participant C as Callback

    W->>C: confirm(summary, path)
    C->>U: Display summary
    U->>C: Action choice
    alt action = proceed
        C->>W: Return proceed
        W->>W: Continue to Phase 2
    else action = revise
        C->>W: Return revise with feedback
        W->>W: Restart Phase 1 with feedback
    else action = cancel
        C->>W: Return cancel
        W->>W: Terminate
    end
```

---

## 3. Preview

### Purpose

Show a preview of generated content for user review and feedback before finalization.

### When It Happens

- After Phase 2 content generation
- Before Phase 3 validation
- When content may be lengthy or complex

### Payload Structure

```python
{
    "content": "## Overview\nPython async/await provides...\n\n## Key Concepts\n- Coroutines\n- Event loops",
    "highlights": [
        "Clear explanations of async/await",
        "Practical examples throughout",
        "Common pitfalls covered"
    ],
    "potential_issues": [
        "Could add more error handling examples",
        "Consider adding a troubleshooting section"
    ]
}
```

### Display Format (CLI)

```
‚ï≠‚îÄ üìù Content Preview ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ ## Overview                           ‚îÇ
‚îÇ Python async/await provides...        ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ ## Key Concepts                       ‚îÇ
‚îÇ ‚Ä¢ Coroutines                          ‚îÇ
‚îÇ ‚Ä¢ Event loops                         ‚îÇ
‚îÇ ‚Ä¢ async/await syntax                  ‚îÇ
‚îÇ ...                                   ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

Highlights:
  ‚Ä¢ Clear explanations of async/await
  ‚Ä¢ Practical examples throughout
  ‚Ä¢ Common pitfalls covered

Looks good? (proceed/refine/cancel) [proceed]:
```

### User Actions

| Action    | Description          | Follow-up                        |
| --------- | -------------------- | -------------------------------- |
| `proceed` | Accept content       | Proceed to validation            |
| `refine`  | Request improvements | Ask for feedback, refine content |
| `cancel`  | Cancel job           | Terminate workflow               |

### Response Format

**Proceed:**

```python
{
    "action": "proceed"
}
```

**Refine:**

```python
{
    "action": "refine",
    "feedback": "Add more error handling examples and a troubleshooting section"
}
```

**Cancel:**

```python
{
    "action": "cancel"
}
```

### Workflow Integration

```mermaid
sequenceDiagram
    participant W as Workflow
    participant U as User
    participant C as Callback

    W->>W: Generate content
    W->>C: preview(content, highlights)
    C->>U: Display preview
    U->>C: Action choice
    alt action = proceed
        C->>W: Return proceed
        W->>W: Proceed to Phase 3
    else action = refine
        C->>W: Return refine with feedback
        W->>W: Refine content with feedback
        W->>C: preview(refined_content)
    else action = cancel
        C->>W: Return cancel
        W->>W: Terminate
    end
```

---

## 4. Validation

### Purpose

Display validation results and ask for final acceptance or refinement.

### When It Happens

- After Phase 3 validation is complete
- Before final skill acceptance
- When validation issues are found

### Payload Structure

```python
{
    "report": "Status: PASSED\n\nAll checks passed:\n‚úì YAML frontmatter valid\n‚úì Documentation complete\n‚úì Examples present",
    "passed": true,
    "skill_content": "Full skill content...",
    "saved_path": "/path/to/skills/technical_skills/python/async",
    "validation_score": 0.95
}
```

### Display Format (CLI)

```
‚ï≠‚îÄ ‚úÖ Validation Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ Status: PASSED                        ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ All checks passed:                    ‚îÇ
‚îÇ ‚úì YAML frontmatter valid               ‚îÇ
‚îÇ ‚úì Documentation complete               ‚îÇ
‚îÇ ‚úì Examples present (5)                ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ Validation Score: 0.95                ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

Accept? (proceed/refine/cancel) [proceed]:
```

### User Actions

| Action    | Description          | Follow-up                           |
| --------- | -------------------- | ----------------------------------- |
| `proceed` | Accept skill         | Mark as completed, save to taxonomy |
| `refine`  | Request improvements | Refine based on feedback            |
| `cancel`  | Cancel job           | Terminate workflow                  |

### Response Format

**Proceed:**

```python
{
    "action": "proceed"
}
```

**Refine:**

```python
{
    "action": "refine",
    "feedback": "Add 2 more examples to meet the minimum requirement"
}
```

**Cancel:**

```python
{
    "action": "cancel"
}
```

### Workflow Integration

```mermaid
sequenceDiagram
    participant W as Workflow
    participant U as User
    participant C as Callback

    W->>W: Validate skill
    W->>C: validate(report, passed)
    C->>U: Display report
    U->>C: Action choice
    alt action = proceed
        C->>W: Return proceed
        W->>W: Save to taxonomy, complete
    else action = refine
        C->>W: Return refine with feedback
        W->>W: Refine skill
        W->>W: Re-validate
        W->>C: validate(new_report, new_passed)
    else action = cancel
        C->>W: Return cancel
        W->>W: Terminate
    end
```

---

## Interaction Flow

```mermaid
stateDiagram-v2
    [*] --> Clarify: Start
    Clarify --> Confirm: Questions answered
    Confirm --> Generate: Approved
    Confirm --> Clarify: Revise

    Generate --> Preview: Content generated
    Preview --> Validate: Approved
    Preview --> Generate: Refine

    Validate --> [*]: Accepted/Cancelled
    Validate --> Generate: Refine needed
```

---

## Error Handling

| Error                  | Handling                             |
| ---------------------- | ------------------------------------ |
| **Timeout**            | Cancel job, save error message       |
| **Invalid action**     | Prompt user for valid action         |
| **Empty response**     | Use default action (usually proceed) |
| **Malformed response** | Ask user to re-submit                |

---

## Auto-Approve Mode

In auto-approve mode, all interactions use default responses:

```python
if auto_approve:
    if interaction_type == "clarify":
        return {"answers": {"response": ""}}
    else:
        return {"action": "proceed"}
```

---

## See Also

- **[HITL Overview](index.md)** - System overview
- **[Callbacks Documentation](callbacks.md)** - Callback interface
- **[Runner Documentation](runner.md)** - Runner implementation
- **[DSPy HITL Signatures](../dspy/signatures.md#hitl-signatures)** - DSPy signatures


============================================================
END FILE: docs/hitl/interactions.md
============================================================

============================================================
FILE: docs/hitl/runner.md
============================================================

# HITL Runner

**Last Updated**: 2026-01-13
**Location**: `src/skill_fleet/cli/hitl/runner.py`

## Overview

The HITL runner is the shared implementation for handling Human-in-the-Loop interactions in CLI commands. It polls the API for job status, displays prompts, collects user input, and submits responses.

`‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`
The runner is **shared** by both `create` and `chat` commands. This centralizes the polling logic and interaction handling, ensuring consistent behavior across all CLI entry points.
`‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`

## API Reference

### run_hitl_job()

Poll and satisfy HITL prompts until the job reaches a terminal state.

```python
async def run_hitl_job(
    *,
    console: Console,
    client: SkillFleetClient,
    job_id: str,
    auto_approve: bool = False,
    ui: PromptUI | None = None,
    show_thinking: bool = True,
    force_plain_text: bool = False,
    poll_interval: float = HITL_POLL_INTERVAL,
) -> dict[str, Any]:
    """Poll and satisfy HITL prompts until the job reaches a terminal state.

    Args:
        console: Rich console for output
        client: HTTP client for API communication
        job_id: Job identifier to track
        auto_approve: Skip all prompts and auto-proceed
        ui: Prompt UI implementation (prompt-toolkit when available, Rich fallback)
        show_thinking: Show rationale panels when provided by the server
        force_plain_text: Disable arrow-key dialogs (useful for limited terminals/CI)
        poll_interval: Seconds between polls (default: 2.0)

    Returns:
        Final prompt payload with status and result fields

    Raises:
        TimeoutError: If HITL interaction times out (1 hour default)
    """
```

## Constants

```python
# Default polling interval (seconds)
HITL_POLL_INTERVAL = 2.0

# HITL timeout (seconds)
HITL_TIMEOUT = 3600.0  # 1 hour
```

## Implementation Flow

```mermaid
flowchart TD
    Start([Start]) --> Poll[Poll for HITL prompt]
    Poll --> Status{Job status}
    Status -->|terminal| Return([Return final data])
    Status -->|pending_hitl| Type{Interaction type}
    Status -->|other| Wait[Wait poll_interval]
    Wait --> Poll

    Type --> Auto{Auto-approve?}
    Auto -->|Yes| Submit[Submit default response]
    Auto -->|No| Handle[Handle interaction type]

    Handle --> Clarify{clarify?}
    Handle --> Confirm{confirm?}
    Handle --> Preview{preview?}
    Handle --> Validate{validate?}
    Handle --> Unknown[Unknown type]

    Clarify --> Display1[Display questions]
    Confirm --> Display2[Display summary]
    Preview --> Display3[Display preview]
    Validate --> Display4[Display report]
    Unknown --> Display5[Display raw data]

    Display1 --> Collect1[Collect answers]
    Display2 --> Collect2[Collect action + feedback]
    Display3 --> Collect3[Collect action + feedback]
    Display4 --> Collect4[Collect action + feedback]
    Display5 --> Collect5[Collect generic action]

    Collect1 --> Submit
    Collect2 --> Submit
    Collect3 --> Submit
    Collect4 --> Submit
    Collect5 --> Submit

    Submit --> Poll
```

## Interaction Handlers

### Clarify Handler

Clarify prompts are normalized and displayed one question at a time.

- If `questions` is a numbered markdown string, the runner splits it into individual questions.
- Each question is rendered in its own panel for readability.

```python
if interaction_type == "clarify":
    rationale = prompt_data.get("rationale", "")

    if rationale:
        console.print(Panel(
            Markdown(rationale),
            title="[dim]Why I'm asking[/dim]",
            border_style="dim"
        ))

    # The runner displays each question panel separately.
    # Answers are collected one-at-a-time, then submitted as one combined response.
    answer = await ui.ask_text("Your answer (or /cancel)", default="")

    if answers.strip().lower() in {"/cancel", "/exit", "/quit"}:
        await client.post_hitl_response(job_id, {"action": "cancel"})
    else:
        await client.post_hitl_response(job_id, {"answers": {"response": answer}})
```

### Confirm Handler

```python
if interaction_type == "confirm":
    summary = prompt_data.get("summary", "")
    path = prompt_data.get("path", "")

    console.print(Panel(
        Markdown(summary),
        title="[bold cyan]üìã Understanding Summary[/bold cyan]",
        border_style="cyan"
    ))

    if path:
        console.print(f"[dim]Proposed path: {path}[/dim]")

    action = await ui.choose_one(
        "Proceed?",
        [("proceed", "Proceed"), ("revise", "Revise"), ("cancel", "Cancel")],
        default_id="proceed",
    )

    payload = {"action": action}
    if action == "revise":
        payload["feedback"] = await ui.ask_text("What should change?", default="")

    await client.post_hitl_response(job_id, payload)
```

### Preview Handler

```python
if interaction_type == "preview":
    content = prompt_data.get("content", "")
    highlights = prompt_data.get("highlights", [])

    console.print(Panel(
        Markdown(content),
        title="[bold blue]üìù Content Preview[/bold blue]",
        border_style="blue"
    ))

    if highlights:
        console.print("[dim]Highlights:[/dim]")
        for h in highlights:
            console.print(f"  ‚Ä¢ {h}")

    action = Prompt.ask(
        "Looks good?",
        choices=["proceed", "refine", "cancel"],
        default="proceed",
        show_choices=True,
    )

    payload = {"action": action}
    if action == "refine":
        payload["feedback"] = Prompt.ask("What should be improved?", default="")

    await client.post_hitl_response(job_id, payload)
```

### Validate Handler

```python
if interaction_type == "validate":
    report = prompt_data.get("report", "")
    passed = prompt_data.get("passed", False)

    title_style = "green" if passed else "red"
    title_icon = "‚úÖ" if passed else "‚ö†Ô∏è"

    console.print(Panel(
        Markdown(report),
        title=f"[bold {title_style}]{title_icon} Validation Report[/bold {title_style}]",
        border_style=title_style,
    ))

    action = Prompt.ask(
        "Accept?",
        choices=["proceed", "refine", "cancel"],
        default="proceed",
        show_choices=True,
    )

    payload = {"action": action}
    if action == "refine":
        payload["feedback"] = Prompt.ask("What should be improved?", default="")

    await client.post_hitl_response(job_id, payload)
```

## Helper Functions

### \_render_questions()

```python
