<!-- Chunk 1840: bytes 7620095-7622026, type=function -->
def build_lm_for_task(task: str, config: dict[str, Any] | None = None) -> dspy.LM:
    """
    Build LM optimized for specific task.

    Args:
        task: Task type (understanding, generation, validation, hitl)
        config: Optional configuration override

    Returns:
        Configured LM instance

    """
    cfg = config or load_fleet_config()

    # Task-specific defaults
    task_configs = {
        "understanding": {"model": "google/gemini-3-flash", "temperature": 0.5},
        "generation": {"model": "google/gemini-3-pro", "temperature": 0.7},
        "validation": {"model": "google/gemini-3-flash", "temperature": 0.3},
        "hitl": {"model": "google/gemini-3-flash", "temperature": 0.5},
    }

    task_config = task_configs.get(task, task_configs["understanding"])

    # Override from config if present
    if cfg and "llm" in cfg:
        llm_cfg = cfg["llm"]
        if "default_model" in llm_cfg:
            task_config["model"] = llm_cfg["default_model"]
        if "temperature" in llm_cfg:
            task_config["temperature"] = llm_cfg["temperature"]

    # Override from environment
    if model := os.getenv("DSPY_MODEL"):
        task_config["model"] = model
    if temp := os.getenv("DSPY_TEMPERATURE"):
        task_config["temperature"] = float(temp)

    # Extract typed values for dspy.LM
    model_str: str = str(task_config["model"])
    temperature: float = float(task_config["temperature"])
    model_type: str = "chat"
    max_tokens_val = task_config.get("max_tokens")
    max_tokens: int | None = int(max_tokens_val) if max_tokens_val is not None else None
    cache: bool = bool(task_config.get("cache", True))
    num_retries: int = int(task_config.get("num_retries", 3))

    return dspy.LM(
        model=model_str,
        model_type=model_type,
        temperature=temperature,
        max_tokens=max_tokens,
        cache=cache,
        num_retries=num_retries,
    )


