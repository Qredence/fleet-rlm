<!-- Chunk 629: bytes 965716-969688, type=function -->
def semantic_metric(example, pred, trace=None):
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')
    emb1 = model.encode(example.output)
    emb2 = model.encode(pred.output)
    similarity = (emb1 @ emb2.T).item()
    return similarity > 0.8
```

### 3. Monitor Token Usage

```python
import dspy

# Enable token tracking
dspy.settings.configure(
    lm=dspy.OpenAI(model="gpt-4"),
    trace=[]
)

# After optimization
total_tokens = sum(len(trace) for trace in dspy.settings.trace)
print(f"Total tokens used: {total_tokens}")
```

### 4. Save and Reuse Compiled Programs

```python
import pickle

# Save
with open("optimized_program.pkl", "wb") as f:
    pickle.dump(compiled, f)

# Load
with open("optimized_program.pkl", "rb") as f:
    loaded_program = pickle.load(f)
```

### 5. Document Optimization Process

```python
"""
Optimization Log for MyProgram

Date: 2024-01-15
Optimizer: BootstrapFewShot
Parameters:
  - max_labeled_demos: 5
  - max_bootstrapped_demos: 10

Training Set: 50 examples
Dev Set: 10 examples

Results:
  - Baseline: 0.65
  - Optimized: 0.82
  - Improvement: +0.17

Notes:
  - Best performance with 5 demonstrations
  - More rounds didn't improve results
"""
```

### 6. Use Separate Train/Dev/Test Sets

```python
# Split data
trainset = data[:80]
devset = data[80:90]
testset = data[90:]

# Optimize on trainset
teleprompter = dspy.BootstrapFewShot(max_labeled_demos=5)
compiled = teleprompter.compile(program, trainset=trainset)

# Tune on devset
# ... hyperparameter tuning ...

# Final evaluation on testset
final_score = evaluate(compiled, testset)
```

## Common Issues and Solutions

### Issue: Overfitting

**Problem**: Great performance on training data, poor on test data

**Solution**:
1. Use fewer demonstrations (`max_labeled_demos`)
2. Add regularization (fewer rounds)
3. Use cross-validation
4. Increase training data diversity

### Issue: Underfitting

**Problem**: Poor performance on all data

**Solution**:
1. Increase demonstrations
2. Use Chain of Thought
3. Improve training data quality
4. Try different teleprompters (MIPROv2, GEPA)

### Issue: Slow Optimization

**Problem**: Optimization takes too long

**Solution**:
1. Use smaller training set for tuning
2. Reduce `max_labeled_demos`
3. Use KNNFewShot instead of BootstrapFewShot
4. Enable multi-threading (`num_threads`)

### Issue: Unstable Results

**Problem**: Different results on each run

**Solution**:
1. Set random seed
2. Use more training examples
3. Increase `max_bootstrapped_demos`
4. Use deterministic teleprompters (LabeledFewShot)


============================================================
END FILE: .fleet/skills/dspy-optimization/references/optimizers.md
============================================================

============================================================
FILE: .fleet/skills/dspy-optimization/scripts/optimize-dspy.py
============================================================

#!/usr/bin/env python3
"""
Optimize DSPy programs with custom metrics and examples.

This script runs DSPy optimization with teleprompters, evaluates performance
against a metric, and saves the optimized program.

Usage:
    python optimize-dspy.py --module <module_name> --metric <metric_name> --examples <examples_file>
    python optimize-dspy.py --module reasoner --metric accuracy --examples train.jsonl

Options:
    --module: Name of the DSPy module to optimize
    --metric: Name of the metric function to use
    --examples: Path to examples file (JSONL format)
    --teleprompter: Teleprompter to use (default: bootstrap)
    --max-trials: Maximum number of optimization trials (default: 10)
    --output: Output file path for optimized program
"""

import argparse
import json
import pickle
import sys
from pathlib import Path

try:
    import dspy
except ImportError:
    print("Error: dspy is not installed. Install with: pip install dspy-ai")
    sys.exit(1)


