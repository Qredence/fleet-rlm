<!-- Chunk 1686: bytes 6712405-6715135, type=function -->
def _coerce_json_value(value: Any, seen: set[int] | None = None) -> Any:
    if seen is None:
        seen = set()

    if isinstance(value, Reasoning):
        return coerce_reasoning_text(value)

    if isinstance(value, dict):
        value_id = id(value)
        if value_id in seen:
            return "<cycle>"
        seen.add(value_id)
        return {key: _coerce_json_value(item, seen) for key, item in value.items()}

    if isinstance(value, list):
        value_id = id(value)
        if value_id in seen:
            return "<cycle>"
        seen.add(value_id)
        return [_coerce_json_value(item, seen) for item in value]

    if isinstance(value, tuple):
        value_id = id(value)
        if value_id in seen:
            return "<cycle>"
        seen.add(value_id)
        return [_coerce_json_value(item, seen) for item in value]

    model_dump = getattr(value, "model_dump", None)
    if callable(model_dump):
        value_id = id(value)
        if value_id in seen:
            return "<cycle>"
        seen.add(value_id)
        return _coerce_json_value(model_dump(mode="json"), seen)

    return value


async def stream_dspy_response(streaming_program: Any, **kwargs: Any) -> AsyncIterator[str]:
    """
    Convert DSPy streaming program to FastAPI Server-Sent Events format.

    This is the async generator function that yields SSE-formatted chunks.

    Args:
        streaming_program: Streamified DSPy program (async mode)
        **kwargs: Arguments to pass to the streaming program

    Yields:
        SSE-formatted strings (data: {...}\\n\\n)

    SSE Event Types:
        - "reasoning": Thinking/reasoning tokens from ChainOfThought
        - "status": Status messages (LM calls, tool calls, etc.)
        - "prediction": Final prediction result
        - "[DONE]": Stream complete marker

    Example:
        >>> stream = create_streaming_module(module, async_mode=True)
        >>> from fastapi.responses import StreamingResponse
        >>> return StreamingResponse(
        ...     stream_dspy_response(stream, question="What is 2+2?"),
        ...     media_type="text/event-stream"
        ... )

    """
    async for chunk in streaming_program(**kwargs):
        if isinstance(chunk, dspy.Prediction):
            labels = _coerce_json_value(chunk.labels())
            yield f"data: {json.dumps({'type': 'prediction', 'data': labels})}\n\n"
        elif isinstance(chunk, dspy.streaming.StreamResponse):
            yield f"data: {json.dumps({'type': 'reasoning', 'chunk': chunk.chunk})}\n\n"
        elif isinstance(chunk, dspy.streaming.StatusMessage):
            yield f"data: {json.dumps({'type': 'status', 'message': chunk.message})}\n\n"
    yield "data: [DONE]\n\n"


