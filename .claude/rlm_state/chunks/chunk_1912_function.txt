<!-- Chunk 1912: bytes 7728803-7731567, type=function -->
def get_phase5_lm() -> dspy.LM:
    """Get LM for Phase 5: Package task (worker role, thinking_level: medium)."""
    return get_reasoning_lm("skill_package")


__all__ = [
    "ConfigModelLoader",
    "get_reasoning_lm",
    "get_phase1_lm",
    "get_phase2_lm",
    "get_phase3_lm",
    "get_phase4_lm",
    "get_phase5_lm",
]


============================================================
END FILE: src/skill_fleet/infrastructure/tracing/config.py
============================================================

============================================================
FILE: src/skill_fleet/infrastructure/tracing/mlflow.py
============================================================

r"""
MLflow integration for skill creation experiment tracking.

Based on: https://dspy.ai/tutorials/math/#mlflow-dspy-integration

This module provides functions for setting up MLflow experiments and logging
skill creation metrics, decision trees, and checkpoint results.

## Enhanced Features (v2.0)

- **Hierarchical run structure**: Parent run for skill creation, child runs for phases
- **Tag organization**: Tags for user_id, job_id, skill_type for easy filtering
- **LM usage tracking**: Automatic extraction of token usage from DSPy predictions
- **Complete artifacts**: Full skill content, validation reports, quality assessments
- **Quality metrics**: Validation scores, issues found, refinements made

Usage:
    # Start MLflow UI in one terminal
    mlflow ui

    # Run skill creation with debug mode
    uv run skill-fleet create-skill \\
      --task "Create async Python skill" \\
      --debug

The debug mode will automatically log to MLflow, capturing:
- Phase decision trees
- Checkpoint validation results
- Reasoning trace metrics
- System information
- Token usage from DSPy LMs
- Complete skill artifacts

Example:
    >>> # Start parent run for skill creation
    >>> start_parent_run("my-skill-creation", user_id="user123", job_id="job456")
    >>> # Start child run for phase 1
    >>> with start_child_run("phase1_task_analysis"):
    ...     log_phase_metrics("phase1", "extract_problem", {"accuracy": 0.95})
    ...     log_lm_usage(prediction)  # Auto-extracts token usage
    >>> # Log quality metrics
    >>> log_quality_metrics({"score": 0.95, "issues_count": 2})
    >>> # Log complete skill artifacts
    >>> log_skill_artifacts(content="...", metadata={"name": "my-skill"})
    >>> end_parent_run()

"""

from __future__ import annotations

import logging
from datetime import UTC
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from collections.abc import Mapping

logger = logging.getLogger(__name__)

# Default MLflow configuration
DEFAULT_EXPERIMENT_NAME = "skill-fleet-phase1-phase2"
DEFAULT_TRACKING_URI = "mlruns"


