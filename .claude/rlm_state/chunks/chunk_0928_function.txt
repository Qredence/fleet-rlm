<!-- Chunk 928: bytes 3353325-3368495, type=function -->
def long_running_task(task_id: str):
    # This runs after response is sent
    result = process_heavy_computation()
    # Update status in DB
    mark_task_complete(task_id, result)

@app.post("/process")
async def start_process(background_tasks: BackgroundTasks):
    task_id = generate_task_id()
    background_tasks.add_task(long_running_task, task_id)
    return {"task_id": task_id, "status": "processing"}

# Better: Celery for production (handles retries, distributed)
```

## Common Mistakes

| Mistake | Why It's Wrong | Fix |
|---------|----------------|-----|
| Creating DB engine at import time | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown |
| Using `requests` in async endpoints | Blocks entire event loop | Use `httpx.AsyncClient` |
| Forgetting `exclude_unset=True` | Optional fields become `None` and overwrite data | Use `exclude_unset=True` for PATCH |
| Sync fixtures with async tests | Tests hang or fail mysteriously | Use `@pytest.mark.asyncio` with async fixtures |
| Global state for dependencies | Can't test, hard to manage lifecycle | Use `Depends()` with yield |
| Not setting `pool_recycle` | Database closes idle connections, causing errors | Set `pool_recycle=3600` or similar |
| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale | Proper async conversion |

## Real-World Impact
- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors
- **Test execution time** reduced 80% with proper async fixtures
- **API response validation** caught 15% of frontend bugs before deployment
- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory


============================================================
END FILE: docs/internal/plans/archive/2026-01-09-fastapi-production-patterns-design.md
============================================================

============================================================
FILE: docs/internal/plans/archive/2026-01-12-cli-chat-ux.md
============================================================

# 2026-01-12 — CLI Chat Interactive UX (API-backed)

## Goal

- Make `skill-fleet chat` feel conversational and resilient while keeping a **single source of truth** in the FastAPI job + HITL workflow.
- Ensure the API remains the canonical DSPy execution surface (CLI is a thin client).

## Current State

- CLI `create`/`chat` uses:
  - `POST /api/v2/skills/create` to start a background job
  - `GET /api/v2/hitl/{job_id}/prompt` to poll for HITL prompts
  - `POST /api/v2/hitl/{job_id}/response` to answer prompts
- There is no separate `/api/v2/chat/*` surface.
- Shared HITL runner keeps `create` and `chat` behavior consistent.
- Template guidance lives in:
  - `config/templates/SKILL_md_template.md`
  - `config/templates/metadata_template.json`

## Findings (CLI chat UX)

- Polling UX: while the job is `running`, the CLI is silent (no progress affordance).
- HITL input UX: task descriptions and refinement feedback are often multi-line, but prompts are currently single-line.
- Resilience: jobs are stored in-memory; restart loses state (CLI sees 404).
- Mixed execution modes: some CLI commands call the API, others run local DSPy/workflow code (confusing when troubleshooting).

## Recommendations

### CLI UX

1. Add a Rich `Live` status line/spinner while waiting for prompts.
2. Add multi-line input helpers for:
   - initial task description
   - refine/revise feedback
3. Add resume/reconnect:
   - `skill-fleet chat --resume <job_id>`
   - `skill-fleet create --resume <job_id>` (optional)
4. Provide consistent command affordances during HITL:
   - `/help`, `/cancel`, `/exit`

### API

1. Add job inspection:
   - `GET /api/v2/jobs/{job_id}` → status, timestamps, current phase/type
2. Add explicit cancellation:
   - `POST /api/v2/jobs/{job_id}/cancel`
3. Replace polling with push (later):
   - SSE or WebSocket endpoint for HITL prompt delivery
4. Persist jobs in Redis (production mode) to survive restarts.

### DSPy + templates

1. Treat `SKILL_md_template.md` as the canonical authoring structure for generated output.
2. Standardize `metadata.json` output shape across the taxonomy (align to `metadata_template.json` + add a migration if needed).
3. Decide whether to converge `src/skill_fleet/core/` and `src/skill_fleet/workflow/` or document them as intentionally separate stacks.

## Execution Checklist

- [ ] Add `/api/v2/jobs/*` endpoints and `SkillFleetClient` methods.
- [ ] Update CLI HITL runner to show live “waiting” status and support resume.
- [ ] Add unit tests for HITL runner behavior and cancel/clarify parsing in `SkillCreationProgram`.
- [ ] Update documentation (`AGENTS.md`, README) to describe the API-backed chat flow and templates.

## Validation

```bash
# from repo root
uv run ruff check src/skill_fleet
uv run pytest -q tests/unit
```



============================================================
END FILE: docs/internal/plans/archive/2026-01-12-cli-chat-ux.md
============================================================

============================================================
FILE: docs/internal/plans/archive/2026-01-12-cli-code-review.md
============================================================

# CLI Code Review Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Create a comprehensive code review plan for the `src/skill_fleet/cli/` directory to identify improvements, ensure consistency, and maintain code quality standards.

**Architecture:** Systematic review of CLI components categorized by file type, functionality, and dependencies, using industry best practices for Python CLI applications and the project's own conventions.

**Tech Stack:** Python 3.12+, argparse/Typer, Rich, httpx, asyncio, pytest

---

## Overview

This plan reviews all CLI code in `src/skill_fleet/cli/` across multiple dimensions:

1. **Code Quality**: Structure, patterns, and maintainability
2. **Testing**: Test coverage and quality
3. **Documentation**: Docstrings, comments, and usage docs
4. **Best Practices**: Adherence to Python/Typer/Rich conventions
5. **Project Conventions**: Alignment with skill-fleet standards
6. **Dependencies**: Imports, API usage, and version compatibility
7. **Error Handling**: Exception management and user feedback
8. **Security**: Input validation and sensitive data handling

**Scope:** All 14 Python files in `src/skill_fleet/cli/`:
- `main.py` (argparse-based, 676 lines)
- `app.py` (Typer-based, 65 lines)
- `interactive_cli.py` (Rich-based, 670 lines)
- `onboarding_cli.py` (108 lines)
- `client.py` (httpx client, 64 lines)
- `commands/create.py` (94 lines)
- `commands/list_skills.py` (33 lines)
- `commands/serve.py` (20 lines)
- `commands/chat.py` (191 lines)
- `interactive_typer.py`
- `utils/__init__.py`
- `hitl/__init__.py`
- `commands/__init__.py`
- `__init__.py`

---

## Task 1: Review CLI Architecture and Organization

**Files:**
- Review: `src/skill_fleet/cli/main.py`, `src/skill_fleet/cli/app.py`, `src/skill_fleet/cli/commands/`

**Step 1: Analyze dual CLI implementation**

```bash
# Identify discrepancies between argparse (main.py) and Typer (app.py) implementations
uv run python -c "
import ast
import sys

main_imports = set()
with open('src/skill_fleet/cli/main.py') as f:
    tree = ast.parse(f.read())
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                main_imports.add(alias.name.split('.')[0])
        elif isinstance(node, ast.ImportFrom):
            main_imports.add(node.module.split('.')[0])

print('main.py imports:', sorted(main_imports))
"

uv run python -c "
import ast
with open('src/skill_fleet/cli/app.py') as f:
    tree = ast.parse(f.read())
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            print(f'Function: {node.name}')
        elif isinstance(node, ast.ClassDef):
            print(f'Class: {node.name}')
"
```

**Step 2: Document command overlap/conflicts**

Create `docs/plans/cli-review-architecture.md`:

```markdown
# CLI Architecture Analysis

## Two CLI Implementations Identified

### main.py (argparse-based)
- Entry point: `cli_entrypoint()`
- Commands: create-skill, validate-skill, onboard, analytics, migrate, generate-xml, optimize, interactive
- Lines: 676

### app.py (Typer-based)
- Entry point: `app()` (not currently registered in pyproject.toml)
- Commands: create, list, serve, chat
- Lines: 65

## Issues to Address
1. Dual CLI implementations create confusion
2. Different command naming conventions (kebab-case vs lowercase)
3. app.py commands duplicate main.py functionality
4. No migration plan from argparse to Typer documented
```

**Step 3: Create architecture summary**

Run: `uv run python -c "
from pathlib import Path
import ast

cli_dir = Path('src/skill_fleet/cli')
files = list(cli_dir.rglob('*.py'))
print(f'Total Python files: {len(files)}')
for f in files:
    lines = len(f.read_text().splitlines())
    rel_path = f.relative_to(cli_dir)
    print(f'  {rel_path}: {lines} lines')
"
```

**Step 4: Commit architecture findings**

```bash
git add docs/plans/cli-review-architecture.md
git commit -m "docs: document CLI architecture review findings"
```

---

## Task 2: Review Code Quality and Patterns

**Files:**
- Review: `src/skill_fleet/cli/main.py`, `src/skill_fleet/cli/app.py`, `src/skill_fleet/cli/commands/*.py`

**Step 1: Check for code duplication**

```bash
# Find duplicate patterns across CLI files
uv run python -c "
from pathlib import Path
import re

cli_dir = Path('src/skill_fleet/cli')
files = ['main.py', 'app.py', 'interactive_cli.py', 'client.py']

print('=== Common Patterns Across CLI Files ===')
for py_file in files:
    path = cli_dir / py_file
    content = path.read_text()

    # Check for async patterns
    async_def_count = len(re.findall(r'async def', content))
    print(f'{py_file}: {async_def_count} async functions')

    # Check for Rich usage
    console_count = len(re.findall(r'Console\(\)', content))
    print(f'  {console_count} Console() instantiations')

    # Check for error handling
    try_count = len(re.findall(r'try:', content))
    except_count = len(re.findall(r'except', content))
    print(f'  {try_count} try blocks, {except_count} except blocks\n')
"
```

**Step 2: Analyze function complexity**

```bash
# Identify long functions (>50 lines)
uv run python -c "
import ast
from pathlib import Path

cli_dir = Path('src/skill_fleet/cli')
for py_file in cli_dir.rglob('*.py'):
    if '__pycache__' in str(py_file):
        continue

    content = py_file.read_text()
    tree = ast.parse(content)

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            func_lines = node.end_lineno - node.lineno
            if func_lines > 50:
                print(f'{py_file.relative_to(cli_dir)}: {node.name}() is {func_lines} lines')
"
```

**Step 3: Check magic numbers and literals**

```bash
# Find hardcoded values that should be constants
uv run rg '#[0-9]+|\"[^\"]{30,}\"' src/skill_fleet/cli/
```

**Step 4: Document quality issues**

Update `docs/plans/cli-review-architecture.md`:

```markdown
## Code Quality Issues

### Long Functions (>50 lines)
- `main.py:interactive_skill_cli()` - Complex interaction loop
- `interactive_cli.py:run()` - 164 lines, needs refactoring
- `commands/chat.py:chat_command()` - 95 lines, complex state management

### Code Duplication
- Multiple Console() instantiations across files
- Repeated error handling patterns (try/except/finally with client.close())
- Similar async wrapper patterns in commands/create.py, list_skills.py, chat.py

### Hardcoded Values
- Main loop constants: `refresh_per_second=10` in interactive_cli.py:529
- Sleep delays: `await asyncio.sleep(2)` in multiple locations
- UI element counts: Hardcoded in several places

### Recommendations
1. Extract shared async wrapper pattern to utility function
2. Create Rich console manager singleton
3. Define constants for all magic numbers
4. Break down long functions into smaller, testable units
```

**Step 5: Commit quality findings**

```bash
git add docs/plans/cli-review-architecture.md
git commit -m "docs: add code quality issues to review"
```

---

## Task 3: Review Testing Coverage

**Files:**
- Review: `tests/test_onboarding.py`, `tests/test_api.py`, existing test patterns

**Step 1: Analyze current test coverage**

```bash
# Run pytest with coverage on CLI code
uv run pytest tests/ --cov=src/skill_fleet/cli --cov-report=term-missing

# Check which files have no tests
uv run python -c "
from pathlib import Path

cli_dir = Path('src/skill_fleet/cli')
test_dir = Path('tests')

cli_files = [f.relative_to(cli_dir) for f in cli_dir.rglob('*.py') if f.name != '__init__.py']
test_files = set(test_dir.rglob('*cli*.py'))

print('CLI files with no dedicated tests:')
for cli_file in cli_files:
    test_name = f'test_{cli_file.stem}'
    if not any(test_name in t.name for t in test_files):
        print(f'  - src/skill_fleet/cli/{cli_file}')
"
```

**Step 2: Identify missing test scenarios**

```bash
# Create checklist of untested CLI behaviors
cat > docs/plans/cli-test-coverage.md << 'EOF'
# CLI Test Coverage Analysis

## Commands with No Tests
- `main.py:create_skill()` - Complex workflow with reasoning tracing
- `main.py:validate_skill()` - Skill validation logic
- `main.py:migrate_skills_cli()` - Migration tool
- `main.py:generate_xml_cli()` - XML generation
- `main.py:optimize_workflow_cli()` - MIPROv2/GEPA optimization
- `main.py:show_analytics()` - Analytics display
- `app.py:serve_command()` - Server startup
- `app.py:create_command()` - Typer-based create flow
- `app.py:list_command()` - List skills
- `app.py:chat_command()` - Interactive chat dashboard

## Missing Test Scenarios

### main.py
- [ ] Command-line argument parsing and validation
- [ ] Error handling for missing required arguments
- [ ] JSON output format validation
- [ ] Dry-run mode for migrate command
- [ ] Reasoning tracer integration (cli, debug, full modes)

### interactive_cli.py
- [ ] Session persistence and loading
- [ ] Checklist status display
- [ ] Multi-skill queue management
- [ ] Command handling (/help, /exit, /save, etc.)
- [ ] Streaming display with Rich Live
- [ ] Multi-choice question formatting

### client.py
- [ ] HTTP client initialization with custom URLs
- [ ] API error handling (4xx, 5xx responses)
- [ ] Timeout and retry logic
- [ ] Connection cleanup in finally blocks

### Commands/*.py
- [ ] HITL polling loop behavior
- [ ] Auto-approve mode flow
- [ ] Interactive prompts and validation
- [ ] Dashboard layout rendering
- [ ] Session state transitions
EOF
```

**Step 3: Create test template**

Create `tests/cli/test_cli_commands.py`:

```python
"""Tests for CLI commands."""

import pytest
from unittest.mock import MagicMock, patch
from pathlib import Path

from skill_fleet.cli.main import create_skill, validate_skill, migrate_skills_cli
from skill_fleet.cli.app import app, CLIConfig


@pytest.fixture
