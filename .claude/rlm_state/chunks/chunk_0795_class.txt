<!-- Chunk 795: bytes 2671318-2674997, type=class -->
class InMemoryCache:
    def __init__(self):
        self._cache: dict[str, CacheEntry] = {}
        self._stats = CacheStats()

    def get(self, key: str) -> Any | None:
        entry = self._cache.get(key)
        if entry is None:
            self._stats.misses += 1
            return None

        if entry.is_expired():
            del self._cache[key]
            self._stats.misses += 1
            return None

        self._stats.hits += 1
        return entry.value

    def set(self, key: str, value: Any, ttl: int = 300):
        self._cache[key] = CacheEntry(
            key=key,
            value=value,
            expires_at=time.time() + ttl,
        )
```

## Redis Migration Guide

For production deployments requiring distributed caching, migrate to Redis:

### 1. Install Dependencies

```bash
uv add redis
```

### 2. Configure Redis

```yaml
# config/config.yaml
cache:
  backend: redis  # or "memory" for in-memory
  redis:
    url: "redis://localhost:6379/0"
    prefix: "skill_fleet:"
    default_ttl: 300
```

### 3. Use Redis Backend

```python
from skill_fleet.app.cache_manager import get_cache_backend

# Automatically uses Redis if configured
cache = get_cache_backend()
await cache.set("key", value, ttl=300)
value = await cache.get("key")
```

### Redis vs In-Memory

| Feature | In-Memory | Redis |
|---------|-----------|-------|
| **Setup** | No dependencies | Requires Redis server |
| **Performance** | Fastest (local) | Fast (network) |
| **Scalability** | Single-process | Distributed |
| **Persistence** | Lost on restart | Optional persistence |
| **Use Case** | Development, single-instance | Production, multi-instance |

## Best Practices

### 1. Choose Appropriate TTL

```python
# ❌ Too short - cache ineffective
@cache_endpoint(ttl=10)
async def get_taxonomy():
    pass

# ✅ Appropriate for data that changes infrequently
@cache_endpoint(ttl=300)
async def get_taxonomy():
    pass

# ❌ Too long - stale data
@cache_endpoint(ttl=3600)
async def get_job_status(job_id: str):
    pass

# ✅ Short TTL for rapidly changing data
@cache_endpoint(ttl=30)
async def get_job_status(job_id: str):
    pass
```

### 2. Always Invalidate on Write

```python
async def update_skill(skill_id: str, data: dict):
    skill = await repository.update(skill_id, data)

    # Always invalidate cache after writes
    cache_manager.invalidate_pattern(f"/skills/{skill_id}/*")

    return skill
```

### 3. Use User-Specific Caching for Personalized Data

```python
@cache_user_data  # Automatically includes user_id in cache key
async def get_user_recommendations(user_id: str):
    return get_recommendations(user_id)
```

### 4. Monitor Cache Hit Rates

```python
# Periodically check cache health
stats = cache_manager.get_stats()
if stats['hit_rate'] < 0.7:
    logger.warning(f"Low cache hit rate: {stats['hit_rate']:.2%}")
    # Consider adjusting TTL or cache size
```

## Troubleshooting

### Low Hit Rate

**Symptoms**: Cache hit rate below 70%

**Solutions**:
1. Increase TTL for frequently accessed data
2. Check cache key generation - ensure consistency
3. Verify invalidation patterns aren't too broad
4. Monitor cache size - may need eviction policy

### Stale Data

**Symptoms**: Cache returns outdated data

**Solutions**:
1. Reduce TTL for rapidly changing data
2. Implement write-through caching
3. Add explicit invalidation on data changes
4. Use cache versioning

### High Memory Usage

**Symptoms**: Cache grows unbounded

**Solutions**:
1. Implement LRU eviction policy
2. Set maximum cache size
3. Use shorter TTLs
4. Move to Redis with configured maxmemory

## Configuration

```python
# In skill_fleet/api/cache_config.py
