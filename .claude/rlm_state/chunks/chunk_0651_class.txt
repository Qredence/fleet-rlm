<!-- Chunk 651: bytes 1028112-1032527, type=class -->
class ModuleMonitor(dspy.Module):
    def __init__(self, module, name, tracer=None, quality_metric=None):
        super().__init__()
        self.module = module
        self.name = name
        self.tracer = tracer or ExecutionTracer()
        self.quality_metric = quality_metric
    
    def forward(self, **kwargs):
        trace = self.tracer.start_trace(self.name, inputs=kwargs)
        
        try:
            result = self.module(**kwargs)
            
            if self.quality_metric:
                trace.quality_score = self.quality_metric(result)
            
            self.tracer.end_trace(trace, success=True)
            return result
        
        except Exception as e:
            self.tracer.end_trace(trace, success=False, error=str(e))
            raise
```

### Step 3: Use in Production

```python
from skill_fleet.core.dspy.monitoring import ModuleMonitor, ExecutionTracer

# Create shared tracer
tracer = ExecutionTracer(max_traces=1000)

# Wrap critical modules
generator = GenerateSkillContentModule()
monitored = ModuleMonitor(
    generator,
    name="skill_generator",
    tracer=tracer,
    quality_metric=lambda x: score_quality(x.skill_content),
)

# Use normally
result = monitored(task="Create skill")

# Check metrics anytime
metrics = monitored.get_metrics()
print(f"Success rate: {metrics['success_rate']:.2%}")
print(f"Avg quality: {metrics['avg_quality_score']:.3f}")

# Export for analysis
tracer.export_traces("production_traces.json")
```

## Verification Checklist

After completing Phase 1:

- [ ] All signatures have Literal types where appropriate
- [ ] OutputField descriptions include quality indicators
- [ ] Docstrings are concise (1-2 sentences)
- [ ] Training data has 50-100 examples
- [ ] Training data covers all major categories
- [ ] Training data includes all 3 skill styles
- [ ] JSON structure validated
- [ ] Monitoring package created and tested
- [ ] ModuleMonitor successfully wraps test module
- [ ] ExecutionTracer collects and exports traces
- [ ] All type checks pass (`uv run ty check src/`)

## Next Steps

After Phase 1 completion:
1. **Baseline Evaluation**: Test unoptimized program on held-out test set
2. **Phase 2**: Run optimization with improved signatures and training data
3. **Monitoring**: Integrate ModuleMonitor into production workflow
4. **Documentation**: Update project docs with new patterns

## Estimated Timeline

- Signature enhancements: 2-4 hours
- Training data expansion: 3-5 hours
- Monitoring infrastructure: 2-3 hours
- **Total**: 1-2 days for complete Phase 1

## Common Issues

**Type errors after adding Literals**:
- Add `from __future__ import annotations`
- Import from `typing` module
- Run type checker to verify

**Training data extraction fails**:
- Check SKILL.md files have valid YAML frontmatter
- Verify frontmatter includes `name` and `description`
- Check file encoding is UTF-8

**Monitoring overhead too high**:
- Set `log_inputs=False, log_outputs=False`
- Remove quality_metric parameter
- Reduce max_traces limit


============================================================
END FILE: .fleet/skills/dspy-optimization-workflow/references/phase1-implementation.md
============================================================

============================================================
FILE: .fleet/skills/dspy-optimization-workflow/references/troubleshooting.md
============================================================

# Troubleshooting Guide - DSPy Optimization

Common issues and solutions when optimizing DSPy programs in skills-fleet.

## Low Quality Scores (<0.70)

### Symptom
After optimization, quality scores remain below 0.70, or improvement is minimal.

### Possible Causes & Solutions

**1. Insufficient Training Data**
```bash
# Check training set size
jq length config/training/trainset_v4.json

# Should be 50-100 examples minimum
```

✅ **Solution**: Expand training data
```bash
# Extract more examples
uv run python scripts/expand_training_data.py

# Generate synthetic examples
uv run python scripts/generate_synthetic_examples.py
```

**2. Training Data Not Diverse**
```bash
# Check category distribution
jq '[.[].expected_taxonomy_path] | group_by(.) | map({category: .[0], count: length})' config/training/trainset_v4.json
```

✅ **Solution**: Add examples from underrepresented categories

**3. Metric Too Strict**
```python
# Test metric on examples
