<!-- Chunk 1788: bytes 7453699-7456592, type=function -->
def phase1_checkpoint_score(
    problem_statement: str,
    is_new_skill: bool | None,
    skill_type: str,
    proposed_path: str,
    overlapping_skills: list | None = None,
) -> float:
    """
    Calculate Phase 1 checkpoint score.

    This is a convenience function for calculating the checkpoint score
    directly from Phase 1 outputs, without requiring a Prediction object.

    Args:
        problem_statement: Problem statement
        is_new_skill: Whether this is a new skill
        skill_type: Skill type
        proposed_path: Taxonomy path
        overlapping_skills: List of overlapping skills (optional)

    Returns:
        Float score from 0.0 to 1.0

    Examples:
        >>> score = phase1_checkpoint_score(
        ...     problem_statement="Clear problem here",
        ...     is_new_skill=True,
        ...     skill_type="technical",
        ...     proposed_path="technical/programming/python"
        ... )
        >>> print(score)  # Should be 1.0

    """
    pred = {
        "problem_statement": problem_statement,
        "is_new_skill": is_new_skill,
        "skill_type": skill_type,
        "proposed_path": proposed_path,
        "overlapping_skills": overlapping_skills or [],
    }

    return phase1_completeness_reward({}, pred)


__all__ = [
    "phase1_completeness_reward",
    "phase1_checkpoint_score",
    "VALID_SKILL_TYPES",
]


============================================================
END FILE: src/skill_fleet/core/optimization/rewards/phase1_rewards.py
============================================================

============================================================
FILE: src/skill_fleet/core/optimization/rewards/phase2_rewards.py
============================================================

"""
Reward/metric functions for Phase 2: Scope & Boundaries.

These metric functions are used for:
- DSPy MultiChainComparison for critical decisions
- Checkpoint validation scoring
- MIPROv2 optimization

From guidelines lines 1066-1073, Phase 2 validation criteria:
1. metadata.json is complete and valid
2. Directory structure is planned
3. All capabilities have content outlines
4. Examples are planned for each capability
5. Tests are planned
6. Type determined using decision matrix
7. Weight assigned based on capability count
8. Load priority chosen using decision tree

The phase2_validity_metric function returns a 0.0-1.0 score based on
how well the Phase 2 outputs meet these criteria.
"""

from __future__ import annotations

import logging
from typing import Any

logger = logging.getLogger(__name__)

# Valid values from guidelines
VALID_SKILL_TYPES = [
    "cognitive",
    "technical",
    "domain",
    "tool",
    "mcp",
    "specialization",
    "task_focus",
    "memory",
]

VALID_WEIGHTS = ["lightweight", "medium", "heavyweight"]

VALID_PRIORITIES = ["always", "task_specific", "on_demand", "dormant"]


