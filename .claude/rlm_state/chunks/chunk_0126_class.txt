<!-- Chunk 126: bytes 168933-176773, type=class -->
class AnalysisSignature(dspy.Signature):
    """Signature for analysis tasks."""

    data = dspy.InputField(desc="Data to analyze")
    criteria = dspy.InputField(desc="Analysis criteria")
    findings = dspy.OutputField(desc="Key findings")
    recommendations: List[str] = dspy.OutputField(desc="List of recommendations")
    priority = dspy.OutputField(desc="Priority level (high/medium/low)")


============================================================
END FILE: .fleet/factory/skills/dspy-basics/templates/signature-template.py
============================================================

============================================================
FILE: .fleet/factory/skills/dspy-configuration/SKILL.md
============================================================

---
name: dspy-configuration
description: DSPy configuration, LM setup, caching, and version management. Use when configuring language models, enabling caching, managing dependencies, or setting up multi-provider configurations.
---

# DSPy Configuration

DSPy configuration, LM setup, caching, and version management.

## Quick Start

### Configure LM Globally
```python
import dspy

# Configure LM globally
dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))
response = qa(question="How many floors are in the castle?")
print('GPT-4o-mini:', response.answer)
```

### Switch LM Locally
```python
# Change LM within a context block
with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):
    response = qa(question="How many floors are in the castle?")
    print('GPT-3.5-turbo:', response.answer)
```

### Enable Caching
```python
# Configure caching
dspy.configure_cache(
    enable_disk_cache=True,
    enable_memory_cache=True,
)
```

### Configure Responses API
```python
dspy.configure(
    lm=dspy.LM(
        "openai/gpt-5-mini",
        model_type="responses",
        temperature=1.0,
        max_tokens=16000,
    ),
)
```

## When to Use This Skill

Use this skill when:
- Configuring language models for DSPy
- Setting up LM switching (global/local)
- Enabling and customizing caching
- Configuring multi-provider LMs
- Managing version compatibility
- Setting up Responses API for advanced models

## Core Concepts

### LM Configuration
Configure which language models to use and how to switch between them.

**Key features:**
- **Global configuration**: `dspy.configure(lm=...)` for entire session
- **Local overrides**: `dspy.context(lm=...)` for code blocks
- **Multi-provider**: Support for OpenAI, Anthropic, Together AI, and more
- **Responses API**: Enable advanced model features

**See:** [references/lm-config.md](references/lm-config.md) for:
- LM configuration patterns
- Multi-provider setup
- Responses API configuration
- Best practices

### Caching
Manage cache behavior to improve performance and control costs.

**Cache layers:**
- **In-memory cache**: Fast access using cachetools.LRUCache
- **On-disk cache**: Persistent storage using diskcache.FanoutCache
- **Server-side cache**: Managed by LLM provider (OpenAI, Anthropic)

**See:** [references/caching.md](references/caching.md) for:
- Cache architecture details
- Custom cache key implementation
- Cache debugging techniques
- Configuration options

### Version Management
Track and manage dependency versions to ensure compatibility.

**Key features:**
- **Automatic versioning**: `save()` captures dependency versions
- **Version checking**: Alerts on version mismatches
- **Compatibility**: Prevents issues from outdated dependencies

**See:** [references/versioning.md](references/versioning.md) for:
- Dependency versioning patterns
- Version mismatch detection
- Compatibility best practices

## Scripts

The `scripts/` directory provides reusable tools:

- **compile-dspy.py**: Compile DSPy modules with proper caching
- **clear-cache.py**: Clear DSPy cache safely

## Progressive Disclosure

This skill uses progressive disclosure:

1. **SKILL.md** (this file): Quick reference and navigation
2. **references/**: Detailed technical docs loaded as needed

Load reference files only when you need detailed information on a specific topic.

## Related Skills

- **dspy-basics**: Signature design, basic modules, program composition
- **dspy-optimization**: Teleprompters, metrics, optimization workflows
- **dspy-advanced**: ReAct agents, tool calling, output refinement


============================================================
END FILE: .fleet/factory/skills/dspy-configuration/SKILL.md
============================================================

============================================================
FILE: .fleet/factory/skills/dspy-configuration/references/caching.md
============================================================

# DSPy Caching

Manage cache behavior to improve performance and control costs. This guide covers cache architecture, configuration, and debugging.

## Table of Contents

- [Caching Overview](#caching-overview)
- [Cache Architecture](#cache-architecture)
- [Basic Cache Configuration](#basic-cache-configuration)
- [Custom Cache Implementation](#custom-cache-implementation)
- [Cache Debugging](#cache-debugging)
- [Best Practices](#best-practices)

## Caching Overview

### What is Caching?

DSPy caching stores LM responses to avoid redundant API calls, improving performance and reducing costs.

### Why Use Caching?

- **Performance**: Faster response times for cached results
- **Cost reduction**: Fewer API calls to LM providers
- **Consistency**: Same input always returns same output
- **Debugging**: Replay cached responses for debugging

### Cache Layers

DSPy caching is architected in three distinct layers:

1. **In-memory cache**: Fast access using cachetools.LRUCache
2. **On-disk cache**: Persistent storage using diskcache.FanoutCache
3. **Server-side cache**: Managed by LM provider (e.g., OpenAI, Anthropic)

## Cache Architecture

### In-Memory Cache

Implemented using `cachetools.LRUCache` for fast access to frequently used data.

**Characteristics:**
- Fastest access time
- Limited by memory
- Lost on program restart
- Thread-safe

### On-Disk Cache

Implemented using `diskcache.FanoutCache` for persistent storage.

**Characteristics:**
- Persistent across restarts
- Slower than in-memory cache
- Limited by disk space
- Thread-safe

### Server-Side Cache

Managed by LLM service provider (e.g., OpenAI, Anthropic).

**Characteristics:**
- Provider-managed
- Transparent to user
- May have cost implications
- Provider-specific policies

DSPy does not directly control the server-side prompt cache, but offers users the flexibility to enable, disable, and customize the in-memory and on-disk caches.

## Basic Cache Configuration

### Enable All Caches (Default)

Both in-memory and on-disk caching are enabled by default:

```python
import dspy

# Caching is enabled by default
# No action required to start using the cache
```

### Disable All Caches

Disable both in-memory and on-disk caching:

```python
import dspy

# Configure cache settings
dspy.configure_cache(
    enable_disk_cache=False,
    enable_memory_cache=False,
)
```

### Disable In-Memory Cache Only

```python
import dspy

# Keep disk cache, disable memory cache
dspy.configure_cache(
    enable_memory_cache=False,
    enable_disk_cache=True,
)
```

### Disable On-Disk Cache Only

```python
import dspy

# Keep memory cache, disable disk cache
dspy.configure_cache(
    enable_memory_cache=True,
    enable_disk_cache=False,
)
```

### Configure Custom Disk Cache Directory

```python
import dspy

# Specify custom cache directory
dspy.configure_cache(
    disk_cache_dir="/path/to/custom/cache",
)
```

## Custom Cache Implementation

### Custom Cache Key

Override `cache_key()` method to create custom cache keys:

```python
import dspy
from typing import Dict, Any, Optional
import ujson
from hashlib import sha256

