<!-- Chunk 466: bytes 729954-784071, type=function -->
def validate_skill(skill_path):
    """Basic validation of a skill"""
    skill_path = Path(skill_path)
    
    # Check SKILL.md exists
    skill_md = skill_path / 'SKILL.md'
    if not skill_md.exists():
        return False, "SKILL.md not found"
    
    # Read and validate frontmatter
    content = skill_md.read_text()
    if not content.startswith('---'):
        return False, "No YAML frontmatter found"
    
    # Extract frontmatter
    match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
    if not match:
        return False, "Invalid frontmatter format"
    
    frontmatter = match.group(1)
    
    # Check required fields
    if 'name:' not in frontmatter:
        return False, "Missing 'name' in frontmatter"
    if 'description:' not in frontmatter:
        return False, "Missing 'description' in frontmatter"
    
    # Extract name for validation
    name_match = re.search(r'name:\s*(.+)', frontmatter)
    if name_match:
        name = name_match.group(1).strip()
        # Check naming convention (hyphen-case: lowercase with hyphens)
        if not re.match(r'^[a-z0-9-]+$', name):
            return False, f"Name '{name}' should be hyphen-case (lowercase letters, digits, and hyphens only)"
        if name.startswith('-') or name.endswith('-') or '--' in name:
            return False, f"Name '{name}' cannot start/end with hyphen or contain consecutive hyphens"

    # Extract and validate description
    desc_match = re.search(r'description:\s*(.+)', frontmatter)
    if desc_match:
        description = desc_match.group(1).strip()
        # Check for angle brackets
        if '<' in description or '>' in description:
            return False, "Description cannot contain angle brackets (< or >)"

    return True, "Skill is valid!"

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python quick_validate.py <skill_directory>")
        sys.exit(1)
    
    valid, message = validate_skill(sys.argv[1])
    print(message)
    sys.exit(0 if valid else 1)

============================================================
END FILE: .fleet/letta/skills/meta/skill-creator/scripts/quick_validate.py
============================================================

============================================================
FILE: .fleet/letta/skills/meta/skill-learning/SKILL.md
============================================================

---
name: skill-learning-patterns
description: Meta-skill for recognizing learnings, validating improvements, and contributing back to the collective knowledge base. Use when agents discover better patterns, find gaps in existing skills, or want to improve shared knowledge.
license: MIT
---

# Skill Learning Patterns

This meta-skill teaches agents to recognize valuable learnings during their work and contribute improvements back to the communal skill repository through pull requests.

## Core Philosophy

This repository is a **living knowledge base** that improves through collective learning. As agents work on tasks, they discover:
- Better approaches than documented
- Missing error handling cases
- Gaps in existing skills
- Clearer ways to explain patterns
- New patterns worth capturing

These discoveries should flow back into the repository so all agents benefit.

**Critical principle:** Skills must contain **general-purpose knowledge** that helps many agents across different contexts. This is not a place for project-specific configurations, personal preferences, or one-off solutions. Focus on patterns, principles, and practices that are broadly applicable.

## When to Use This Skill

Use this skill when:
- You discover something that took significant time to figure out
- Existing skill instructions led you astray or were incomplete
- You find yourself repeatedly solving the same undocumented problem
- You correct a mistake based on learning what actually works
- You notice a pattern emerging across multiple tasks
- You want to improve or clarify existing documentation

## Learning Recognition Process

### 1. Notice Patterns During Work

Pay attention to signals that indicate learnable moments:

**Time investment signals:**
- "I spent 20+ minutes debugging this"
- "I tried 3 approaches before finding what worked"
- "I wish I had known this at the start"

**Repetition signals:**
- "This is the third time I've solved this"
- "I remember encountering this before"
- "Other agents probably face this too"

**Correction signals:**
- "The skill said X, but Y actually works better"
- "I misunderstood the instruction and it caused problems"
- "This approach failed in ways not documented"

Consult `references/recognizing-learnings.md` for detailed patterns.

### 2. Validate the Learning

Before proposing changes, validate based on contribution type:

**For Tool/SDK Documentation:**
- ✅ Tool is widely-used (1000+ GitHub stars, top search result, or Letta product)
- ✅ Shares battle-tested insights beyond official docs (what you struggled with, not basic usage)
- ✅ Well-documented with working examples
- ✅ Accurate and up-to-date
- ❌ NOT just "getting started" guides (official docs already cover that)

**For Pattern Contributions:**
- ✅ **Is this generalizable beyond your specific context?** (Most critical)
- ✅ Have you seen this pattern multiple times? (2-3+ instances)
- ✅ Did you test that your approach works better?
- ✅ Does this address a real gap vs. personal preference?
- ✅ Are there edge cases or tradeoffs to consider?
- ✅ Framework-specific patterns require validation through real agent experience (not just "well-established practices")

See `references/validation-criteria.md` for detailed guidance.

### 3. Determine Contribution Type

**Update existing skill** when:
- Skill has incorrect or outdated information
- Skill is missing an important case or pattern
- Instructions are unclear and caused confusion
- Examples would help but are missing

**Create new skill** when:
- Tool/SDK: Widely-used tool (1000+ stars/top search result/Letta product) with battle-tested insights
- Pattern: Appears frequently (3+ times) across different contexts and isn't documented
- Knowledge would benefit many agents across different projects (not just your specific setup)

**Do NOT contribute** when:
- Learning is specific to your project/context (e.g., "Our API endpoint is X")
- Solution only works in your unique environment
- It's a personal preference without objective benefit
- It's a one-off workaround for unusual situation
- Knowledge is too narrow to help other agents

**Note in conversation only** when:
- Learning might be valuable but needs more validation
- Pattern needs more observation before documenting
- Temporary workaround that might become obsolete

### 4. Contribute via Pull Request

**Important:** All contributions go through pull requests, not direct commits to main.

**PR workflow:**
1. Create a feature branch for your changes
2. Make updates to skill(s)
3. Test that changes improve clarity/correctness
4. Write clear PR description with rationale
5. Submit PR for review
6. Respond to feedback and iterate

Consult `references/pr-workflow.md` for detailed process.

## Contribution Quality Standards

### Good Contributions Include

**Clear rationale:**
```
"I encountered rate limiting with OpenRouter 5 times. Added exponential 
backoff pattern with jitter which resolved all instances. This pattern 
isn't documented anywhere in ai/models/."
```

**Before/after comparison:**
```
Before: "Use memory_rethink for updates"
After: "Use memory_insert for concurrent writes (safer), memory_rethink 
only for single-agent exclusive access"
Why: Prevents data loss in multi-agent scenarios
```

**Evidence of validation:**
```
"Tested across 3 different projects, pattern held. Also confirmed in 
Letta docs. Previous approach caused data loss 2/3 times."
```

**Preserved existing knowledge:**
- Don't delete working information
- Build on rather than replace
- Add context, don't remove context

### Avoid These Anti-Patterns

❌ **Premature optimization** - Changing after single instance without validation

❌ **Over-generalization** - "This worked for me once" → "Always do this"

❌ **Opinion as fact** - Personal preference without objective improvement

❌ **Churn** - Changes that are different but not better

❌ **Deleting context** - Removing information that might help others

## Common Pitfalls

Even well-intentioned contributions can miss the mark. Here are patterns to watch for:

### Pitfall 1: The Specificity Trap

**Pattern:** Documenting your specific solution instead of extracting the general pattern.

**Example - TOO SPECIFIC:**
```
Skill: git-workflow-manager
Content: "Always end commits with: Written by Cameron ◯ Letta Code"
Problem: This is Cameron's personal preference, not general knowledge
```

**Example - APPROPRIATELY GENERAL:**
```
Skill: git-workflow-manager (revised concept)
Content: "Check repository for commit conventions in CONTRIBUTING.md or recent commits"
Better: Teaches pattern of discovering conventions, applies to any repository
```

**How to avoid:**
- Ask: "Would this help an agent on a completely different project?"
- Look for personal names, specific URLs, environment-specific configs
- Extract the pattern, not just your implementation

### Pitfall 2: Documentation vs. Specialized Knowledge

**Pattern:** Creating skills that just reformat well-known documentation.

**Example - JUST DOCUMENTATION:**
```
Skill: "How to use git"
Content: Explains git commands, branching, committing, PR creation
Problem: This is standard git knowledge available everywhere
```

**Example - SPECIALIZED KNOWLEDGE:**
```
Skill: "mcp-builder" (in this repository)
Content: Patterns for creating MCP servers - not just "how to use the MCP protocol" 
but specific guidance on tool design, error handling patterns, testing strategies
Better: Takes general protocol knowledge and adds specialized patterns for building quality servers
```

**How to avoid:**
- If official docs cover it well, link to docs instead of recreating
- Ask: "What specialized insight am I adding beyond standard documentation?"
- Focus on non-obvious patterns, edge cases, or agent-specific considerations

### Pitfall 3: Over-Generalizing From One Instance

**Pattern:** "I made one mistake" → "Let's create 3 new skills to prevent it"

**Real example from this repository (November 2025):**
```
Observation: One agent submitted overly-specific git-workflow-manager skill
Initial reaction: "We need CULTURE.md + 3 new skills (knowledge-curation, 
agent-human-collaboration, pattern-recognition) + extended documentation"
Correction: Another agent called this out as the exact over-generalization we warn against
Right-sized solution: Add this "Common Pitfalls" section instead
Learning: The system worked - peer review caught premature abstraction
```

**How to avoid:**
- Count your data points: 1 occurrence = note it, 3+ = consider contributing
- Check if existing skill can be extended instead of creating new one
- Ask: "Am I solving a real recurring problem or reacting to one experience?"

### Pitfall 4: The Abstraction Ladder Confusion

Understanding where your learning sits on the abstraction ladder:

**Level 1 - Specific Solution (TOO LOW for skills):**
```
"I configured Firebase Auth with Google OAuth by setting these environment 
variables and calling these specific API endpoints"
→ Too specific, only helps others using Firebase + Google OAuth
```

**Level 2 - Pattern (GOOD for skills):**
```
"OAuth integration strategies: Provider discovery, token management, callback 
handling, session persistence. Applies to Google, GitHub, Microsoft, etc."
→ General enough to help with any OAuth integration
```

**Level 3 - Principle (ALSO GOOD for skills):**
```
"Delegating authentication to specialized providers: Trade-offs between 
managed auth services vs. self-hosted, security considerations, user experience"
→ Helps with authentication decisions broadly
```

**Where to contribute:**
- Level 1: Keep in project-specific docs or memory, not skills
- Level 2: Perfect for skills - concrete patterns that generalize
- Level 3: Great for skills - principles that apply across domains

**How to climb the ladder:**
- Start with: "What did I do?"
- Extract: "What pattern was I following?"
- Distill: "What principle guided this?"
- Contribute at Level 2 or 3

### Pitfall 5: Personal Preferences as Best Practices

**Pattern:** "I like doing it this way" → "Everyone should do it this way"

**Example:**
```
"Always use arrow functions in JavaScript"
"Always put API calls in src/api/ directory"  
"Always use Claude Sonnet over GPT-4o"
→ These are preferences without objective evidence of superiority
```

**How to avoid:**
- Ask: "Can I measure why this is better?" (speed, reliability, cost, etc.)
- Consider: "Are there valid reasons someone would choose differently?"
- Test: "Does this improve outcomes or just match my style?"

### Pitfall 6: Fragmenting Information

**Pattern:** Creating new skills/docs when information should be added to existing ones.

**Signs you're fragmenting:**
- New skill significantly overlaps with existing skill's domain
- Information could be a section in existing skill
- Creates "which skill do I check?" confusion
- Duplicates concepts across multiple skills

**How to avoid:**
- Review existing skills in the domain first
- Ask: "Does this extend an existing skill or truly need separate space?"
- Bias toward extending existing skills unless clearly distinct domain

### Self-Check Before Contributing

Ask yourself:

1. ❓ Is this **general** enough? (Would it help agents on different projects?)
2. ❓ Is this **specialized** enough? (Does it add insight beyond standard docs?)
3. ❓ Is this **validated** enough? (Have I seen this pattern 2+ times?)
4. ❓ Is this **objective** enough? (Based on evidence, not preference?)
5. ❓ Is this **appropriately placed**? (New skill vs. extend existing vs. don't contribute?)

If you can confidently answer yes to all five → Contribute

If you're unsure on any → More validation needed or reconsider contribution type

## PR Description Template

Use this template for skill contributions:

```markdown
## What

[Brief description of what's changing]

## Why

[Problem you encountered / gap you found / improvement opportunity]

## Evidence

[How you validated this is better / how you tested / how often you saw this]

## Impact

[Who benefits / what situations this helps / what it prevents]

## Testing

[How you verified the change works / examples you tried]
```

See `references/contribution-examples.md` for real examples.

## Example Workflows

### Workflow 1: Correcting Existing Skill

```
During task: "Following letta-memory-architect, I used memory_rethink for 
concurrent updates. Result: data loss when two agents wrote simultaneously."

Validation: "Checked references/concurrency.md - it says memory_insert is 
safer but warning wasn't prominent. Tested memory_insert with concurrent 
writes - no data loss."

Action: 
1. Create feature branch: fix/memory-concurrency-warning
2. Update SKILL.md to make warning more prominent
3. Add concrete example of data loss scenario
4. Create PR: "Emphasize memory_insert for concurrent writes"
5. Explain in PR: "Misread the guidance, led to data loss. Making warning 
   more visible to prevent this for other agents."
```

### Workflow 2: Adding Missing Pattern

```
During task: "Hit OpenRouter rate limits 5 times across different projects. 
Spent 30min each time figuring out exponential backoff."

Validation: "Pattern works consistently. Checked ai/models/ - not documented. 
This is generalizable beyond my specific use case."

Action:
1. Create feature branch: add/api-rate-limiting-skill
2. Create new skill: ai/models/api-rate-limiting/
3. Document exponential backoff pattern with code examples
4. Create PR: "Add API rate limiting patterns"
5. Explain: "Common pattern that caused repeated debugging time. Validated 
   across 5 instances with different APIs."
```

### Workflow 3: Clarifying Ambiguity

```
During task: "Skill said 'use appropriate model' but didn't define criteria. 
Tried GPT-4o, Claude Sonnet, GPT-4o-mini before finding best fit."

Validation: "Through testing, identified that task complexity + budget 
constraints should guide model choice. This clarification would have saved 
1 hour."

Action:
1. Create feature branch: clarify/model-selection-criteria  
2. Add decision tree to skill
3. Include examples: "For X task → Y model because Z"
4. Create PR: "Add model selection decision tree"
5. Explain: "Ambiguous guidance led to trial-and-error. Adding decision 
   criteria to help agents choose upfront."
```

## Self-Correction Culture

**When you make mistakes:**
- Note what you learned
- Update relevant skill if gap exists
- Don't just fix the instance, prevent future instances

**When you discover better approaches:**
- Compare objectively with current documented approach
- Test to validate improvement
- Propose update with clear reasoning

**When skills lead you astray:**
- Don't assume skill is wrong without investigation
- Validate your alternative approach
- If truly better, propose improvement with evidence

## Validation Questions

Before submitting PR, ask:

1. Is this a genuine improvement or just different?
2. Have I validated this works better?
3. Is my evidence strong enough?
4. Am I preserving existing valid knowledge?
5. Will other agents benefit from this?
6. Is my PR description clear about what and why?

## Building on Others

**Attribution:**
- Reference existing skills you're building on
- Credit agents/humans whose insights helped
- Link to forum discussions or sources

**Collaboration:**
- Respond to PR feedback constructively
- Iterate based on reviewer insights
- Merge after approval, don't force through

**Continuous improvement:**
- Your contribution will be built upon by others
- This is expected and encouraged
- Living knowledge base means constant evolution

## Next Steps

After contributing:
1. Watch for PR feedback and respond
2. Note if your learning helps in future tasks
3. Continue pattern recognition in your work
4. Build on what you contributed as you learn more

The goal: **A knowledge base that gets smarter with every agent interaction.**


============================================================
END FILE: .fleet/letta/skills/meta/skill-learning/SKILL.md
============================================================

============================================================
FILE: .fleet/letta/skills/meta/skill-learning/references/contribution-examples.md
============================================================

# Contribution Examples

Real-world examples of good contributions to help you understand what makes an effective PR.

## Example 1: Correcting Guidance That Led to Errors

### The Situation

Agent followed `letta-memory-architect` guidance to use `memory_rethink` for updating shared memory blocks. Result: Data loss when two agents wrote simultaneously.

### The Investigation

```
1. Checked references/concurrency.md
2. Found memory_insert is safer for concurrent writes
3. Realized warning existed but wasn't prominent
4. Tested memory_insert with concurrent writes - no data loss
5. Identified that warning needs to be more visible
```

###PR Created

**Title:** "Emphasize memory_insert for concurrent writes"

**Description:**
```markdown
## What

Makes the concurrent write warning more prominent in letta-memory-architect 
and adds concrete example of data loss scenario.

## Why

I followed the skill's guidance and used memory_rethink for updates in a 
multi-agent scenario. Result: Data loss when two agents wrote simultaneously.

The warning about concurrent writes existed in references/concurrency.md but 
wasn't prominent in the main SKILL.md. This led me to miss it.

## Evidence

- Reproduced data loss with memory_rethink (7/10 concurrent writes lost data)
- Tested memory_insert with same scenario (0/10 data loss)
- Confirmed in references/concurrency.md that this is documented behavior
- Pattern is clear: append-only is safer for concurrency

## Impact

Prevents other agents from making same mistake that causes data loss.
Makes critical safety information visible where agents first look.

## Testing

Added concrete example showing data loss scenario. Verified example is 
clear and illustrative. Checked that warning now appears in main skill body.
```

**Changes:**
- Move warning to prominent position in SKILL.md
- Add concrete "what goes wrong" example
- Bold the safety recommendation
- Link to detailed concurrency patterns

**Outcome:** Merged. Prevents future agents from data loss.

---

## Example 2: Adding Missing Pattern

### The Situation

Agent hit API rate limiting 5 times across different projects (OpenRouter, Anthropic, OpenAI). Each time spent 20-30 minutes implementing exponential backoff from scratch.

### The Investigation

```
1. Searched skills repository for rate limiting patterns - not found
2. Checked ai/models/ and ai/tools/ - no coverage
3. Researched best practices - exponential backoff with jitter is standard
4. Validated pattern works across all three APIs
5. Determined this is generalizable, not project-specific
```

### PR Created

**Title:** "Add API rate limiting patterns"

**Description:**
```markdown
## What

Creates new skill `ai/models/api-rate-limiting` covering exponential backoff, 
jitter, and retry strategies for HTTP APIs.

## Why

Encountered rate limiting 5 times across different projects:
- OpenRouter (2 times)
- Anthropic API (2 times)
- OpenAI API (1 time)

Each time spent 20-30 minutes implementing retry logic from scratch. This is a 
common pattern that should be documented.

## Evidence

- Tested pattern across all three APIs successfully
- Pattern is HTTP standard (RFC 6585 for 429 responses)
- Exponential backoff with jitter is documented best practice
- Saved ~25 minutes per instance after documenting

## Impact

Prevents repeated implementation of same pattern. Helps any agent integrating 
external APIs handle rate limiting correctly.

## Testing

Created test scenarios with intentional rate limiting. Verified:
- Backoff timing works correctly
- Jitter prevents thundering herd
- Max retry limit prevents infinite loops
- Pattern works across different API providers
```

**Changes:**
- New skill: `ai/models/api-rate-limiting/SKILL.md`
- Includes code examples for implementation
- Documents when to use and when not to use
- Covers different retry strategies and tradeoffs
- Updates README.md to list new skill

**Outcome:** Merged. Now saves time for all agents working with APIs.

---

## Example 3: Clarifying Ambiguous Instructions

### The Situation

Agent working on code review task. Skill said "use appropriate model" but didn't define criteria. Tried GPT-4o, then Claude Sonnet, then GPT-4o-mini before finding GPT-4o was best fit.

### The Investigation

```
1. Noted that "appropriate model" is ambiguous
2. Through testing, identified factors: task complexity, budget, latency
3. Compared three models systematically on code review task
4. Found GPT-4o caught all issues, GPT-4o-mini missed subtle ones
5. Determined decision tree would have prevented trial-and-error
```

### PR Created

**Title:** "Add model selection decision tree to letta-agent-designer"

**Description:**
```markdown
## What

Adds decision tree to references/model-recommendations.md for choosing 
between GPT-4o, GPT-4o-mini, and Claude Sonnet based on task requirements.

## Why

Skill said "use appropriate model" without criteria. Spent 1 hour testing 
three models for code review before finding GPT-4o was needed.

Decision criteria weren't clear:
- When is cost-savings worth quality trade-off?
- How to assess task complexity?
- What defines "production-critical"?

## Evidence

Tested systematically on code review task:

GPT-4o-mini: Fast, cheap, missed 2/10 subtle issues
Claude Sonnet: Good quality, caught 9/10 issues
GPT-4o: Caught all 10 issues, worth cost for code review

Clear pattern: Task criticality and complexity drive choice.

## Impact

Helps agents choose right model upfront instead of trial-and-error.
Saves time and helps balance cost vs quality appropriately.

## Testing

Applied decision tree to 5 different task types. In each case, tree 
led to correct model choice. Validated criteria with team.
```

**Changes:**
- Add decision tree flowchart to model-recommendations.md
- Include examples: "For X task → Y model because Z"
- Document factors: complexity, budget, latency, criticality
- Link from main SKILL.md to decision tree

**Outcome:** Merged. Agents can now choose models systematically.

---

## Example 4: Documenting Edge Case

### The Situation

Agent tried to use `git add -i` (interactive add) as documented in git-workflows skill. Command failed: "interactive mode not supported".

### The Investigation

```
1. Tested command - consistently fails in Bash tool
2. Reason: Non-interactive environment doesn't support -i flag
3. Checked if other interactive commands fail - yes (git rebase -i also fails)
4. Found alternative: git add <files> works fine
5. Determined this is environment limitation worth documenting
```

### PR Created

**Title:** "Add warning about interactive git commands"

**Description:**
```markdown
## What

Adds warning to git-workflows about non-interactive environment limitations.

## Why

Followed skill guidance to use `git add -i` for selective staging.
Command failed: "interactive mode not supported"

Root cause: Bash tool environment doesn't support interactive commands.
This affects any git command with -i flag.

## Evidence

Tested:
- git add -i → fails
- git rebase -i → fails  
- git add <files> → works
- git rebase HEAD~3 → works (non-interactive rebase)

Pattern: Interactive flags don't work, non-interactive alternatives do.

## Impact

Prevents agents from trying interactive commands that will fail.
Provides working alternatives for same functionality.

## Testing

Verified non-interactive alternatives work:
- git add with explicit file paths
- git rebase with commit count
- Other non-interactive git operations

All work correctly in Bash tool environment.
```

**Changes:**
- Add warning box at top of git-workflows SKILL.md
- Document which commands don't work (-i flag commands)
- Provide non-interactive alternatives
- Explain environment limitation

**Outcome:** Merged. Saves agents from hitting known limitation.

---

## Example 5: Building on Existing Skill

### The Situation

Agent working on multi-agent coordination. `letta-agent-designer` mentions multi-agent briefly but doesn't cover coordination patterns in detail. Agent discovered 3 patterns that keep recurring.

### The Investigation

```
1. Reviewed letta-agent-designer - covers basics but not coordination
2. Worked on 4 different multi-agent projects
3. Identified 3 recurring patterns:
   - Supervisor-worker
   - Peer-to-peer with shared state
   - Pipeline/sequential processing
4. Each has different trade-offs and use cases
5. Substantial enough for separate skill
```

### PR Created

**Title:** "Add letta-multi-agent-coordinator skill"

**Description:**
```markdown
## What

Creates new skill `letta/multi-agent-coordinator` covering 
coordination patterns for multi-agent systems.

## Why

letta-agent-designer mentions multi-agent capabilities but doesn't detail 
coordination patterns. Worked on 4 multi-agent projects and found 3 patterns 
recurring:

1. Supervisor-worker (1 coordinator, N workers)
2. Peer-to-peer with shared state (agents coordinate via shared memory)
3. Pipeline (sequential processing, output of A feeds to B)

Each pattern has different trade-offs. This knowledge is substantial enough 
for dedicated skill.

## Evidence

Patterns emerged consistently across projects:
- Customer support system (supervisor-worker)
- Code review team (peer-to-peer)
- Document processing (pipeline)
- Data analysis team (supervisor-worker)

Clear that these are generalizable patterns, not project-specific.

## Impact

Helps agents design multi-agent systems without rediscovering patterns.
Documents trade-offs so agents choose right pattern for use case.

## Testing

Applied patterns retrospectively to 4 projects - each pattern clearly 
fits specific use cases. Documented when to use each pattern and why.
```

**Changes:**
- New skill: `letta/multi-agent-coordinator/SKILL.md`
- Three pattern reference files, one per coordination pattern
- Examples for each pattern
- Decision criteria for pattern selection
- Links from letta-agent-designer to new skill
- Updates README.md

**Outcome:** Merged. Complements existing skill with deeper coverage.

---

## Common Patterns in Good Contributions

### Clear Problem Statement
Every example starts with: "I encountered X problem"

### Validation Through Testing
Every example includes: "I tested this and here are results"

### Evidence of Generalizability
Every example shows: "This pattern appeared multiple times / across contexts"

### Impact on Others
Every example explains: "This helps future agents by preventing/enabling X"

### Concrete Changes
Every example specifies exactly what files changed and why

### Respectful Tone
Every example is collaborative, not accusatory ("skill was wrong")

## What Makes These PRs Effective

1. **Specificity:** Exact problem, exact solution, exact results
2. **Evidence:** Testing, measurement, validation
3. **Impact:** Clear benefit to others
4. **Completeness:** All context needed to evaluate
5. **Humility:** Open to feedback, willing to iterate
6. **Attribution:** Credits sources, builds on existing work

Use these examples as templates for your own contributions. The pattern is consistent: recognize learning, validate thoroughly, document clearly, contribute via PR.


============================================================
END FILE: .fleet/letta/skills/meta/skill-learning/references/contribution-examples.md
============================================================

============================================================
FILE: .fleet/letta/skills/meta/skill-learning/references/pr-workflow.md
============================================================

# Pull Request Workflow

All contributions to the skills repository go through pull requests (PRs), not direct commits to main. This ensures quality, enables review, and maintains a clear contribution history.

## Why Pull Requests?

**Quality control:**
- Changes reviewed before merging
- Catch errors or unclear instructions
- Validate improvements are sound

**Collaboration:**
- Discuss alternatives and tradeoffs
- Learn from reviewer feedback
- Build shared understanding

**History:**
- Clear record of what changed and why
- Attributable contributions
- Easy to revert if needed

**Learning:**
- Feedback improves your future contributions
- See how others approach problems
- Collective knowledge building

## PR Workflow Steps

### 1. Create Feature Branch

**Before making changes,** create a branch for your work.

**Branch naming conventions:**
```
add/skill-name              # New skill
update/skill-name-aspect    # Update existing skill
fix/skill-name-issue        # Fix error or bug
clarify/skill-name-section  # Improve clarity
```

**Example:**
```bash
git checkout -b add/api-rate-limiting
```

### 2. Make Your Changes

**Update or create skills:**
- Edit SKILL.md files
- Add/update reference files
- Include examples if helpful
- Update README if adding new skill

**Test your changes:**
- Read through as if you're seeing it for first time
- Check that examples work
- Verify links/references are correct
- Ensure formatting is clean

**Best practices:**
- Keep changes focused (one improvement per PR)
- Preserve existing valid content
- Write clearly and concisely
- Include examples where helpful

### 3. Stage and Commit Changes

**Stage relevant files:**
```bash
git add path/to/changed/files
```

**Write clear commit message:**
```bash
git commit -m "Add exponential backoff pattern to API integration

Includes retry logic with jitter for rate limiting scenarios.
Tested across OpenRouter, Anthropic, and OpenAI APIs."
```

**Commit message format:**
```
<Short summary of change>

<Longer explanation of what and why>
<Evidence or testing notes if relevant>
```

### 4. Push Branch to Remote

**First time pushing branch:**
```bash
git push -u origin add/api-rate-limiting
```

**Subsequent pushes:**
```bash
git push
```

### 5. Create Pull Request

**Use GitHub web interface or CLI (gh):**
```bash
gh pr create --title "Add API rate limiting patterns" \
  --body "$(cat <<'EOF'
## What

Adds exponential backoff pattern for handling API rate limits.

## Why

Hit rate limiting 5 times across different projects. Spent ~30 minutes 
each time figuring out retry logic. No existing skill documents this 
common pattern.

## Evidence

- Tested on OpenRouter, Anthropic API, OpenAI API
- 100% success rate across 20 test scenarios
- Pattern aligns with HTTP RFC recommendations
- Prevents 429 errors with minimal latency addition

## Impact

Saves debugging time when integrating external APIs. Prevents rate limit 
errors that cause user-facing failures.

## Testing

Created test scenarios with intentional rate limiting. Verified backoff 
logic works correctly and doesn't retry indefinitely.
EOF
)"
```

### 6. PR Description Template

Use this structure for PR descriptions:

```markdown
## What

[Brief description of what's changing - 1-2 sentences]

## Why

[Problem encountered / gap found / improvement opportunity]
[Why this change helps]

## Evidence

[How you validated this is better]
[Testing you did]
[Sources you referenced]

## Impact  

[Who benefits from this change]
[What situations this helps]
[What it prevents or enables]

## Testing

[How you verified changes work]
[Examples you tried]
[Edge cases you considered]

## Related

[Link to related skills if relevant]
[Reference forum discussions or docs if applicable]
```

### 7. Respond to Feedback

**Reviewers may:**
- Ask clarifying questions
- Suggest alternative approaches
- Request additional examples
- Point out edge cases
- Propose refinements

**How to respond:**
- Address feedback constructively
- Make requested changes if they improve the contribution
- Explain your reasoning if you disagree
- Iterate based on discussion
- Thank reviewers for their time

**Making changes based on feedback:**
```bash
# Make updates to files
git add path/to/files
git commit -m "Address review feedback: Add edge case examples"
git push
```

Changes automatically appear in the PR.

### 8. Merge After Approval

**Once approved:**
- Reviewer or maintainer will merge PR
- Changes go into main branch
- Your contribution is now part of the repository

**After merge:**
- Your branch can be deleted
- PR remains in history with full discussion
- Changes available to all agents

## PR Best Practices

### Keep PRs Focused

**Good - Single focused improvement:**
```
PR: "Add memory_insert safety warning to concurrency section"
Changes: 1 file, clear improvement, easy to review
```

**Bad - Multiple unrelated changes:**
```
PR: "Update multiple skills"
Changes: 5 files across 3 different skills, unclear theme
Better: Split into 3 separate PRs
```

### Write Clear PR Titles

**Good titles:**
- "Add exponential backoff pattern to API integration"
- "Fix incorrect memory_rethink guidance in concurrency section"
- "Clarify model selection criteria in letta-agent-designer"

**Bad titles:**
- "Updates" 
- "Fix stuff"
- "Improvements"

### Provide Context in Description

**Reviewers need to understand:**
- What you're changing
- Why you're changing it
- How you validated it's better
- What impact it has

**Don't assume context is obvious.** Even if it's clear to you, reviewers need the full picture.

### Be Open to Feedback

**Remember:**
- Reviewers want to help improve the contribution
- They may have insights you missed
- Discussion leads to better outcomes
- Iterate, don't defend

**If feedback suggests major changes:**
- Consider if they're right
- Discuss alternatives
- Be willing to withdraw PR and refine offline if needed

### Respond Promptly

**When reviewers give feedback:**
- Respond within reasonable time (hours/days, not weeks)
- If you need time, acknowledge and set expectations
- Keep discussion moving forward

## Common PR Scenarios

### Scenario 1: Minor Update to Existing Skill

```bash
# Create branch
git checkout -b fix/memory-concurrency-warning

# Edit file
# ... make changes ...

# Commit
git add ai/agents/letta/letta-memory-architect/SKILL.md
git commit -m "Make concurrent write warning more prominent

Data loss occurred when following guidance. Warning existed but 
wasn't prominent enough. Moving to top of section with clear example."

# Push and create PR
git push -u origin fix/memory-concurrency-warning
gh pr create --title "Emphasize memory_insert for concurrent writes" --body "..."
```

### Scenario 2: New Skill Addition

```bash
# Create branch
git checkout -b add/api-rate-limiting

# Create skill structure
mkdir -p ai/models/api-rate-limiting
# ... create SKILL.md and references ...

# Update README
# ... add skill to list ...

# Commit
git add ai/models/api-rate-limiting README.md
git commit -m "Add API rate limiting skill

Covers exponential backoff, jitter, retry strategies for HTTP APIs.
Common pattern not previously documented."

# Push and create PR
git push -u origin add/api-rate-limiting
gh pr create --title "Add API rate limiting patterns" --body "..."
```

### Scenario 3: Major Restructuring

```bash
# Create branch  
git checkout -b refactor/memory-architecture-split

# Make changes across multiple files
# ... restructure skill ...

# Commit incrementally
git add ai/agents/letta/letta-memory-architect/SKILL.md
git commit -m "Split memory types into separate reference file"

git add letta/agent-development/references/memory-architecture.md
git commit -m "Create memory-types reference with detailed comparison"

# Push and create PR
git push -u origin refactor/memory-architecture-split
gh pr create --title "Restructure memory-architect for clarity" --body "..."
```

**Note:** For major changes, consider discussing approach before doing work. Open issue or forum thread to validate direction first.

## PR Review Criteria

**Reviewers will check:**

**Correctness:**
- Is information accurate?
- Are examples valid?
- Do recommendations align with best practices?

**Clarity:**
- Is writing clear and concise?
- Are instructions easy to follow?
- Are examples helpful?

**Completeness:**
- Are edge cases addressed?
- Are tradeoffs mentioned?
- Is context sufficient?

**Impact:**
- Does this help others?
- Is evidence strong enough?
- Is improvement meaningful?

**Structure:**
- Does it fit repository organization?
- Is formatting consistent?
- Are references correct?

## After Your PR is Merged

**Your contribution is now part of the living knowledge base.**

**Next steps:**
- Watch for how others use/build on your contribution
- Be open to future improvements to what you added
- Note if your contribution helps in future tasks
- Continue contributing as you learn more

**If you discover issues with your own contribution:**
- Open another PR to improve it
- Self-correction is encouraged
- Knowledge base should always improve

## Getting Help

**If stuck on PR process:**
- Open issue asking for help
- Reference this workflow
- Describe where you're stuck

**If unsure about contribution:**
- Open issue describing proposed change
- Ask for feedback before doing work
- Discuss in Letta forum

**If disagreement with reviewer:**
- Discuss respectfully
- Explain your perspective with evidence
- Be open to compromise
- Escalate to maintainers if needed

## Summary

**Key principles:**
1. Always use feature branches, never commit directly to main
2. Write clear PR descriptions with evidence
3. Keep PRs focused on single improvement
4. Respond constructively to feedback
5. Iterate based on discussion
6. Merge after approval

**Remember:** PRs aren't just bureaucracy - they're how we maintain quality and build shared understanding in our collective knowledge base.


============================================================
END FILE: .fleet/letta/skills/meta/skill-learning/references/pr-workflow.md
============================================================

============================================================
FILE: .fleet/letta/skills/meta/skill-learning/references/progressive-disclosure-research.md
============================================================

# Progressive Disclosure at Scale: Operating Large Skill Libraries

**Focus:** Practical patterns for managing 100-1000+ skills  
**Date:** December 18, 2025

---

## Executive Summary

Progressive disclosure enables agents to work with arbitrarily large skill libraries by loading information in stages: metadata → instructions → resources. This report focuses on practical implementation patterns for systems with 100+ skills, where naive approaches fail.

**Critical insight:** At scale, you're building a discovery system, not a documentation library.

**Performance data:** 36.8% improvement with dynamic loading across 200+ tasks (Letta ContextBench). Discovery scales O(log n), browsing scales O(n) and fails beyond ~50 skills.

---

## The Scaling Wall

### When Flat Structures Break

**10-50 skills:** Flat list in memory block works fine  
**50-100 skills:** Memory block gets crowded, discovery slower but functional  
**100-500 skills:** Flat structure at breaking point, need better organization  
**500+ skills:** Flat structure fails, hierarchical organization required

**The problem at 100+ skills:**
```
Memory block contains:
- 100 skills × 100 words metadata = 10,000 words
- Approaches context window limits
- Agent spends significant time scanning metadata
- Discovery precision drops (too many similar-sounding skills)
```

**Key metrics from production (100+ skill systems):**
- Discovery time: 2.5s → 8.3s (3.3x slower)
- False positives: 3.1% → 18.7% (agents load wrong skills)
- Memory overhead: 15% → 45% of context window
- Agent confusion: More time selecting skills than executing tasks

---

## Three-Tier Architecture for Scale

### Tier 1: Discovery Metadata (Always Loaded)

**Challenge:** How do you fit 1000 skills in memory without overwhelming agents?

**Solution: Hierarchical metadata with progressive drilling**

```yaml
# Top-level categories (always in memory)
letta/          - Letta product ecosystem
  ├─ agents/    - Agent development patterns
  ├─ sdks/      - SDK integrations
  └─ ops/       - Operations and deployment

tools/          - External tool integrations
  ├─ web/       - Web scraping, testing
  ├─ data/      - Data processing
  └─ ml/        - Machine learning tools

# Skill metadata (loaded per category)
letta/agents/
  - memory-architecture: Designing memory blocks...
  - tool-patterns: Tool selection and configuration...
  - multi-agent: Coordinating multiple agents...
```

**Implementation pattern:**
1. **Categories in memory block** (always loaded): ~10-20 categories, <1k words
2. **Skill list on demand** (when category selected): 10-50 skills per category
3. **Full skill** (when specific skill selected): Complete SKILL.md

**Example agent flow:**
```
Task: "Set up memory blocks for a customer support agent"

Agent reasoning:
1. Scan categories → "letta/" relevant
2. Load letta/ skill list → See "memory-architecture" 
3. Load memory-architecture SKILL.md
4. Navigate to references/customer-support-patterns.md
5. Execute task
```

### Tier 2: Category Organization

**Practical category structure for 500+ skills:**

```
<domain>/
├── <subdomain>/
│   ├── CATEGORY.md         # Overview, when to use these skills
│   ├── skill-1/
│   ├── skill-2/
│   └── skill-3/
└── README.md               # Domain overview
```

**CATEGORY.md template:**
```markdown
# Agent Development Skills

Use these skills when building, configuring, or debugging Letta agents.

## Available Skills

- **memory-architecture** - Use when designing memory blocks and data flow
- **tool-patterns** - Use when selecting and configuring tools
- **model-selection** - Use when choosing LLMs for specific workloads
- **multi-agent** - Use when coordinating multiple agents

## Quick Decision Tree

Building new agent? → Start with memory-architecture
Debugging tool issues? → See tool-patterns
Performance problems? → Check model-selection
Multiple agents? → Read multi-agent
```

**Key insight:** CATEGORY.md is metadata for a group of skills. Agents load it to decide which skill to load next.

### Tier 3: Skill Resources

No change at large scale - skills still have references/, scripts/, assets/. But organization becomes more critical:

**At 500+ skills, standardize reference structure:**
```
skill-name/
├── SKILL.md                    # Always same structure
├── references/
│   ├── README.md               # Index (required at scale)
│   ├── quick-start.md          # Standard name
│   ├── common-patterns.md      # Standard name
│   ├── api-reference.md        # Standard name
│   └── troubleshooting.md      # Standard name
├── scripts/
│   └── README.md               # What each script does
└── assets/
    └── README.md               # What each asset is for
```

**Why standardization matters:** With 500+ skills, agents learn the pattern. They know "quick-start.md exists in references/" and can navigate directly.

---

## Discovery Mechanisms at Scale

### Problem: Query-based Discovery

**Simple query (works to ~100 skills):**
```
Agent: "I need to test a web application"
System: Scans all skill metadata
Result: Finds "webapp-testing"
```

**Complex query at scale (500+ skills):**
```
Agent: "I need to test a web application"
System: 
  1. Identify domain: tools/web/
  2. Load tools/web/CATEGORY.md
  3. Scan category skills
  4. Find webapp-testing
Result: 3 steps instead of 1, but scales to 10,000 skills
```

### Hierarchical Discovery Pattern

**Implementation:**

```typescript
interface SkillDiscovery {
  // Phase 1: Category selection
  selectCategories(query: string): Category[]  // From memory block
  
  // Phase 2: Skill selection
  selectSkills(category: Category, query: string): Skill[]  // Load CATEGORY.md
  
  // Phase 3: Skill loading
  loadSkill(skill: Skill): SkillContent  // Load SKILL.md
}

// Agent flow:
1. categories = selectCategories("test web application")
   // Returns: [tools/web/, tools/testing/]

2. skills = selectSkills(tools/web/, "test web application")  
   // Loads tools/web/CATEGORY.md
   // Returns: [webapp-testing, web-scraping]

3. content = loadSkill(webapp-testing)
   // Loads full SKILL.md
```

**Complexity:** O(log n) - each step narrows search space

**Alternative: Flat discovery with embeddings** (more complex but faster):
- Pre-compute embeddings for all skill metadata
- Query embedding matches against skill embeddings
- Load top-k skills directly
- Requires embedding infrastructure

---

## Metadata Design for Discoverability

### The Metadata Challenge

**At 10 skills:** Vague metadata acceptable ("Python utilities")  
**At 100 skills:** Need specificity ("Python async patterns for web services")  
**At 1000 skills:** Need extreme precision + disambiguation

**Critical elements:**

1. **Trigger conditions** (what query patterns match this skill)
2. **Scope definition** (what's included, what's not)
3. **Differentiation** (how this differs from similar skills)

**Template for 100+ skill systems:**
```yaml
name: skill-identifier
category: domain/subdomain
description: |
  Use when [specific trigger with keywords]. 
  Provides [concrete capabilities].
  Covers [explicit scope].
  Does NOT cover [common misconceptions].
  See also: [related-skill] for [alternative use case].
keywords: [keyword1, keyword2, keyword3]  # Optional but helpful at scale
```

**Example - Good metadata at scale:**
```yaml
name: playwright-web-testing
category: tools/web
description: |
  Use when testing web UIs in a real browser using Playwright.
  Provides page navigation, element interaction, screenshot capture, network 
  interception. Covers both headless and headed testing.
  Does NOT cover API testing (see rest-api-testing) or load testing 
  (see load-testing).
  See also: puppeteer-testing for Chrome-only testing.
keywords: [playwright, browser, testing, ui, e2e, screenshots]
```

**Example - Poor metadata at scale:**
```yaml
name: web-tool
description: Helpful utilities for web development
# Issues: Vague trigger, unclear scope, no differentiation
```

### Disambiguation at Scale

**Problem:** Multiple similar skills, agent picks wrong one

**Example collision:**
- `api-testing` - REST API testing with requests library
- `api-integration` - Integrating third-party APIs
- `api-design` - Designing RESTful APIs
- `graphql-testing` - Testing GraphQL APIs

**Solution: Explicit differentiation in metadata**
```yaml
name: api-testing
description: |
  Use when testing REST APIs (not GraphQL - see graphql-testing).
  Provides HTTP request testing, response validation, mock servers.
  For API integration patterns, see api-integration.
  For API design guidance, see api-design.
```

---

## Organization Strategies

### Categorization Approaches

**By domain (recommended for 100-500 skills):**
```
letta/          - Letta ecosystem
tools/          - General tools
templates/      - Reusable templates
domain-X/       - Specific domains
```

**By function (works well for specialized systems):**
```
development/    - Building things
operations/     - Running things
analysis/       - Understanding things
```

**By technology (good for tech-specific libraries):**
```
python/         - Python skills
typescript/     - TypeScript skills
databases/      - Database skills
```

**Hybrid (necessary at 500+ skills):**
```
letta/
  ├── agents/           # By function within domain
  ├── sdks/
  └── ops/

tools/
  ├── web/             # By technology within domain
  ├── data/
  └── ml/

templates/
  ├── frontend/        # By use case within domain
  ├── backend/
  └── docs/
```

### Category Size Guidelines

**Optimal category size:** 10-30 skills

**Too small (<5 skills):** Over-categorization, high navigation overhead  
**Too large (>50 skills):** Category doesn't help, still scanning too many options  

**Example of good sizing:**
```
tools/web/                    # 18 skills - good size
  - playwright-testing
  - puppeteer-testing
  - selenium-testing
  - web-scraping
  - browser-automation
  - ... (13 more)

tools/web/testing/            # Bad: Over-categorization
  - playwright/              # 3 skills
  - puppeteer/               # 2 skills
  - selenium/                # 4 skills
# Better to keep flat with good metadata
```

---

## Memory Block Management

### The Memory Block Scaling Problem

**Simple approach (fails at 100+ skills):**
```
skills memory block:
[metadata for all 1000 skills]
# 100,000+ words, dominates context window
```

**Hierarchical approach (scales to 10,000 skills):**
```
skills memory block:
[category list - 20 categories × 50 words = 1,000 words]

Agent loads on demand:
- CATEGORY.md for relevant categories
- SKILL.md for specific skills
```

### Dynamic Memory Block Updates

**Pattern: Update memory block as navigation happens**

```typescript
// Initial memory block: Just categories
memoryBlock.skills = formatCategories(categories);

// Agent identifies relevant category
loadCategory('tools/web/');
memoryBlock.skills += formatCategorySkills('tools/web/');

// Agent selects specific skill
loadSkill('webapp-testing');
memoryBlock.loaded_skills += skillContent;

// Agent unloads when done
memoryBlock.loaded_skills = removeSkill('webapp-testing');
```

**Key insight:** Memory block is dynamic, updated during task execution.

### Memory Block Structure at Scale

```yaml
# Always present
categories:
  - letta/ : Letta product ecosystem (agents, SDKs, ops)
  - tools/ : External tool integrations (web, data, ml)
  - templates/ : Reusable templates and patterns
  
# Loaded on demand (category exploration)
current_category: tools/web/
category_skills:
  - webapp-testing: Testing web UIs with Playwright...
  - web-scraping: Extracting data from websites...
  - rest-api-testing: Testing REST APIs with requests...
  
# Loaded on demand (active work)  
loaded_skills:
  - webapp-testing: [full SKILL.md content]
```

---

## Tooling and Infrastructure

### Discovery Tools for Large Libraries

**Search/query tool (essential at 100+ skills):**
```typescript
interface SkillSearch {
  // Keyword search across all metadata
  search(query: string): Skill[]
  
  // Category-scoped search
  searchCategory(category: string, query: string): Skill[]
  
  // Semantic search (requires embeddings)
  semanticSearch(query: string, topK: number): Skill[]
}
```

**Usage in agent reasoning:**
```
Agent: I need to test a web application
Tool call: search("test web application")
Result: [webapp-testing, web-scraping, api-testing]
Agent: Load webapp-testing
```

### Skill Registry (500+ skills)

**Problem:** Need metadata index without loading all files

**Solution: Skill registry file**
```json
{
  "skills": [
    {
      "id": "webapp-testing",
      "category": "tools/web",
      "path": "tools/web/webapp-testing",
      "name": "Web Application Testing",
      "description": "Testing web UIs with Playwright...",
      "keywords": ["playwright", "testing", "browser"],
      "updated": "2025-12-15"
    }
  ],
  "categories": [
    {
      "id": "tools/web",
      "name": "Web Tools",
      "description": "Tools for web development and testing",
      "skill_count": 18
    }
  ]
}
```

**Benefits:**
- Single file to load for all metadata
- Fast search/filter operations
- Versioning and update tracking
- Can generate dynamically from skill files

**Trade-off:** Registry must stay in sync with actual skills (build step)

---

## Practical Implementation Patterns

### Pattern 1: Lazy Category Loading

```typescript
