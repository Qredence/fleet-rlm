<!-- Chunk 915: bytes 3197922-3243841, type=class -->
class FormatValidationResults(dspy.Signature):
    """Format validation results for human-readable display."""
```

**Inputs:**
| Field | Type | Description |
|-------|------|-------------|
| `validation_report` | `str` | JSON validation report |
| `skill_content` | `str` | The skill content that was validated |

**Outputs:**
| Field | Type | Description |
|-------|------|-------------|
| `formatted_report` | `str` | Human-readable report |
| `critical_issues` | `list[ValidationCheckItem]` | Critical issues that MUST be fixed |
| `warnings` | `list[ValidationCheckItem]` | Warnings that SHOULD be addressed |
| `auto_fixable` | `bool` | True if all issues can be auto-fixed |

**Used by**: `ValidationFormatterModule` with `dspy.Predict`

---

## Pydantic Models

All signatures use Pydantic models for type-safe outputs. Key models are defined in `src/skill_fleet/core/config/models.py`:

- **`TaskIntent`**: Purpose, problem_statement, target_audience, value_proposition
- **`SkillMetadata`**: name, description, taxonomy_path, tags, version, type
- **`DependencyAnalysis`**: required, recommended, conflicts
- **`ValidationReport`**: passed, checks, issues, warnings
- **`ClarifyingQuestion`**: question, rationale, suggested_answers

## See Also

- **[Modules Documentation](modules.md)** - How signatures are used in modules
- **[Programs Documentation](programs.md)** - How modules are orchestrated
- **[DSPy Overview](index.md)** - Architecture and concepts


============================================================
END FILE: docs/dspy/signatures.md
============================================================

============================================================
FILE: docs/getting-started/MLFLOW_SETUP.md
============================================================

# MLflow Setup for DSPy Autologging

**Last Updated**: 2026-01-25

## Quick Start

### 1. Start MLflow UI Server

```bash
# Option 1: Simple start (default)
./scripts/start-mlflow.sh

# Option 2: External access (port 5001, recommended)
./scripts/start-mlflow-public.sh

# Option 3: Custom configuration
uv run mlflow ui \
    --host localhost \
    --port 5001 \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./mlartifacts
```

**Note:** Port 5000 is often used by macOS AirPlay/ControlCenter. Use port 5001 if needed.

### 2. Open MLflow UI

Navigate to: **http://localhost:5001**

**Note:** If port 5001 doesn't work, check:
```bash
# See what's using port 5001
lsof -i :5001

# Try alternative ports
uv run mlflow ui --port 5002
```

### 3. Test DSPy Autologging

```bash
# Run the test script
uv run python scripts/test_mlflow_tracking.py

# Check the run in MLflow UI
# Experiment: "skill-fleet-test"
# Run: "test-dspy-tracking"
```

---

## What Gets Tracked

With `mlflow.dspy.autolog()` enabled (DSPy 3.1.2+), MLflow automatically tracks:

### DSPy Operations
- **Module calls**: Every DSPy module invocation
- **Signatures**: Input/output for each signature
- **LM calls**: Language model requests and responses
- **Parameters**: Model settings, temperature, max_tokens
- **Metrics**: Performance metrics and evaluation scores

### Hierarchical Runs (NEW)
Skill creation workflows now use **parent-child run hierarchies**:

```
skill_creation_job_abc123 (parent)
‚îú‚îÄ‚îÄ understanding (child)
‚îÇ   ‚îú‚îÄ‚îÄ LM calls: 5
‚îÇ   ‚îú‚îÄ‚îÄ Tokens: 2,340
‚îÇ   ‚îî‚îÄ‚îÄ Duration: 12.3s
‚îú‚îÄ‚îÄ generation (child)
‚îÇ   ‚îú‚îÄ‚îÄ LM calls: 3
‚îÇ   ‚îú‚îÄ‚îÄ Tokens: 1,890
‚îÇ   ‚îî‚îÄ‚îÄ Duration: 8.7s
‚îî‚îÄ‚îÄ validation (child)
    ‚îú‚îÄ‚îÄ Quality score: 0.85
    ‚îú‚îÄ‚îÄ Tokens: 456
    ‚îî‚îÄ‚îÄ Duration: 3.2s
```

### Tag Organization (ENHANCED)
Runs are automatically tagged with:

| Tag | Description | Example |
|-----|-------------|---------|
| `user_id` | User who initiated the job | `user_123` |
| `job_id` | Unique job identifier | `job_abc123` |
| `skill_type` | Type of skill being created | `guide`, `tool_integration` |
| `model` | LLM model used | `gemini-3-flash` |
| `phase` | Current workflow phase | `understanding`, `generation`, `validation` |
| `service` | Service creating the run | `SkillService` |

### Complete Artifact Logging (ENHANCED)
All skill artifacts are automatically logged:

| Artifact | Description | Location |
|----------|-------------|----------|
| `skill_content.md` | Generated SKILL.md content | Artifacts/ |
| `metadata.json` | Skill metadata | Artifacts/ |
| `validation_report.json` | Quality validation results | Artifacts/ |
| `conversation_history.json` | HITL conversation transcript | Artifacts/ |

### LM Usage Tracking (DSPy 3.1.2)
DSPy 3.1.2 provides complete LM usage tracking:

```python
# Automatically tracked metrics
mlflow.log_metric("lm_calls", 15)           # Total LM calls
mlflow.log_metric("lm_tokens_total", 5432)  # Total tokens used
mlflow.log_metric("lm_tokens_input", 2341)  # Input tokens
mlflow.log_metric("lm_tokens_output", 3091) # Output tokens
mlflow.log_metric("lm_duration", 45.2)      # Total LM time (seconds)
```

### Optimization Workflows
- **MIPROv2**: Prompt optimization iterations
- **GEPA**: Reflection cycles and improvements
- **BootstrapFewShot**: Few-shot example selection
- **Evaluation**: Metric scores and comparisons

### Custom Tags
You can add custom tags for filtering:
```python
with MLflowContext(
    run_name="skill-creation",
    tags={"phase": "generation", "skill_type": "python"}
):
    result = program.forward(...)
```

---

## Using MLflow During Development

### Track Skill Creation Workflows (ENHANCED)

```python
from skill_fleet.services.monitoring import MLflowContext, setup_dspy_autologging

# Setup at application startup
setup_dspy_autologging(experiment_name="skill-creation")

# Parent run for the entire skill creation job
with mlflow.start_run(run_name=f"skill_creation_{job_id}") as parent_run:
    # Parent-level tags
    mlflow.set_tag("user_id", user_id)
    mlflow.set_tag("job_id", job_id)
    mlflow.set_tag("service", "SkillService")

    # Child run for Phase 1: Understanding
    with mlflow.start_run(run_name="understanding", nested=True):
        mlflow.set_tag("phase", "understanding")
        result = understanding_program.forward(...)
        # LM metrics auto-logged by DSPy 3.1.2

    # Child run for Phase 2: Generation
    with mlflow.start_run(run_name="generation", nested=True):
        mlflow.set_tag("phase", "generation")
        result = generation_program.forward(...)
        # Artifacts auto-logged
        mlflow.log_text(content, "skill_content.md")

    # Child run for Phase 3: Validation
    with mlflow.start_run(run_name="validation", nested=True):
        mlflow.set_tag("phase", "validation")
        report = validate_skill(result.skill)
        mlflow.log_dict(report.to_dict(), "validation_report.json")
        mlflow.log_metric("quality_score", report.score)
```

### Compare Optimizers

```python
# MIPROv2
with MLflowContext(run_name="miprov2-run", tags={"optimizer": "miprov2"}):
    optimized = teleprompter.compile(program, trainset)

# GEPA
with MLflowContext(run_name="gepa-run", tags={"optimizer": "gepa"}):
    optimized = teleprompter.compile(program, trainset)

# Compare in MLflow UI side-by-side
```

### Track Evaluation Metrics

```python
from dspy.evaluate import Evaluate

# Metrics are automatically logged with autologging
evaluator = Evaluate(
    metric=your_metric,
    devset=dev_set,
    num_threads=4,
)

evaluator(program)
# All results automatically tracked in MLflow!
```

---

## MLflow UI Features

### Experiments Page
- View all experiments
- Compare runs across experiments
- Filter by tags and parameters

### Run Details
- **Parameters**: LLM settings, model info
- **Metrics**: DSPy performance scores
- **Artifacts**: Saved models, prompts, plots
- **Traces**: Detailed execution timeline

### Comparing Runs
- Select multiple runs
- View metrics side-by-side
- Compare parameters
- Visualize performance differences

---

## Troubleshooting

### MLflow UI Won't Start
```bash
# Check if port 5000 is in use
lsof -i :5000

# Kill existing MLflow process
killall mlflow

# Use different port
uv run mlflow ui --port 5001
```

### Can't See Runs in UI
```bash
# Verify backend store
ls -la mlflow.db

# Check if runs exist
sqlite3 mlflow.db "SELECT run_uuid, status FROM runs ORDER BY start_time DESC LIMIT 5;"

# Reset (WARNING: deletes all data)
rm mlflow.db
```

### DSPy Autologging Not Working
```python
# Verify DSPy version
uv run python -c "import dspy; print(dspy.__version__)"
# Should be 3.1.2+

# Check if MLflow autologging is enabled
import mlflow
mlflow.dspy.autolog()  # Should return None (no error)

# Verify experiment
import mlflow
mlflow.set_experiment("test")
print("Experiment set successfully")
```

---

## Advanced Configuration

### Remote MLflow Server

```python
# Setup for remote tracking
setup_dspy_autologging(
    tracking_uri="http://your-mlflow-server:5000",
    experiment_name="skill-creation-prod"
)
```

### Custom Artifact Location

```python
# Run with custom artifact root
with MLflowContext(
    run_name="my-run",
    tags={"artifact_type": "model"}
):
    # Artifacts saved to: ./mlartifacts/<run_uuid>/
    result = program.forward(...)
```

### Environment Variables

```bash
# MLflow configuration
export MLFLOW_TRACKING_URI=http://localhost:5000
export MLFLOW_EXPERIMENT_NAME=skill-creation
export MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
```

---

## Best Practices

1. **One Experiment Per Project**: Use descriptive experiment names (e.g., "skill-creation", "optimization")

2. **Descriptive Run Names**: Include what you're testing
   ```python
   run_name="miprov2-skill-creation-python-decorators"
   ```

3. **Use Tags for Filtering**:
   ```python
   tags={
       "phase": "generation",
       "optimizer": "miprov2",
       "model": "gemini-3-flash"
   }
   ```

4. **Log Parameters Manually** (if needed):
   ```python
   mlflow.log_param("temperature", 0.7)
   mlflow.log_param("max_tokens", 4096)
   ```

5. **Log Metrics Manually** (if needed):
   ```python
   mlflow.log_metric("quality_score", 0.85)
   mlflow.log_metric("validation_passed", 1)
   ```

---

## Files Created

- ‚úÖ `scripts/start-mlflow.sh`: MLflow UI starter script
- ‚úÖ `scripts/test_mlflow_tracking.py`: DSPy autologging test
- ‚úÖ `src/skill_fleet/services/monitoring/mlflow_setup.py`: MLflow integration utilities

---

## Next Steps

1. **Start MLflow UI**: `./scripts/start-mlflow.sh`
2. **Run skill creation**: Your DSPy workflows will be automatically tracked
3. **View results**: Check http://localhost:5000
4. **Compare runs**: Use MLflow UI to compare different configurations

Happy tracking! üöÄ


============================================================
END FILE: docs/getting-started/MLFLOW_SETUP.md
============================================================

============================================================
FILE: docs/getting-started/STREAMING_QUICKSTART.md
============================================================

# Streaming TUI Quick Start Guide

Get real-time streaming responses with thinking/reasoning display in 5 minutes.

## ‚úÖ Prerequisites

- Python 3.12+
- Node.js 18+ (for TUI)
- npm 9+ or yarn 1.22+
- Skills Fleet API running (`uv run skill-fleet serve`)

## üöÄ Quick Start

### Step 1: Install TUI Dependencies

```bash
cd cli/tui
npm install
npm run build
```

### Step 2: Start the API Server

```bash
# In one terminal
uv run skill-fleet serve
```

### Step 3: Launch the Streaming Chat TUI

```bash
# In another terminal
uv run skill-fleet chat
```

**What you'll see:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üöÄ Skills Fleet TUI                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ Assistant: Welcome to Skills Fleet!    ‚îÇ
‚îÇ You can type requests and see my       ‚îÇ
‚îÇ thinking process in real-time.         ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ You: create a JSON parsing skill      ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ üí≠ [thought] Analyzing user message... ‚îÇ
‚îÇ ü§î [reasoning] Looking for keywords... ‚îÇ
‚îÇ ‚öôÔ∏è  [internal] Running classification  ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ > _                                     ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìù Examples to Try

### 1. Create a Skill

```
You: create a skill for JSON parsing
```

**Expected Output:**
- Shows thinking process: "Analyzing user message..." ‚Üí "Looking for keywords..." ‚Üí "Running classification..."
- Streams response chunks as they're generated
- Shows suggested next steps

### 2. Run Optimization

```
You: /optimize reflection_metrics trainset_v4.json
```

**Expected Output:**
- Thinking: "Setting up Reflection Metrics optimizer..."
- Response: "Starting optimization job... ID: xyz-123"
- Suggested next: "/status xyz-123"

### 3. List Skills

```
You: /list --filter python
```

**Expected Output:**
- Thinking: "Querying skill taxonomy..."
- Response: Tabulated list of Python skills with metadata

## üéØ Key Features

### Real-Time Thinking Display

Each thinking chunk is labeled with an emoji and type:
- üí≠ **thought** - Initial observation
- ü§î **reasoning** - Logical deduction
- ‚öôÔ∏è **internal** - Implementation detail
- ‚ñ∂Ô∏è **step** - Multi-step process

### Agentic Suggestions

Based on your message, the assistant suggests next steps:

```
üí° Suggested next steps:
‚Ä¢ /list --filter python
‚Ä¢ /validate skills/python/async
‚Ä¢ /promote <job_id>
```

### Tab Navigation

```
Commands:
- Tab / Ctrl+Tab : Switch tabs
- Alt+1-4 : Jump to specific tab
- ? : Show help
- Ctrl+C : Exit
```

Current tabs:
- **üí¨ Chat** - Conversational interface with streaming
- **üìö Skills** - Browse and manage skills (coming soon)
- **‚öôÔ∏è Jobs** - Monitor running optimization jobs (coming soon)
- **üöÄ Optimize** - Configure and run optimizers (coming soon)

## üìä Comparing Stream vs Sync Responses

### Streaming (Real-time)

```bash
# HTTP Request
POST /api/v1/chat/stream
Content-Type: application/json

{"message": "optimize my skill"}

# Response (Server-Sent Events)
event: thinking
data: {"type": "thought", "content": "Analyzing request...", "step": 1}

event: thinking
data: {"type": "reasoning", "content": "User wants to optimize...", "step": 2}

event: response
data: {"type": "response", "content": "I'll help you optimize your skill..."}

event: complete
data:
```

### Synchronous (Fallback)

```bash
# HTTP Request
POST /api/v1/chat/sync
Content-Type: application/json

{"message": "optimize my skill"}

# Response (JSON)
{
  "message": "optimize my skill",
  "thinking": [
    "Analyzing request...",
    "User wants to optimize...",
    "Setting up optimizer..."
  ],
  "response": "I'll help you optimize your skill...",
  "thinking_summary": "Full thinking process shown above"
}
```

## üîß Troubleshooting

### TUI doesn't launch

**Solution**: Fall back to simple terminal chat:
```bash
uv run skill-fleet chat --no-tui
```

### Streaming not working

**Check**:
1. API server is running (`uv run skill-fleet serve`)
2. Node.js is installed (`node --version` should be 18+)
3. API URL is correct: `http://localhost:8000`

**Test with curl**:
```bash
curl -N -X POST http://localhost:8000/api/v1/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "hello"}'
```

### Slow response

**Likely cause**: LM cold start or network latency

**Check**:
- API logs for errors
- Network connectivity
- Available RAM on API server

## üìö Advanced Usage

### Custom Context

Pass conversation context for better responses:

```
You: create a skill for my chatbot with context from previous skills
```

The system automatically tracks:
- Previous skills created
- Running jobs
- Training data

### Command Syntax

Explicit commands (override agentic inference):

```
/optimize reflection_metrics trainset_v4.json --fast
/list --filter python --sort recent
/validate skills/python/async
/promote job-xyz-123
/status job-xyz-123
/help
```

### Environment Variables

```bash
# Custom API URL
export SKILL_FLEET_API_URL=http://api.example.com:8000

# Custom user ID
export SKILL_FLEET_USER_ID=john.doe

# Launch TUI
uv run skill-fleet chat
```

## üéì Under the Hood

1. **User Message** ‚Üí Sent to `/api/v1/chat/stream` via HTTP POST
2. **Intent Parsing** ‚Üí StreamingIntentParser yields thinking chunks
3. **Response Generation** ‚Üí StreamingAssistant generates response chunks
4. **SSE Streaming** ‚Üí Events sent as they're generated
5. **TUI Rendering** ‚Üí Ink.js updates display in real-time

See [STREAMING_ARCHITECTURE.md](./STREAMING_ARCHITECTURE.md) for details.

## üìñ Learn More

- [Streaming Architecture](./STREAMING_ARCHITECTURE.md) - Technical deep-dive
- [DSPy Optimization Guide](./OPTIMIZATION_GUIDE.md) - Optimization workflows
- [API Docs](http://localhost:8000/docs) - Interactive API documentation

## üêõ Report Issues

Found a bug? Test streaming failing?

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
uv run skill-fleet chat

# Check logs in API server terminal
```

## ‚ú® What's Next

- [ ] Skills tab: Browse & manage skills
- [ ] Jobs tab: Monitor optimizations
- [ ] Optimization tab: Configure/run optimizers
- [ ] Reflection Metrics: Integration & highlighting
- [ ] Persistent conversation history
- [ ] Theme customization


============================================================
END FILE: docs/getting-started/STREAMING_QUICKSTART.md
============================================================

============================================================
FILE: docs/getting-started/index.md
============================================================

# Skill Fleet ‚Äî Getting Started Guide

## 1. Overview

Skill Fleet is a hierarchical, DSPy-powered capability framework that lets agents load or generate skills on-demand. Instead of bloated monolithic prompts, it keeps skills modular, searchable, and compliant with the [agentskills.io](https://agentskills.io) specification.

This guide shows how to get started with the CLI and API. For detailed documentation on specific topics, see:

- **[DSPy Documentation](../dspy/)** - 3-phase workflow, signatures, modules, programs
- **[API Documentation](../api/)** - REST API endpoints, schemas, jobs
- **[CLI Documentation](../cli/)** - Command reference, interactive chat
- **[LLM Configuration](../llm/)** - Provider setup, task-specific models
- **[HITL System](../hitl/)** - Human-in-the-Loop interactions

## 2. Prerequisites

1. **Python 3.12+** and the [`uv`](https://github.com/astral-sh/uv) package manager.
2. **Bun** (optional but required for the TypeScript/React TUI).
3. Valid API keys exported through `.env` (`GOOGLE_API_KEY`, optional provider keys, DSPy overrides).
4. The `skills/` directory stores the taxonomy‚Äîkeep it versioned and backed up.

## 3. Installation & Environment

```bash
# Clone
# from repo root
git clone https://github.com/Qredence/skill-fleet.git
cd skill-fleet

# Python deps
uv sync --group dev
# Optional TUI deps
bun install

# Create environment file
cp .env.example .env
# Edit .env to set required API keys:
# GOOGLE_API_KEY, DSPY_CACHEDIR, DSPY_TEMPERATURE, etc.
```

## 4. Core User Workflows

### 4.1 Start the API Server

```bash
uv run skill-fleet serve             # production mode
uv run skill-fleet serve --reload    # dev mode with autoreload
```

The FastAPI server is the single source of truth: all CLI interactions (`create`, `chat`, `list`) call it. It configures DSPy on startup and exposes the job + HITL workflow at `/api/v2/skills/create` and `/api/v2/hitl/{job_id}`.

**API Versioning Note**: The main API is v2 (`/api/v2/*`), which is production-ready. An experimental v1 API exists for chat streaming (`/api/v1/chat/*`). See [API Versioning Guide](../api/MIGRATION_V1_TO_V2.md) for details.

### 4.2 Create a Skill (CLI)

```bash
uv run skill-fleet create "Describe the capability"
uv run skill-fleet create "..." --auto-approve
```

- Starts a background job that runs the SkillCreationProgram workflow orchestrator.
- Respond to HITL prompts (`clarify`, `confirm`, `preview`, `validate`) in the terminal.
- Job auto-saves to `skills/_drafts/<job_id>/` and writes `metadata.json` + `SKILL.md` (frontmatter + template guidance).

### 4.3 Interactive Chat Experience

```bash
uv run skill-fleet chat
uv run skill-fleet chat "Create a Docker best practices skill"
uv run skill-fleet chat --auto-approve
```

- Runs the same job + HITL endpoints but wraps them in a guided UI.
- Commands: `/help`, `/exit`. After each job, you‚Äôre prompted to start another.
- Use `--auto-approve` to skip prompts and let the job run to completion.

### 4.4 Validation & Migration

```bash
uv run skill-fleet validate skills/<path>
uv run skill-fleet migrate --dry-run
uv run skill-fleet migrate
```

- `validate` ensures agentskills.io frontmatter and structure are correct.
- `migrate` converts legacy skills to the current schema in `skills/`, use `--dry-run` first.

### 4.5 Generate XML or Analytics

```bash
uv run skill-fleet generate-xml -o available_skills.xml
uv run skill-fleet analytics --user-id default
```

- XML follows `<available_skills>` from agentskills.io for prompt injection.
- Analytics command summarizes usage and recommendations.

## 5. Templates & Compliance

- `config/templates/SKILL_md_template.md` describes the required YAML frontmatter and body structure (kebab-case `name`, concise `description`).
- `config/templates/metadata_template.json` shows the optional metadata fields (version, type, load priority, dependencies, capabilities).
- New skills should follow the template hints inserted into the DSPy generation instructions.

## 6. Troubleshooting & Tips

| Scenario | Tip |
| --- | --- |
| API unreachable | Ensure `uv run skill-fleet serve` is running and `SKILL_FLEET_API_URL` matches. |
| HITL prompt times out | Restart the job; state is in-memory. |
| Skill description too vague | Use `chat` to walk through clarifying questions (type `/help` to review the command list). |
| Need to see available skills | `uv run skill-fleet list` or inspect `skills/` manually. |

## 7. Quick Commands Cheat Sheet

```bash
# Start API
uv run skill-fleet serve

# Create skill
uv run skill-fleet create "task"

# Guided chat
uv run skill-fleet chat

# Validate/migrate
uv run skill-fleet validate skills/<path>
uv run skill-fleet migrate --dry-run

# Generate XML
uv run skill-fleet generate-xml -o available_skills.xml
```

## 8. Related Resources

- [AGENTS.md](../AGENTS.md) (working guide and tooling expectations)
- `docs/skill-creator-guide.md`, `docs/agentskills-compliance.md`
- `skills/` taxonomy for existing skill examples


============================================================
END FILE: docs/getting-started/index.md
============================================================

============================================================
FILE: docs/getting-started/skill-creation-guidelines.md
============================================================

# Skills Fleet: Skill Creation Guidelines

## 1. Purpose & Scope

### Why This Document Exists

This document provides **comprehensive, domain-agnostic guidelines** for creating skills in the Skills Fleet system. It bridges the gap between:

- **Theoretical architecture** (covered in `overview.md`)
- **Practical implementation** (this document)
- **CLI operations** (covered in `getting-started/index.md` and `cli-reference.md`)
- **DSPy workflow** (covered in `skill-creator-guide.md`)

### Who Should Use This

- **Skill Creators**: People using the skill-fleet system to create new skills
- **Maintainers**: People evolving the skill taxonomy and quality standards
- **AI Agents**: Systems that need to understand skill creation patterns
- **Reviewers**: People validating and approving new skills

### What Skills Are in This System

Skills are **first-class, versioned artifacts** stored in:

```
skills/
```

Each skill represents a discrete capability that can be:

- **Technical**: Programming languages, frameworks, tools
- **Domain**: Subject matter expertise (medical, legal, etc.)
- **Cognitive**: Thinking patterns and reasoning approaches
- **Tool**: Proficiency with specific software or platforms
- **MCP**: Model Context Protocol capabilities
- **Specialization**: Advanced or focused applications
- **Task Focus**: Problem-solving methodologies
- **Memory**: Memory management patterns

### Relationship to Other Documentation

| Document                           | Purpose                                   | Use When                            |
| ---------------------------------- | ----------------------------------------- | ----------------------------------- |
| `overview.md`                      | System architecture and taxonomy design   | Understanding the big picture       |
| `getting-started/index.md`         | Installation, CLI workflow, and templates | Getting started quickly             |
| `skill-creator-guide.md`           | 6-step DSPy workflow overview             | Understanding automated creation    |
| `**skill-creation-guidelines.md**` | **Practical creation guidelines**         | **Creating or modifying skills**    |
| `cli-reference.md`                 | Complete CLI command reference            | Looking up specific commands        |
| `agentskills-compliance.md`        | agentskills.io specification              | Ensuring cross-system compatibility |

---

## 2. Skill Philosophy & Principles

### Core Principles

#### 1. Skills as First-Class Artifacts

Skills are **stable, versioned entities** on disk, not ephemeral prompts. This means:

- Each skill has a unique `skill_id` (path-style)
- Each skill has semantic versioning (`1.0.0`)
- Each skill has evolution metadata tracking changes
- Skills are discoverable via taxonomy paths
- Skills are composable via dependencies

**Why?** Stability and auditability. You can trace when a skill was created, how it evolved, and what depends on it.

#### 2. Taxonomy-Driven Organization

Skills live in a **hierarchical, path-addressable taxonomy**:

```
technical_skills/programming/languages/python/fastapi
domain_knowledge/medical/terminology
tool_proficiency/version_control/git
```

**Why?**

- **Discoverability**: Predictable paths make skills easy to find
- **Semantic meaning**: Paths encode domain, category, and specificity
- **Composability**: Dependencies can reference paths directly
- **Scalability**: New branches can be added without restructuring

#### 3. Capability Granularity

Each skill should have **3-7 atomic capabilities**.

**Why 3-7?**

- **< 3**: Too narrow, consider merging with related skill
- **> 7**: Too broad, consider splitting into multiple skills
- **3-7**: Sweet spot for focus and comprehensiveness

Each capability must be:

- **Atomic**: Can be understood and tested independently
- **Cohesive**: Relates to a single responsibility
- **Testable**: Has clear input/output contracts
- **Cross-referencable**: Can be linked from other capabilities

#### 4. Production-Ready Patterns

Skills should emphasize **patterns that work in production**, not just examples that work in tutorials.

**What this means:**

- Document anti-patterns and silent failures
- Show both wrong (‚ùå) and right (‚úÖ) approaches
- Include error handling and edge cases
- Specify version requirements
- Provide performance considerations

**Why?** Code that "works locally" but breaks under load is NOT production-ready. Skills should prevent these failures.

### Why This Structure?

#### File-Based vs Database

**Choice:** File system storage, not a database.

**Benefits:**

- **Inspectability**: Use `git`, `grep`, `find` to explore skills
- **Version control**: Track changes with standard VCS tools
- **Editable**: Use any text editor, no special tools required
- **Transparent**: No black box, everything is visible

**Trade-offs:**

- Slower lookups than database for large fleets
- Requires manual consistency checks (validation CLI)

#### Path-Style IDs

**Choice:** Path-style IDs with slashes (`technical/programming/languages/python/fastapi`), not dots or UUIDs.

**Benefits:**

- **Human-readable**: IDs explain what the skill is
- **Composable**: Can reference parent paths for dependencies
- **Navigable**: File system structure mirrors skill hierarchy
- **Semantic**: Paths encode domain knowledge

**Trade-offs:**

- Requires governance to avoid naming conflicts
- Path changes break references (use aliases for migrations)

#### Strict Metadata

**Choice:** Required metadata fields with validation.

**Benefits:**

- **Prevents drift**: Enforces consistency as taxonomy grows
- **Enables discovery**: Search by type, weight, tags
- **Supports composition**: Dependencies and capabilities are explicit
- **Tracks evolution**: Version history and change logs

**Trade-offs:**

- More friction when creating skills
- Requires maintenance when standards evolve

#### agentskills.io Compliance

**Choice:** Follow [agentskills.io](https://agentskills.io) specification.

**Benefits:**

- **Cross-system compatibility**: Skills work across different agent platforms
- **XML generation**: Automatic generation of agent context injection
- **Standardization**: Industry-wide format for skill definitions
- **Future-proof**: Aligns with emerging standards

**Trade-offs:**

- Additional frontmatter requirements
- Must maintain compliance as spec evolves

---

## 3. Skill Creation Interrogations (Discovery Questions)

Before creating a skill, ask these questions to ensure proper scoping and avoid duplication.

### Phase 1: Understanding the Need

#### Questions to Ask

1. **What problem does this skill solve?**

- Be specific: "Handle async database connections in FastAPI" not "database stuff"
- Identify the pain point this skill addresses

2. **Is this a new skill or enhancement to existing?**

- Search existing taxonomy first
- Check if capability fits in existing skill

3. **Does it overlap with existing skills?**

- Use `find` and `grep` to search for related terms
- Check dependency graph for related skills

4. **What domain does it belong to?**

- `cognitive`: Thinking patterns, reasoning approaches
- `technical`: Programming, frameworks, tools
- `domain`: Subject matter expertise
- `tool`: Software/platform proficiency
- `mcp`: Model Context Protocol
- `specialization`: Advanced/focused applications
- `task_focus`: Problem-solving methodologies
- `memory`: Memory management patterns

#### Decision Points

**Search existing taxonomy:**

```bash
# Search for related terms
find skills -name "*.md" | xargs grep -l "database"

# Check for similar capabilities
find skills -name "metadata.json" | xargs grep -l "sql"
```

**Determine appropriate branch:**

- If it's programming-related: `technical_skills/programming/`
- If it's subject matter: `domain_knowledge/`
- If it's a tool: `tool_proficiency/`
- If it's thinking patterns: `cognitive/`

**Output of Phase 1:**

- Clear problem statement
- Decision: new skill vs enhancement
- Domain classification
- Taxonomy path draft

### Phase 2: Scope & Boundaries

#### Questions to Ask

1. **What type of skill?**

- Review the 8 types and choose the best fit
- Use the [Type Determination Matrix](#type-determination-matrix) below

2. **What weight?**

- `lightweight`: Small, focused, 1-3 capabilities
- `medium`: Multi-capability, 4-7 capabilities
- `heavyweight`: Complex workflows, 8+ capabilities (rare)

3. **What load priority?**

- `always`: Core skills loaded at startup
- `task_specific`: Loaded when task matches intent
- `on_demand`: Loaded only when referenced
- `dormant`: Archived or experimental

4. **How many capabilities?**

- Target 3-7 capabilities
- Each capability should be atomic and testable
- Use the [Capability Design Principles](#capability-design-principles) below

5. **What dependencies on other skills?**

- List skills this one builds upon
- Avoid circular dependencies
- Use the [Dependency Composition Rules](#dependency-composition-rules) below

#### Decision Framework

##### Type Determination Matrix

| Skill Characteristics                  | Best Type   |
| -------------------------------------- | ----------- |
| Implementation patterns, code examples | `technical` |
| Subject matter, terminology, facts     | `domain`    |

- Thinking strategies, reasoning methods | `cognitive` |
- Software/platform proficiency | `tool` |
- MCP server/protocol related | `mcp` |
- Advanced application of other skills | `specialization` |
- Problem-solving methodologies | `task_focus` |
- Memory management patterns | `memory` |

##### Weight Guidelines

| Factor        | Lightweight     | Medium              | Heavyweight       |
| ------------- | --------------- | ------------------- | ----------------- |
| Capabilities  | 1-3             | 4-7                 | 8+                |
| Documentation | < 500 lines     | 500-2000 lines      | > 2000 lines      |
| Examples      | 1-2             | 3-5                 | 6+                |
| Dependencies  | 0-2             | 3-5                 | 6+                |
| Complexity    | Simple concepts | Moderate complexity | Complex workflows |

**Rule of thumb:** Start with `lightweight` or `medium`. Use `heavyweight` sparingly.

##### Load Priority Decision Tree

```
Is this a core/foundational skill?
‚îú‚îÄ Yes ‚Üí load_priority: "always"
‚îî‚îÄ No
   ‚îú‚îÄ Is this commonly used across tasks?
   ‚îÇ  ‚îú‚îÄ Yes ‚Üí load_priority: "task_specific"
   ‚îÇ  ‚îî‚îÄ No
   ‚îÇ     ‚îú‚îÄ Is this rarely needed or experimental?
   ‚îÇ     ‚îÇ  ‚îú‚îÄ Yes ‚Üí load_priority: "on_demand" or "dormant"
   ‚îÇ     ‚îÇ  ‚îî‚îÄ No ‚Üí load_priority: "task_specific"
```

**Examples:**

- `technical/programming/languages/python` ‚Üí `always` (foundational)
- `technical/programming/web_frameworks/python/fastapi` ‚Üí `task_specific` (common but not universal)
- `domain_knowledge/medical/neurosurgery` ‚Üí `on_demand` (rarely needed)
- `experimental/new_technology` ‚Üí `dormant` (experimental)

##### Dependency Composition Rules

1. **No cycles**: If A depends on B, B cannot depend on A
2. **Prefer abstractions**: If multiple skills need X, extract X into a shared skill
3. **Specify versions**: For external dependencies, always specify versions
4. **Minimize depth**: Dependency trees should be shallow (prefer 2-3 levels max)
5. **Document why**: Explain why each dependency is needed

**Good dependency:**

```json
{
  "dependencies": [
    "technical/programming/languages/python" // Reason: Uses Python syntax
  ]
}
```

**Bad dependency (too specific):**

```json
{
  "dependencies": [
    "technical/programming/web_frameworks/python/fastapi" // Too narrow
  ]
}
```

**Output of Phase 2:**

- Skill type determined
- Weight assigned
- Load priority chosen
- Capability count estimated
- Dependencies listed

### Phase 3: Capability Breakdown

#### Questions to Ask

1. **What are the atomic capabilities?**

- Break down into smallest testable units
- Each capability should solve one specific problem

2. **Can each be tested independently?**

- Write a test for each capability
- If you can't test it independently, it's not atomic

3. **Are they cohesive (single responsibility)?**

- Each capability should have one clear purpose
- If it does multiple things, split it

4. **Do they have clear boundaries?**

- No overlap between capabilities
- Clear distinction where one ends and another begins

#### Capability Design Principles

1. **Atomic and Testable**

- Each capability is independently testable
- Has clear inputs and outputs
- Can be demonstrated in isolation

2. **Single Responsibility**

- Each capability does one thing well
- Avoid multi-purpose capabilities

3. **Clear Input/Output Contracts**

- Specify what the capability requires
- Specify what the capability produces
- Document edge cases

4. **Cross-Referencable**

- Other skills can reference this capability
- Has a stable identifier (kebab-case name)
- Related capabilities are linked

#### Example: Breaking Down a Skill

**Initial idea:** "FastAPI database skill"

**Too broad - contains multiple capabilities:**

- Connection lifecycle management
- Query execution
- Transaction handling
- Migration management
- Testing utilities

**Better breakdown:**

1. `database-lifecycle-management` - Engine creation, pooling, shutdown
2. `async-query-execution` - Running async queries
3. `transaction-management` - Commit/rollback handling
4. `async-testing` - Testing async endpoints with databases

**Output of Phase 3:**

- List of atomic capabilities
- Test plan for each capability
- Clear boundaries between capabilities
- Cross-references to related capabilities

---

## 4. Structure & Format Requirements

### Directory Structure

Every skill directory MUST follow this structure:

```
skill-name/
‚îú‚îÄ‚îÄ metadata.json          # Optional: Internal metadata (v2 standard)
‚îú‚îÄ‚îÄ SKILL.md              # Required: Main doc with YAML frontmatter
‚îú‚îÄ‚îÄ references/            # Recommended: Deep technical documentation
‚îÇ   ‚îú‚îÄ‚îÄ capability-1.md
‚îÇ   ‚îú‚îÄ‚îÄ capability-2.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ scripts/               # Optional: Runnable utility scripts
‚îú‚îÄ‚îÄ examples/              # Recommended: Runnable demo projects
‚îÇ   ‚îú‚îÄ‚îÄ 01-example-name/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ example.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_example.py
‚îÇ   ‚îî‚îÄ‚îÄ 02-example-name/
‚îú‚îÄ‚îÄ tests/                 # Recommended: Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ test_skill.py
‚îî‚îÄ‚îÄ guides/                # Recommended: Step-by-step workflows
    ‚îú‚îÄ‚îÄ requirements.json
    ‚îú‚îÄ‚îÄ config.yaml
    ‚îú‚îÄ‚îÄ reference.md
    ‚îî‚îÄ‚îÄ ...
```

**Required files:**

- `SKILL.md` - Main documentation with agentskills.io-compliant YAML frontmatter

**Recommended files/directories:**

- `references/` - Detailed technical documentation for specific capabilities
- `examples/` - At least one runnable usage example
- `tests/` - Automated tests for the skill
- `guides/` - Step-by-step guides for common workflows
- `metadata.json` - Optional metadata for internal system tracking (overrides frontmatter)

**Optional files:**

- `best_practices.md` - Additional best practices
- `integration.md` - Integration patterns with other skills
- `troubleshooting.md` - Common issues and solutions

### Naming Conventions

#### Directory Names

Use **kebab-case** (lowercase with hyphens):

```
‚úÖ fastapi-production-patterns
‚úÖ python-decorators
‚úÖ medical-terminology

‚ùå FastAPI_Patterns (camelCase)
‚ùå python_decorators (snake_case)
‚ùå python decorators (spaces)
```

#### SKILL.md Name

The `name` field in YAML frontmatter must match the directory name:

```
Directory: fastapi-production-patterns/
SKILL.md name: fastapi-production-patterns
```

#### Capability Files

Use **kebab-case** for capability files:

```
‚úÖ database-lifecycle-management.md
‚úÖ async-query-execution.md
‚úÖ partial-updates.md

‚ùå DB_Lifecycle.md (mixed case)
‚ùå async_query.md (snake_case)
```

#### skill_id

Use **path-style with slashes**:

```
‚úÖ technical/programming/web-frameworks/python/fastapi
‚úÖ domain_knowledge/medical/terminology
‚úÖ tool_proficiency/version_control/git

‚ùå technical.programming.web-frameworks (dots)
‚ùå technical_programming_web_frameworks (underscores)
```

### metadata.json Format

Every skill must have a `metadata.json` file:

```json
{
  "skill_id": "technical/programming/web-frameworks/python/fastapi",
  "name": "fastapi-production-patterns",
  "description": "Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints",
  "version": "1.0.0",
  "type": "technical",
  "weight": "medium",
  "load_priority": "task_specific",
  "dependencies": [],
  "capabilities": [
    "database-lifecycle-management",
    "pydantic-partial-updates",
    "async-conversion",
    "dependency-injection",
    "async-testing",
    "file-upload-handling",
    "background-tasks",
    "python-to-api-conversion"
  ],
  "category": "Web Development",
  "tags": [
    "python",
    "fastapi",
    "rest-api",
    "asyncio",
    "pydantic",
    "sqlalchemy",
    "production",
    "async"
  ],
  "created_at": "2026-01-07T02:34:11.401895+00:00",
  "last_modified": "2026-01-09T16:45:00.000000+00:00",
  "evolution": {
    "version": "1.0.0",
    "parent_id": null,
    "evolution_path": "initial_release",
    "change_log": "Initial packaging of FastAPI skill set with production patterns following TDD methodology.",
    "validation_score": 1.0,
    "integrity_hash": "a1b2c3d4e5f6g7h8"
  }
}
```

**Field descriptions:**

| Field           | Type   | Required | Description                                                                         |
| --------------- | ------ | -------- | ----------------------------------------------------------------------------------- |
| `skill_id`      | string | Yes      | Path-style identifier                                                               |
| `name`          | string | Yes      | Kebab-case name (matches directory)                                                 |
| `description`   | string | Yes      | 1-1024 character description                                                        |
| `version`       | string | Yes      | Semantic version (X.Y.Z)                                                            |
| `type`          | string | Yes      | One of: cognitive, technical, domain, tool, mcp, specialization, task_focus, memory |
| `weight`        | string | Yes      | One of: lightweight, medium, heavyweight                                            |
| `load_priority` | string | Yes      | One of: always, task_specific, on_demand, dormant                                   |
| `dependencies`  | array  | Yes      | List of skill_id strings (can be empty)                                             |
| `capabilities`  | array  | Yes      | List of capability name strings                                                     |
| `category`      | string | No       | Broad category for grouping                                                         |
| `tags`          | array  | No       | List of tag strings for search                                                      |
| `created_at`    | string | Yes      | ISO-8601 timestamp                                                                  |
| `last_modified` | string | Yes      | ISO-8601 timestamp                                                                  |
| `evolution`     | object | Yes      | Evolution tracking metadata                                                         |

### SKILL.md Format (agentskills.io compliant)

Every skill must have a `SKILL.md` file with YAML frontmatter:

```markdown
---
name: skill-name
description: 1-1024 character description
license: MIT|Apache-2.0|BSD-3-Clause|etc
compatibility: Requirements and constraints (e.g., "Requires Python 3.8+, FastAPI 0.128.0+")
metadata:
  skill_id: path/to/skill
  version: 1.0.0
  type: technical
  weight: medium
  load_priority: task_specific
---

# Skill Title

## Overview

High-level description of what this skill does (2-3 sentences).

## When to Use

Decision framework for when to apply this skill.

**When to use:**

- Condition 1
- Condition 2
- Condition 3

**When NOT to use:**

- Condition 1
- Condition 2

## Quick Reference

| Problem   | Solution   | Keywords           |
| --------- | ---------- | ------------------ |
| Problem 1 | Solution 1 | keyword1, keyword2 |
| Problem 2 | Solution 2 | keyword3, keyword4 |

## Core Patterns/Capabilities

### Capability 1

**Problem:** What problem does it solve?
**Solution:** How does it solve it?
**Example:** Code or usage example

### Capability 2

...

## Common Mistakes

| Mistake   | Why It's Wrong | Fix      |
| --------- | -------------- | -------- |
| Mistake 1 | Explanation    | Solution |
| Mistake 2 | Explanation    | Solution |

## Real-World Impact

- **Metric 1**: Description ‚Üí measurable outcome
- **Metric 2**: Description ‚Üí measurable outcome

## See Also

- [Related Skill 1](../related-skill-1/SKILL.md)
- [Related Skill 2](../related-skill-2/SKILL.md)
```

**agentskills.io frontmatter requirements:**

| Field                    | Required | Format          | Description              |
| ------------------------ | -------- | --------------- | ------------------------ |
| `name`                   | Yes      | kebab-case      | Skill identifier         |
| `description`            | Yes      | 1-1024 chars    | What this skill does     |
| `license`                | No       | SPDX identifier | License type             |
| `compatibility`          | No       | Free text       | Requirements/constraints |
| `metadata.skill_id`      | Yes      | path-style      | Full path identifier     |
| `metadata.version`       | Yes      | semver          | Semantic version         |
| `metadata.type`          | Yes      | enum            | Skill type               |
| `metadata.weight`        | Yes      | enum            | Skill weight             |
| `metadata.load_priority` | Yes      | enum            | Load priority            |

### Capability Files Format

Each capability should have its own markdown file in `references/`:

````markdown
# Capability Name

## Overview

What this capability does in 1-2 sentences.

## Problem Statement

**Silent failures that cause problems:**

- Issue 1
- Issue 2
- Issue 3

## Pattern/Solution

### ‚ùå Broken (Baseline Failure)

```python
# Show what NOT to do
problematic_code_here
```
````

### ‚úÖ Production Pattern

```python
# Show the RIGHT way
correct_code_here
```

## Key Parameters/Concepts

| Parameter/Concept | Purpose      | Typical Value/Usage |
| ----------------- | ------------ | ------------------- |
| Param 1           | What it does | Example             |
| Param 2           | What it does | Example             |

## Symptoms of Misconfiguration

| Symptom   | Root Cause | Fix   |
| --------- | ---------- | ----- |
| Symptom 1 | Cause 1    | Fix 1 |
| Symptom 2 | Cause 2    | Fix 2 |

## Testing

```python
# Example test
