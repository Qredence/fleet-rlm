<!-- Chunk 280: bytes 353522-359900, type=function -->
def main() -> None:
    """Main entry point."""
    if len(sys.argv) < 2:
        print("Usage: python detect_duplicates.py <file_or_directory>")
        print("       python detect_duplicates.py --min-lines 10 <file_or_directory>")
        sys.exit(1)

    # Parse arguments
    min_lines = 5
    target_path = sys.argv[-1]

    if "--min-lines" in sys.argv:
        idx = sys.argv.index("--min-lines")
        min_lines = int(sys.argv[idx + 1])

    path = Path(target_path)
    if not path.exists():
        print(f"‚ùå Error: Path '{path}' does not exist")
        sys.exit(1)

    # Collect Python files
    python_files = collect_python_files(path)
    if not python_files:
        print(f"‚ùå No Python files found in '{path}'")
        sys.exit(1)

    print(f"üîç Analyzing {len(python_files)} Python files...\n")

    # Find and print duplicates
    duplicates = find_duplicates(python_files, min_lines=min_lines)
    print_duplicates(duplicates)

    # Exit with appropriate code
    sys.exit(0 if not duplicates else 1)


if __name__ == "__main__":
    main()


============================================================
END FILE: .fleet/factory/skills/python-backend-reviewer/scripts/detect_duplicates.py
============================================================

============================================================
FILE: .fleet/factory/skills/rlm/SKILL.md
============================================================

---
name: rlm
description: Run a Recursive Language Model-style loop for long-context tasks. Uses a persistent local Python REPL and an rlm-subcall subagent as the sub-LLM (llm_query).
allowed-tools:
  - Read
  - Write
  - Edit
  - Grep
  - Glob
  - Bash
---

# rlm (Recursive Language Model workflow)

Use this Skill when:
- The user provides (or references) a very large context file (docs, logs, transcripts, scraped webpages) that won't fit comfortably in chat context.
- You need to iteratively inspect, search, chunk, and extract information from that context.
- You can delegate chunk-level analysis to a subagent.

## Mental model

- Main Claude Code conversation = the root LM.
- Persistent Python REPL (`rlm_repl.py`) = the external environment.
- Subagent `rlm-subcall` = the sub-LM used like `llm_query`.

## How to run

### Inputs

This Skill reads `$ARGUMENTS`. Accept these patterns:
- `context=<path>` (required): path to the file containing the large context.
- `query=<question>` (required): what the user wants.
- Optional: `chunk_chars=<int>` (default ~200000) and `overlap_chars=<int>` (default 0).

If the user didn't supply arguments, ask for:
1) the context file path, and
2) the query.

### Step-by-step procedure

1. Initialise the REPL state
   ```bash
   python3 .claude/skills/rlm/scripts/rlm_repl.py init <context_path>
   python3 .claude/skills/rlm/scripts/rlm_repl.py status
   ```

2. Scout the context quickly
   ```bash
   python3 .claude/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 3000))"
   python3 .claude/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(len(content)-3000, len(content)))"
   ```

3. Choose a chunking strategy
   - Prefer semantic chunking if the format is clear (markdown headings, JSON objects, log timestamps).
   - Otherwise, chunk by characters (size around chunk_chars, optional overlap).

4. Materialise chunks as files (so subagents can read them)
   ```bash
   python3 .claude/skills/rlm/scripts/rlm_repl.py exec <<'PY'
   paths = write_chunks('.claude/rlm_state/chunks', size=200000, overlap=0)
   print(len(paths))
   print(paths[:5])
   PY
   ```

5. Subcall loop (delegate to rlm-subcall)
   - For each chunk file, invoke the rlm-subcall subagent with:
     - the user query,
     - the chunk file path,
     - and any specific extraction instructions.
   - Keep subagent outputs compact and structured (JSON preferred).
   - Append each subagent result to buffers (either manually in chat, or by pasting into a REPL add_buffer(...) call).

6. Synthesis
   - Once enough evidence is collected, synthesise the final answer in the main conversation.
   - Optionally ask rlm-subcall once more to merge the collected buffers into a coherent draft.

## Guardrails

- Do not paste large raw chunks into the main chat context.
- Use the REPL to locate exact excerpts; quote only what you need.
- Subagents cannot spawn other subagents. Any orchestration stays in the main conversation.
- Keep scratch/state files under .claude/rlm_state/.


============================================================
END FILE: .fleet/factory/skills/rlm/SKILL.md
============================================================

============================================================
FILE: .fleet/factory/skills/rlm/scripts/rlm_repl.py
============================================================

#!/usr/bin/env python3
"""Persistent mini-REPL for RLM-style workflows in Claude Code.

This script provides a *stateful* Python environment across invocations by
saving a pickle file to disk. It is intentionally small and dependency-free.

Typical flow:
  1) Initialise context:
       python rlm_repl.py init path/to/context.txt
  2) Execute code repeatedly (state persists):
       python rlm_repl.py exec -c 'print(len(content))'
       python rlm_repl.py exec <<'PYCODE'
       # you can write multi-line code
       hits = grep('TODO')
       print(hits[:3])
       PYCODE

The script injects these variables into the exec environment:
  - context: dict with keys {path, loaded_at, content}
  - content: string alias for context['content']
  - buffers: list[str] for storing intermediate text results

It also injects helpers:
  - peek(start=0, end=1000) -> str
  - grep(pattern, max_matches=20, window=120, flags=0) -> list[dict]
  - chunk_indices(size=200000, overlap=0) -> list[(start,end)]
  - write_chunks(out_dir, size=200000, overlap=0, prefix='chunk') -> list[str]
  - add_buffer(text: str) -> None

Security note:
  This runs arbitrary Python via exec. Treat it like running code you wrote.
"""

from __future__ import annotations

import argparse
import io
import os
import pickle
import re
import sys
import textwrap
import time
import traceback
from contextlib import redirect_stderr, redirect_stdout
from pathlib import Path
from typing import Any, Dict, List, Tuple


DEFAULT_STATE_PATH = Path(".claude/rlm_state/state.pkl")
DEFAULT_MAX_OUTPUT_CHARS = 8000


