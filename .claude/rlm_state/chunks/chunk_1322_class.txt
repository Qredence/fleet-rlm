<!-- Chunk 1322: bytes 4859292-4860305, type=class -->
class Settings(BaseSettings):
    tavily_api_key: str
    class Config:
        env_file = ".env"

settings = Settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Establish a shared client for connection pooling
    app.state.client = httpx.AsyncClient(
        base_url="https://api.tavily.com",
        headers={"Content-Type": "application/json"}
    )
    yield
    await app.state.client.aclose()

app = FastAPI(lifespan=lifespan)

@app.post("/search")
async def perform_search(request: TavilySearchRequest):
    payload = {**request.model_dump(), "api_key": settings.tavily_api_key}
    response = await app.state.client.post("/search", json=payload)
    
    if response.status_code != 200:
        raise HTTPException(status_code=response.status_code, detail="Tavily API Error")
    return response.json()
```

### 2. Response Modeling & Optimization
To prevent agents from being overwhelmed by irrelevant metadata, we model the response and filter for LLM-critical fields.

```python
