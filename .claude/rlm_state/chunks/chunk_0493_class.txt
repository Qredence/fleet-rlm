<!-- Chunk 493: bytes 843459-845117, type=class -->
class MyModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predict = dspy.Predict(
            MySignature,
            adapter=adapter  # Use custom adapter
        )

    def forward(self, input_data):
        return self.predict(input=input_data)
```

## Multi-Provider Configuration

### Configure with LiteLLM

```python
import dspy

# OpenAI
lm = dspy.LM("openai/gpt-4o-mini", api_key="YOUR_KEY")

# Anthropic
lm = dspy.LM("anthropic/claude-3-opus-20240229")

# Together AI
lm = dspy.LM("together_ai/meta-llama/Llama-2-70b-chat-hf")

# General provider (via LiteLLM)
lm = dspy.LM(
    "openai/your-model-name",
    api_key="PROVIDER_API_KEY",
    api_base="YOUR_PROVIDER_URL"
)

# Configure DSPy
dspy.configure(lm=lm)
```

### Switching LMs Globally

```python
import dspy

# Configure LM globally
dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))
response = qa(question="Test question")
print('GPT-4o-mini:', response.answer)
```

### Switching LMs Locally

```python
import dspy

# Change LM within a context block
with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):
    response = qa(question="Test question")
    print('GPT-3.5-turbo:', response.answer)
```

### Responses API for Advanced Models

```python
import dspy

# Configure DSPy to use Responses API
dspy.configure(
    lm=dspy.LM(
        "openai/gpt-5-mini",
        model_type="responses",
        temperature=1.0,
        max_tokens=16000,
    ),
)
```

## Best Practices

### 1. Use Built-in Adapters

```python
# Good: Use built-in ChatAdapter or JSONAdapter
module = dspy.Predict(MySignature)

# Bad: Don't create custom adapter unless necessary
