<!-- Chunk 166: bytes 240918-282258, type=function -->
def main():
    parser = argparse.ArgumentParser(description="Validate DSPy signatures")
    parser.add_argument("--signature", required=True, help="Name of the signature class")
    parser.add_argument(
        "--module",
        default="agentic_fleet.dspy_modules.signatures",
        help="Module path",
    )
    parser.add_argument("--strict", action="store_true", help="Enable strict validation")

    args = parser.parse_args()

    # Import the module
    try:
        module = importlib.import_module(args.module)
    except ImportError as e:
        print(f"Error importing module: {e}")
        sys.exit(1)

    # Get the signature class
    if not hasattr(module, args.signature):
        print(f"Error: Signature '{args.signature}' not found in module")
        print(f"Available signatures: {[name for name in dir(module) if not name.startswith('_')]}")
        sys.exit(1)

    signature_class = getattr(module, args.signature)

    # Validate
    success = validate_signature(signature_class, args.strict)
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()


============================================================
END FILE: .fleet/factory/skills/dspy-optimization/scripts/test-signature.py
============================================================

============================================================
FILE: .fleet/factory/skills/hooks-integration/SKILL.md
============================================================

---
name: hooks-integration
description: Factory hooks integration for AgenticFleet development workflows
tags: [factory, hooks, automation, git, ci]
author: AgenticFleet Architect
created: 2024-12-30
---

# Factory Hooks Integration for AgenticFleet

This skill documents how to use Factory hooks to automate common AgenticFleet development workflows.

## Overview

Factory hooks provide AI-assisted automation for common development tasks. This integration sets up hooks for:

- **Config validation** - Validate workflow config before changes
- **DSPy cache management** - Clear cache when signatures change
- **Import validation** - Ensure clean imports in Python files
- **CHANGELOG automation** - Auto-update on version tags
- **Memory system updates** - Sync memory when context changes
- **Auto-formatting** - Format code on save

## Quick Start

### 1. Install Hooks

```bash
# Install both pre-commit framework and enhanced git hooks
make setup-hooks

# Or install individually
make pre-commit-install   # Standard pre-commit hooks
make hooks-install        # Enhanced AgenticFleet hooks
```

### 2. Enable Factory Hooks

Factory hooks are configured in `.factory/hooks.yaml`. They run automatically when:

- You change files matching patterns
- You commit changes
- You create version tags

### 3. Verify Installation

```bash
# Test hooks are installed
ls -la .git/hooks/

# Should show:
# -rwxr-xr-x   pre-commit          # Enhanced pre-commit
# -rwxr-xr-x   pre-push            # Enhanced pre-push
# -rwxr-xr-x   prepare-commit-msg  # Commit message formatting
# -rwxr-xr-x   post-checkout       # Branch switch automation
```

## Hook Details

### Enhanced Pre-commit Hook
**Triggers**: Before commit
**Checks**:
- `.env` file validation (prevents empty API keys)
- `workflow_config.yaml` syntax validation
- DSPy cache warnings
- Prevents committing `.var/` directory files
- Runs quick quality checks (`make check`)

### Enhanced Pre-push Hook  
**Triggers**: Before push to remote
**Checks**:
- Git LFS validation
- Prevents pushing `.var/` files
- Prevents pushing compiled DSPy cache (`.pkl`)
- Validates workflow config structure
- Runs fast tests (optional)

### Prepare-commit-msg Hook
**Triggers**: When writing commit message
**Actions**:
- Auto-prefixes commits with scope (`backend:`, `frontend:`, `dspy:`, etc.)
- Suggests emoji based on changed files
- Maintains conventional commit format

### Post-checkout Hook
**Triggers**: After branch checkout
**Actions**:
- Detects dependency changes (uv.lock, package.json)
- Offers to sync dependencies
- Detects DSPy signature changes
- Offers to clear DSPy cache
- Shows helpful next steps

## Factory-Specific Hooks

### Config Validation Hook
```yaml
- name: validate-config
  description: Validate workflow config before changes
  trigger: before-file-change
  pattern: src/agentic_fleet/config/workflow_config.yaml
  command: make test-config
```

**Why**: Prevents broken configs from being committed. Runs `make test-config` which validates YAML syntax and required sections.

### DSPy Cache Management
```yaml
- name: clear-dspy-cache
  description: Clear DSPy cache when signatures change
  trigger: after-file-change
  pattern: src/agentic_fleet/dspy_modules/signatures.py
  command: make clear-cache
```

**Why**: DSPy compilation cache becomes invalid when signatures change. Auto-clearing prevents hard-to-debug issues.

### Import Validation
```yaml
- name: validate-imports
  description: Validate Python imports
  trigger: before-file-change
  pattern: "**/*.py"
  exclude: "**/tests/**"
  command: uv run ruff check --select=I --fix .
```

**Why**: Ensures clean imports (no unused imports, proper ordering).

### CHANGELOG Automation
```yaml
- name: update-changelog
  description: Update CHANGELOG.md on release
  trigger: on-tag
  pattern: "^v[0-9]+\.[0-9]+\.[0-9]+$"
  command: |
    if command -v git-cliff >/dev/null 2>&1; then
      git cliff --unreleased --strip header --prepend CHANGELOG.md
    fi
```

**Why**: Automatically updates CHANGELOG.md when creating version tags.

### Memory System Sync
```yaml
- name: update-memory
  description: Update memory system when context changes
  trigger: after-file-change
  pattern: ".fleet/context/**/*.md"
  command: |
    echo "Memory system files changed. Consider running:"
    echo "  uv run python .fleet/context/scripts/memory_manager.py init"
```

**Why**: Memory system files changes may require re-indexing.

## Customizing Hooks

### Adding New Hooks

1. Edit `.factory/hooks.yaml`
2. Add new hook definition:
   ```yaml
   - name: your-hook-name
     description: What it does
     trigger: before-file-change|after-file-change|before-commit|after-commit|on-tag
     pattern: "glob/pattern/**"
     command: "command to run"
   ```

### Hook Groups

Groups allow enabling/disabling sets of hooks:

```yaml
# Enable development group
factory hooks enable-group development

# Enable all hooks
factory hooks enable-all

# Disable specific hook
factory hooks disable validate-config
```

### Hook Execution Order

Hooks run in this order:
1. `before-file-change` - Validate before saving
2. `after-file-change` - Auto-format, cache clear
3. `before-commit` - Final validation
4. `after-commit` - Cleanup tasks
5. `on-tag` - Release automation

## Troubleshooting

### Hooks Not Running

```bash
# Check if hooks are executable
ls -la .git/hooks/

# Make hooks executable if needed
chmod +x .git/hooks/*

# Reinstall hooks
make hooks-update
```

### Hook Failing

```bash
# Run hook manually to see error
.git/hooks/pre-commit

# Skip hooks temporarily
git commit --no-verify
git push --no-verify
```

### Factory Hooks Not Working

```bash
# Check Factory hooks configuration
factory hooks status

# Enable hooks
factory hooks enable validate-config
```

### Git LFS Issues

```bash
# Ensure git-lfs is installed
brew install git-lfs  # macOS
git lfs install

# Check LFS tracking
git lfs track
```

## Best Practices

1. **Keep hooks fast** - Hooks should run quickly (<5s)
2. **Fail early** - Better to fail at pre-commit than at CI
3. **Provide helpful messages** - Tell users how to fix issues
4. **Make hooks optional** - Use `read -p` for optional steps
5. **Document changes** - Update commands.md when adding hooks
6. **Test hooks** - Run `make hooks-update` after changes

## Integration with CI

Hooks complement CI but don't replace it:

- **Hooks**: Local, fast, prevent common issues
- **CI**: Comprehensive, runs all tests, enforces policies

Both should validate:
- Config syntax (`make test-config`)
- Code quality (`make check`)
- Tests pass (`make test-fast`)

## Example Workflow

```bash
# 1. Start development
make dev-setup          # Installs dependencies and hooks
make dev                # Starts dev servers

# 2. Make changes
# Hooks will:
# - Auto-format on save
# - Validate config changes
# - Clear DSPy cache when needed

# 3. Commit
git add .
git commit              # Hooks auto-prefix commit, run checks

# 4. Push
git push origin main    # Hooks validate before push
```

## Related Skills

- `repo-cleanup` - Git hygiene and cleanup procedures
- `dspy-agent-framework-integration` - DSPy patterns that hooks protect
- `memory-system-guide` - Memory system that hooks help maintain


============================================================
END FILE: .fleet/factory/skills/hooks-integration/SKILL.md
============================================================

============================================================
FILE: .fleet/factory/skills/initiate-memory/SKILL.md
============================================================

---
name: initializing-memory
description: Comprehensive guide for initializing or reorganizing agent memory. Load this skill when running /init, when the user asks you to set up your memory, or when you need guidance on creating effective memory blocks.
---

# Initializing Memory

The user has requested that you initialize or reorganize your memory state. You have access to the `memory` tool which allows you to create, edit, and manage memory blocks.

## Understanding Your Context

**Important**: You are a Letta Code agent, which is fundamentally different from typical AI coding assistants. Letta Code agents are **stateful** - users expect to work with the same agent over extended periods, potentially for the entire lifecycle of a project or even longer. Your memory is not just a convenience; it's how you get better over time and maintain continuity across sessions.

This command may be run in different scenarios:
- **Fresh agent**: You may have default memory blocks that were created when you were initialized
- **Existing agent**: You may have been working with the user for a while, and they want you to reorganize or significantly update your memory structure
- **Shared blocks**: Some memory blocks may be shared across multiple agents - be careful about modifying these

Before making changes, use the `memory` tool to inspect your current memory blocks and understand what already exists.

## What Coding Agents Should Remember

### 1. Procedures (Rules & Workflows)
Explicit rules and workflows that should always be followed:
- "Never commit directly to main - always use feature branches"
- "Always run lint before running tests"
- "Use conventional commits format for all commit messages"
- "Always check for existing tests before adding new ones"

### 2. Preferences (Style & Conventions)
User and project coding style preferences:
- "Never use try/catch for control flow"
- "Always add JSDoc comments to exported functions"
- "Prefer functional components over class components"
- "Use early returns instead of nested conditionals"

### 3. History & Context
Important historical context that informs current decisions:
- "We fixed this exact pagination bug two weeks ago - check PR #234"
- "This monorepo used to have 3 modules before the consolidation"
- "The auth system was refactored in v2.0 - old patterns are deprecated"
- "User prefers verbose explanations when debugging"

Note: For historical recall, you may also have access to `conversation_search` which can search past conversations. Memory blocks are for distilled, important information worth persisting permanently.

## Memory Scope Considerations

Consider whether information is:

**Project-scoped** (store in `project` block):
- Build commands, test commands, lint configuration
- Project architecture and key directories
- Team conventions specific to this codebase
- Technology stack and framework choices

**User-scoped** (store in `human` block):
- Personal coding preferences that apply across projects
- Communication style preferences
- General workflow habits

**Session/Task-scoped** (consider separate blocks like `ticket` or `context`):
- Current branch or ticket being worked on
- Debugging context for an ongoing investigation
- Temporary notes about a specific task

## Recommended Memory Structure

### Core Blocks (Usually Present)

**`persona`**: Your behavioral guidelines that augment your base system prompt.
- Your system prompt already contains comprehensive instructions for how to code and behave
- The persona block is for **learned adaptations** - things you discover about how the user wants you to behave
- Examples: "User said never use emojis", "User prefers terse responses", "Always explain reasoning before making changes"
- This block may start empty and grow over time as you learn the user's preferences

**`project`**: Project-specific information, conventions, and commands
- Build/test/lint commands
- Key directories and architecture
- Project-specific conventions from README, AGENTS.md, etc.

**`human`**: User preferences, communication style, general habits
- Cross-project preferences
- Working style and communication preferences

### Optional Blocks (Create as Needed)

**`ticket`** or **`task`**: Scratchpad for current work item context.
- **Important**: This is different from the TODO or Plan tools!
- TODO/Plan tools track active task lists and implementation plans (structured lists of what to do)
- A ticket/task memory block is a **scratchpad** for pinned context that should stay visible
- Examples: Linear ticket ID and URL, Jira issue key, branch name, PR number, relevant links
- Information that's useful to keep in context but doesn't fit in a TODO list

**`context`**: Debugging or investigation scratchpad
- Current hypotheses being tested
- Files already examined
- Clues and observations

**`decisions`**: Architectural decisions and their rationale
- Why certain approaches were chosen
- Trade-offs that were considered

## Writing Good Memory Blocks

**This is critical**: In the future, you (or a future version of yourself) will only see three things about each memory block:
1. The **label** (name)
2. The **description**
3. The **value** (content)

The reasoning you have *right now* about why you're creating a block will be lost. Your future self won't easily remember this initialization conversation (it can be searched, but it will no longer be in-context). Therefore:

**Labels should be:**
- Clear and descriptive (e.g., `project-conventions` not `stuff`)
- Consistent in style (e.g., all lowercase with hyphens)

**Descriptions are especially important:**
- Explain *what* this block is for and *when* to use it
- Explain *how* this block should influence your behavior
- Write as if explaining to a future version of yourself who has no context
- Good: "User's coding style preferences that should be applied to all code I write or review. Update when user expresses new preferences."
- Bad: "Preferences"

**Values should be:**
- Well-organized and scannable
- Updated regularly to stay relevant
- Pruned of outdated information

Think of memory block descriptions as documentation for your future self. The better you write them now, the more effective you'll be in future sessions.

## Research Depth

You can ask the user if they want a standard or deep research initialization:

**Standard initialization** (~5-20 tool calls):
- Inspect existing memory blocks
- Scan README, package.json/config files, AGENTS.md, CLAUDE.md
- Review git status and recent commits (from context below)
- Explore key directories and understand project structure
- Create/update your memory block structure to contain the essential information you need to know about the user, your behavior (learned preferences), the project you're working in, and any other information that will help you be an effective collaborator.

**Deep research initialization** (~100+ tool calls):
- Everything in standard initialization, plus:
- Use your TODO or Plan tool to create a systematic research plan
- Deep dive into git history for patterns, conventions, and context
- Analyze commit message conventions and branching strategy
- Explore multiple directories and understand architecture thoroughly
- Search for and read key source files to understand patterns
- Create multiple specialized memory blocks
- May involve multiple rounds of exploration

**What deep research can uncover:**
- **Contributors & team dynamics**: Who works on what areas? Who are the main contributors? (`git shortlog -sn`)
- **Coding habits**: When do people commit? (time patterns) What's the typical commit size?
- **Writing & commit style**: How verbose are commit messages? What conventions are followed?
- **Code evolution**: How has the architecture changed? What major refactors happened?
- **Review patterns**: Are there PR templates? What gets reviewed carefully vs rubber-stamped?
- **Pain points**: What areas have lots of bug fixes? What code gets touched frequently?
- **Related repositories**: Ask the user if there are other repos you should know about (e.g., a backend monorepo, shared libraries, documentation repos). These relationships can be crucial context.

This kind of deep context can make you significantly more effective as a long-term collaborator on the project.

If the user says "take as long as you need" or explicitly wants deep research, use your TODO or Plan tool to orchestrate a thorough, multi-step research process.

## Research Techniques

**File-based research:**
- README.md, CONTRIBUTING.md, AGENTS.md, CLAUDE.md
- Package manifests (package.json, Cargo.toml, pyproject.toml, go.mod)
- Config files (.eslintrc, tsconfig.json, .prettierrc)
- CI/CD configs (.github/workflows/, .gitlab-ci.yml)

**Git-based research** (if in a git repo):
- `git log --oneline -20` - Recent commit history and patterns
- `git branch -a` - Branching strategy
- `git log --format="%s" -50 | head -20` - Commit message conventions
- `git shortlog -sn --all | head -10` - Main contributors
- `git log --format="%an <%ae>" | sort -u` - Contributors with emails (more reliable for deduplication)
- Recent PRs or merge commits for context on ongoing work

**Important: Deduplicate contributors!** Git groups by exact author string, so the same person may appear multiple times with different names (e.g., "jsmith" and "John Smith" are likely the same person). Use emails to deduplicate, and apply common sense - usernames often match parts of full names.

## How to Do Thorough Research

**Don't just collect data - analyze and cross-reference it.**

Shallow research (bad):
- Run commands, copy output
- Take everything at face value
- List facts without understanding

Thorough research (good):
- **Cross-reference findings**: If two pieces of data seem inconsistent, dig deeper
- **Resolve ambiguities**: Don't leave questions unanswered (e.g., "are these two contributors the same person?")
- **Read actual content**: Don't just list file names - read key files to understand them
- **Look for patterns**: What do the commit messages tell you about workflow? What do file structures tell you about architecture?
- **Form hypotheses and verify**: "I think this team uses feature branches" ‚Üí check git branch patterns to confirm
- **Think like a new team member**: What would you want to know on your first day?

**Questions to ask yourself during research:**
- Does this make sense? (e.g., why would there be two contributors with similar names?)
- What's missing? (e.g., no tests directory - is testing not done, or done differently?)
- What can I infer? (e.g., lots of "fix:" commits in one area ‚Üí that area is buggy or complex)
- Am I just listing facts, or do I understand the project?

The goal isn't to produce a report - it's to genuinely understand the project and how this human(s) works so you can be an effective collaborator.

## On Asking Questions

**Ask important questions upfront, then be autonomous during execution.**

### Recommended Upfront Questions

You should ask these questions at the start (bundle them together in one AskUserQuestion call):

1. **Research depth**: "Standard or deep research (comprehensive, as long as needed)?"
2. **Identity**: "Which contributor are you?" (You can often infer this from git logs - e.g., if git shows "cpacker" as a top contributor, ask "Are you cpacker?")
3. **Related repos**: "Are there other repositories I should know about and consider in my research?" (e.g., backend monorepo, shared libraries)
4. **Memory updates**: "How often should I check if I should update my memory?" with options "Frequent (every 3-5 turns)" and "Occasional (every 8-10 turns)". This should be a binary question with "Memory" as the header.
5. **Communication style**: "Terse or detailed responses?"
6. **Any specific rules**: "Rules I should always follow?"

**Why these matter:**
- Identity lets you correlate git history to the user (their commits, PRs, coding style)
- Related repos provide crucial context (many projects span multiple repos)
- Workflow/communication style should be stored in the `human` block
- Rules go in `persona` block

### What NOT to ask

- Things you can find by reading files ("What's your test framework?")
- "What kind of work do you do? Reviewing PRs vs writing code?" - obvious from git log, most devs do everything
- Permission for obvious actions - just do them
- Questions one at a time - bundle them (but don't exhaust the user with too many questions at once)

**During execution**, be autonomous. Make reasonable choices and proceed.

## Memory Block Strategy

### Split Large Blocks

**Don't create monolithic blocks.** If a block is getting long (>50-100 lines), split it:

Instead of one huge `project` block, consider:
- `project-overview`: High-level description, tech stack, repo links
- `project-commands`: Build, test, lint, dev commands
- `project-conventions`: Commit style, PR process, code style
- `project-architecture`: Directory structure, key modules
- `project-gotchas`: Footguns, things to watch out for

This makes memory more scannable and easier to update and share with other agents.

### Update Memory Incrementally

**For deep research: Update memory as you go, not all at once at the end.**

Why this matters:
- Deep research can take many turns and millions of tokens
- Context windows overflow and trigger rolling summaries
- If you wait until the end to write memory, you may lose important details
- Write findings to memory blocks as you discover them

Good pattern:
1. Create block structure early (even with placeholder content)
2. Update blocks after each research phase
3. Refine and consolidate at the end

Remember, your memory tool allows you to easily add, edit, and remove blocks. There's no reason to wait until you "know everything" to write memory. Treat your memory blocks as a living scratchpad.

### Initialize ALL Relevant Blocks

Don't just update a single memory block. Based on your upfront questions, also update:

- **`human`**: Store the user's identity, workflow preferences, communication style
- **`persona`**: Store rules the user wants you to follow, behavioral adaptations
- **`project-*`**: Split project info across multiple focused blocks

And add memory blocks that you think make sense to add (e.g., `project-architecture`, `project-conventions`, `project-gotchas`, etc, or even splitting the `human` block into more focused blocks, or even multiple blocks for multiple users).

## Your Task

1. **Ask upfront questions**: Use AskUserQuestion with the recommended questions above (bundled together). This is critical - don't skip it.
2. **Inspect existing memory**: You may already have some memory blocks initialized. See what already exists, and analyze how it is or is not insufficient or incomplete.
3. **Identify the user**: From git logs and their answer, figure out who they are and store in `human` block. If relevant, ask questions to gather information about their preferences that will help you be a useful assistant to them.
4. **Update human/persona early**: Based on answers, update your memory blocks eagerly before diving into project research. You can always change them as you go, you're not locked into any memory configuration.
5. **Research the project**: Explore based on chosen depth. Use your TODO or plan tool to create a systematic research plan.
6. **Create/update project blocks incrementally**: Don't wait until the end - write findings as you go.
7. **Reflect and review**: See "Reflection Phase" below - this is critical for deep research.
8. **Ask user if done**: Check if they're satisfied or want you to continue refining.

## Reflection Phase (Critical for Deep Research)

Before finishing, you MUST do a reflection step. **Your memory blocks are visible to you in your system prompt right now.** Look at them carefully and ask yourself:

1. **Redundancy check**: Are there blocks with overlapping content? Either literally overlapping (due to errors while making memory edits), or semantically/conceptually overlapping?

2. **Completeness check**: Did you actually update ALL relevant blocks? For example:
   - Did you update `human` with the user's identity and preferences?
   - Did you update `persona` with behavioral rules they expressed?
   - Or did you only update project blocks and forget the rest?

3. **Quality check**: Are there typos, formatting issues, or unclear descriptions in your blocks?

4. **Structure check**: Would this make sense to your future self? Is anything missing? Is anything redundant?

**After reflection**, fix any issues you found. Then ask the user:
> "I've completed the initialization. Here's a brief summary of what I set up: [summary]. Should I continue refining, or is this good to proceed?"

This gives the user a chance to provide feedback or ask for adjustments before you finish.

Remember: Good memory management is an investment. The effort you put into organizing your memory now will pay dividends as you work with this user over time.


============================================================
END FILE: .fleet/factory/skills/initiate-memory/SKILL.md
============================================================

============================================================
FILE: .fleet/factory/skills/prompt-refiner-claude/SKILL.md
============================================================

---
name: prompt-refiner-claude
description: Refine prompts for Claude models (Opus, Sonnet, Haiku) using Anthropic's best practices. Use when preparing complex tasks for Claude.
---

# Claude Prompt Refiner

## When to Use
Invoke this skill when you have a task for Claude that:
- Involves multiple steps or files
- Requires specific output formatting
- Needs careful reasoning or analysis
- Would benefit from structured context

## Refinement Process

### 1. Analyze the Draft Prompt
Review the user's prompt for:
- [ ] Clear outcome definition
- [ ] Sufficient context
- [ ] Explicit constraints
- [ ] Success criteria

### 2. Apply Claude-Specific Patterns

**Structure with XML tags:**
- `<context>` - Background information, codebase state
- `<task>` - The specific action to take
- `<requirements>` - Must-have criteria
- `<constraints>` - Limitations and boundaries
- `<examples>` - Sample inputs/outputs if helpful

**Ordering matters:**
1. Context first (what exists)
2. Task second (what to do)
3. Requirements third (how to do it)
4. Examples last (clarifying edge cases)

### 3. Enhance for Reasoning
For complex tasks, add:
- "Think through the approach before implementing"
- "Consider these edge cases: ..."
- "Explain your reasoning for key decisions"

### 4. Output the Refined Prompt
Present the improved prompt with:
- Clear section headers
- XML tags where beneficial
- Specific, measurable criteria

## Example Transformation

**Before:**
"Add caching to the API"

**After:**

============================================================
END FILE: .fleet/factory/skills/prompt-refiner-claude/SKILL.md
============================================================

============================================================
FILE: .fleet/factory/skills/prompt-refiner-gpt/SKILL.md
============================================================

---
name: prompt-refiner-gpt
description: Refine prompts for GPT models (GPT-5, GPT-5.1, Codex) using OpenAI's best practices. Use when preparing complex tasks for GPT.
---

# GPT Prompt Refiner

## When to Use
Invoke this skill when you have a task for GPT that:
- Requires a specific persona or expertise
- Involves procedural steps
- Needs structured output
- Benefits from explicit examples

## Refinement Process

### 1. Analyze the Draft Prompt
Review for:
- [ ] Clear role/persona definition
- [ ] Step-by-step breakdown (if procedural)
- [ ] Output format specification
- [ ] Concrete examples

### 2. Apply GPT-Specific Patterns

**Role framing:**
Start with "You are a [specific role] working on [specific context]..."

**Numbered procedures:**
Break complex tasks into numbered steps that build on each other.

**Output specification:**
Be explicit: "Return as JSON", "Format as markdown with headers", etc.

**Chain of thought:**
For reasoning tasks, add: "Think through this step by step."

### 3. Structure the Prompt

**Effective order for GPT:**
1. Role definition (who/what)
2. Context (background info)
3. Task (what to do)
4. Steps (how to do it, if procedural)
5. Output format (what to return)
6. Examples (optional clarification)

### 4. Output the Refined Prompt
Present with:
- Clear role statement
- Numbered steps where applicable
- Explicit output requirements

## Example Transformation

**Before:**
"Review this code for security issues"

**After:**

============================================================
END FILE: .fleet/factory/skills/prompt-refiner-gpt/SKILL.md
============================================================

============================================================
FILE: .fleet/factory/skills/python-backend-reviewer/SKILL.md
============================================================

---
name: python-backend-reviewer
description: Expert Python backend code reviewer that identifies over-complexity, duplicates, bad optimizations, and violations of best practices. Use when asked to review Python code quality, check for duplicate code, analyze module complexity, optimize backend code, identify anti-patterns, or ensure adherence to best practices. Ideal for preventing AI-generated code from creating unnecessary files instead of imports, finding repeated validation logic, and catching over-engineered solutions.
---

# Python Backend Code Reviewer

Expert analysis and refactoring of Python backend code to eliminate duplication, reduce complexity, and enforce best practices.

## Overview

This skill helps identify and fix common issues in Python backend code, particularly problems introduced by AI code generation:

- **Duplicate code** across multiple files
- **Recreated utilities** instead of imports
- **Over-engineered** solutions
- **High complexity** functions and classes
- **Anti-patterns** and code smells
- **Concurrency issues** in async code (shared state mutation)

The skill provides automated analysis tools and comprehensive refactoring guidance.

## ‚ö†Ô∏è Architecture-Aware Prioritization

**Static analysis finds issues, but architectural context determines priority.**

Before prioritizing fixes, identify:

1. **Critical paths**: Which code runs on every request?
   - WebSocket/HTTP handlers
   - Main workflow orchestration
   - Shared services/middleware

2. **Secondary paths**: Less critical code
   - CLI tools
   - Scripts
   - Dev-only utilities
   - One-time migrations

3. **Concurrency model**: How is state shared?
   - Are handlers concurrent?
   - Are instances shared across requests?
   - Is there mutable singleton state?

**Prioritization rule**: Correctness in critical paths > Complexity in secondary paths

| Finding | Critical Path | Secondary Path |
|---------|---------------|----------------|
| Shared state mutation | üî¥ Fix immediately | üü° Review |
| High complexity (>25) | üü° Refactor carefully | üü¢ Backlog |
| Duplicates | üü° Extract if >3 occurrences | üü¢ Nice to have |
| God class | üü° Migrate to fa√ßade | üü¢ Low priority |

## Pragmatic Thresholds

For **orchestration/workflow code**, use realistic thresholds:

| Metric | Strict Threshold | Pragmatic Threshold | Notes |
|--------|------------------|---------------------|-------|
| Cyclomatic complexity | 10 | **25** | Orchestrators naturally have decision points |
| Function length | 50 lines | **150 lines** | Async flows can be longer |
| Nesting depth | 4 | **5** | Guard clauses help more than extracting |
| God class methods | 20 | **N/A** | OK if it's a **fa√ßade** that delegates |

**Hard limits (always fix):**
- No functions > 300 lines
- No nesting > 7 levels
- No shared-state mutation without synchronization guard

## Quick Start

### 1. Run Automated Analysis

Start with automated tools to identify issues:

```bash
# Detect duplicate code blocks
uv run python scripts/detect_duplicates.py <path>

# Analyze imports and utility reimplementation
uv run python scripts/analyze_imports.py <path>

# Check code complexity
uv run python scripts/complexity_analyzer.py <path>

# Check for concurrency issues (shared state mutation)
uv run python scripts/concurrency_analyzer.py <path>
```

### 2. Review Analysis Results

Each tool outputs:
- **Severity levels**: Warnings (must fix) vs Info (should review)
- **File locations**: Exact line numbers for each issue
- **Specific recommendations**: What to change and why

### 3. Apply Fixes

Use the reference guides to refactor issues:
- See [refactoring_patterns.md](references/refactoring_patterns.md) for step-by-step fixes
- See [python_antipatterns.md](references/python_antipatterns.md) for anti-pattern examples
- See [best_practices.md](references/best_practices.md) for Python conventions

## Main Workflows

### Review a Python File

When a user asks to review a specific file:

1. **Run all analysis tools** on the file:
   ```bash
   python scripts/detect_duplicates.py path/to/file.py
   python scripts/analyze_imports.py path/to/file.py
   python scripts/complexity_analyzer.py path/to/file.py
   ```

2. **Analyze results** and categorize issues:
   - Critical: Duplicates, high complexity, security issues
   - Important: Utility reimplementation, deep nesting
   - Minor: Style issues, minor inefficiencies

3. **Provide specific fixes**:
   - Quote exact code locations with line numbers
   - Show before/after examples
   - Explain why the change improves the code

4. **Offer to implement fixes** if requested

### Check Backend for Duplicates

When a user asks to check a project/module for duplicates:

1. **Run duplicate detection** on the entire directory:
   ```bash
   python scripts/detect_duplicates.py src/
   ```

2. **Group duplicates by severity**:
   - High: 10+ lines duplicated, 3+ occurrences
   - Medium: 5-10 lines, 2+ occurrences
   - Low: Helper functions that could be extracted

3. **Recommend consolidation strategy**:
   - Extract to shared utilities for cross-cutting concerns
   - Create base classes for inherited behavior
   - Use decorators for repeated patterns

### Analyze Module Over-Engineering

When code appears over-engineered:

1. **Run complexity analysis**:
   ```bash
   python scripts/complexity_analyzer.py --max-complexity 10 --max-length 50 path/
   ```

2. **Identify over-engineering patterns**:
   - Premature abstractions (base classes with one implementation)
   - Excessive configuration options
   - God classes (20+ methods)
   - Deep inheritance hierarchies

3. **Suggest simplifications**:
   - Replace abstractions with simple functions
   - Remove unused configuration
   - Split god classes by responsibility
   - Flatten inheritance

4. **Reference specific patterns** from [python_antipatterns.md](references/python_antipatterns.md)

### Optimize Following Best Practices

When asked to optimize code or ensure best practices:

1. **Run all analysis tools** to get baseline metrics

2. **Check against best practices**:
   - DRY principle violations
   - SOLID principle violations
   - Type hint coverage
   - Error handling patterns
   - Async/await consistency

3. **Prioritize optimizations**:
   - First: Correctness (bugs, security)
   - Second: Maintainability (duplicates, complexity)
   - Third: Performance (N+1 queries, inefficiencies)
   - Fourth: Style (naming, imports)

4. **Reference [best_practices.md](references/best_practices.md)** for specific guidelines

### Analyze Concurrency Safety

When reviewing async code that handles concurrent requests:

1. **Run concurrency analysis**:
   ```bash
   uv run python scripts/concurrency_analyzer.py services/ workflows/
   ```

2. **Prioritize by severity**:
   - **Critical**: Fix before production deployment
   - **Warning**: Review for actual sharing patterns
   - **Info**: Consider but often acceptable

3. **Common fixes for shared state mutation**:
   ```python
   # ‚ùå Before: Mutating shared instance state
   class Workflow:
       async def run(self, task):
           self.current_task = task  # Race condition!

   # ‚úÖ After: Request-scoped state
   class Workflow:
       async def run(self, task):
           execution = ExecutionContext(task=task)
           return await self._execute(execution)
   ```

4. **Alternative patterns**:
   - Pass state through parameters (preferred)
   - Use `contextvars` for request-scoped data
   - Use `asyncio.Lock` for truly shared state
   - Create new instances per request

## Analysis Tools

### detect_duplicates.py

Finds duplicate code blocks using AST analysis.

**Usage:**
```bash
uv run python scripts/detect_duplicates.py <path>
uv run python scripts/detect_duplicates.py --min-lines 10 <path>
```

**Detects:**
- Duplicate functions (identical implementations)
- Duplicate classes
- Repeated code blocks

**Options:**
- `--min-lines N`: Minimum lines for a block to be considered (default: 5)

### analyze_imports.py

Analyzes import organization and detects recreated utilities.

**Usage:**
```bash
uv run python scripts/analyze_imports.py <path>
```

**Detects:**
- Wildcard imports (`from module import *`)
- Relative imports in non-package contexts
- Functions that look like reimplemented utilities
- Common patterns that should use libraries

**Common utilities flagged:**
- JSON serialization ‚Üí use `json` or `orjson`
- Retry logic ‚Üí use `tenacity` or `backoff`
- Validation ‚Üí use `pydantic`
- HTTP clients ‚Üí use `requests` or `httpx`

### complexity_analyzer.py

Measures cyclomatic complexity, function length, and nesting depth.

**Usage:**
```bash
uv run python scripts/complexity_analyzer.py <path>
uv run python scripts/complexity_analyzer.py --max-complexity 10 --max-length 50 <path>
```

**Metrics:**
- **Cyclomatic complexity**: Number of decision points (default threshold: 10)
- **Function length**: Lines in function (default threshold: 50)
- **Nesting depth**: Maximum levels of nested control structures (threshold: 4)
- **God classes**: Classes with 20+ methods

**Options:**
- `--max-complexity N`: Cyclomatic complexity threshold (default: 10)
- `--max-length N`: Function length threshold (default: 50)

### concurrency_analyzer.py

Detects concurrency issues in async Python code.

**Usage:**
```bash
uv run python scripts/concurrency_analyzer.py <path>
```

**Detects:**
- Shared state mutation in async methods (`self.x = y` in async def)
- Module-level mutable state (shared across requests)
- Missing synchronization patterns
- Potentially unsafe singleton patterns

**Severity levels:**
- **Critical**: Mutation of shared state like `client`, `session`, `agent`, `config`
- **Warning**: Any `self.attr` mutation in async context
- **Info**: Module-level mutable objects

**When to use:** Run on services, handlers, and workflow code that handles concurrent requests.

## Reference Documentation

### python_antipatterns.md

Comprehensive catalog of anti-patterns with examples:

- Code duplication patterns
- Over-engineering examples
- God objects
- Complexity issues
- Import problems
- Error handling mistakes
- Performance anti-patterns

**Use when:** You identify an issue but need to see the anti-pattern and solution

### refactoring_patterns.md

Step-by-step refactoring techniques:

- Extract function/variable
- Consolidate duplicates
- Simplify conditionals
- Break up god classes
- Reduce complexity
- Improve imports

**Use when:** You know what's wrong and need concrete refactoring steps

### best_practices.md

Python backend best practices and principles:

- Core principles (DRY, SOLID)
- Code organization
- Type hints
- Error handling
- Async patterns
- Database practices
- API design
- Security guidelines

**Use when:** Establishing coding standards or need authoritative guidance

## Example Reviews

### Example 1: Duplicate Validation Logic

**User request:** "Review this code for quality issues"

**Analysis:**
```bash
uv run python scripts/detect_duplicates.py api/
```

**Finding:** Email validation duplicated in 5 files

**Recommendation:**
```python
# Extract to utils/validation.py
