<!-- Chunk 1199: bytes 4520970-4546449, type=class -->
class SkillAnalytics {
  async analyzeUsagePatterns(userId, timeframe) {
    const usage = await this.getUsageData(userId, timeframe);
    
    return {
      mostUsedSkills: this.rankByFrequency(usage),
      skillCombinations: this.findCommonCombos(usage),
      underutilizedSkills: this.findUnderutilized(usage),
      suggestedSkills: this.predictNeededSkills(usage),
      generationSuccess: this.measureGeneratedSkillQuality(usage)
    };
  }
  
  async optimizeUserTaxonomy(userId) {
    const analytics = await this.analyzeUsagePatterns(userId, '30d');
    
    // Pre-generate frequently needed skills
    for (const skill of analytics.suggestedSkills) {
      if (skill.predictedFrequency > 0.7) {
        await this.skillGenerator.generateSkillOnDemand(
          skill.path, 
          { userId, priority: 'high' }
        );
      }
    }
    
    // Archive rarely used skills
    for (const skill of analytics.underutilizedSkills) {
      if (skill.lastUsed > 90) { // days
        await this.archiveSkill(skill.path);
      }
    }
  }
}
```

## Implementation Checklist

### ‚úÖ Phase 1: Foundation (Do First)
- [ ] Create directory structure with .gitkeep files
- [ ] Implement 3-5 core skills (reasoning, communication, state_mgmt)
- [ ] Build taxonomy_meta.json with versioning
- [ ] Create skill generation templates
- [ ] Implement basic skill loader/mounter

### ‚úÖ Phase 2: Bootstrap System
- [ ] Design onboarding questionnaire
- [ ] Create 4-6 bootstrap profiles (web dev, data science, devops, etc.)
- [ ] Map questions ‚Üí skill paths
- [ ] Implement profile-based skill generation
- [ ] Test with diverse user personas

### ‚úÖ Phase 3: Dynamic Generation
- [ ] Integrate existing skill generation workflow
- [ ] Implement on-demand generation triggers
- [ ] Build skill caching mechanism
- [ ] Add dependency resolution
- [ ] Create skill versioning system

### ‚úÖ Phase 4: Intelligence
- [ ] Implement usage tracking
- [ ] Build analytics dashboard
- [ ] Create skill recommendation engine
- [ ] Add automatic optimization
- [ ] Implement collaborative learning (optional)

## Directory Structure: Recommended Approach

**YES - Create empty structure:**
```
‚úÖ Create all top-level categories with .gitkeep
‚úÖ This provides clear navigation and intent
‚úÖ Makes it obvious what can be generated
‚úÖ Minimal overhead (just empty directories)
```

**NO - Don't pre-populate everything:**
```
‚ùå Don't create all 200+ leaf skills upfront
‚ùå Don't write detailed specs for unused skills
‚ùå Don't maintain unused skill definitions
```

## Sample Initial Structure

```bash
# Create this minimal but complete structure
mkdir -p skills/{_core,_templates,cognitive_skills,technical_skills/{programming,data_engineering,infrastructure,security,apis_integration},domain_knowledge,tool_proficiency,mcp_capabilities,specializations,task_focus_areas,memory_blocks}

# Add .gitkeep to empty dirs
find skills -type d -empty -exec touch {}/.gitkeep \;

# Core files only
touch skills/_core/{reasoning,communication,state_management}.json
touch skills/mcp_capabilities/{context_management,state_management,tool_integration}.json
touch skills/memory_blocks/{project_context,interaction_history}.json
touch skills/taxonomy_meta.json
```

## Migration Path

```javascript
// Version 0.1.0 ‚Üí 0.2.0 ‚Üí 1.0.0
const migrationStrategy = {
  "0.1.0": {
    skills: 8,  // Core only
    generation: "on_demand",
    users: "early_adopters"
  },
  "0.2.0": {
    skills: "~50",  // Bootstrap profiles generated
    generation: "profile_based + on_demand",
    users: "beta_users",
    features: ["usage_analytics", "skill_recommendations"]
  },
  "1.0.0": {
    skills: "200+",  // Comprehensive coverage
    generation: "predictive + collaborative",
    users: "production",
    features: ["full_analytics", "auto_optimization", "cross_user_learning"]
  }
};
```

## Key Advantages of This Approach

1. **Low Initial Overhead**: Start with 8-10 skills instead of 200+
2. **User-Driven Growth**: Skills evolve based on actual usage
3. **Quality Over Quantity**: Generated skills are contextually relevant
4. **Fast Onboarding**: Users get personalized capability sets immediately
5. **Continuous Improvement**: System learns from usage patterns
6. **Maintainable**: Only maintain actively used skills
7. **Scalable**: Can support diverse user bases without bloat

## Success Metrics

Track these to validate your approach:
- Time to first productive task (target: < 5 minutes)
- Skill generation success rate (target: > 90%)
- Skill reuse rate (target: > 60% reused within 30 days)
- User satisfaction with skill availability (target: > 4.5/5)
- System performance (target: < 100ms skill mounting time)

This progressive approach lets you ship quickly, learn from real usage, and scale intelligently.

============================================================
END FILE: plans/archive/skills-taxonomy-implementation-strategy.md
============================================================

============================================================
FILE: plans/archive/taxonomy-system.md
============================================================

# Hierarchical Skills Taxonomy for Agentic Systems

## Overview
This taxonomy enables dynamic skill composition where agents acquire only necessary capabilities based on task requirements, maintaining efficiency while preserving state across interactions.

## Core Structure

```
ROOT
‚îú‚îÄ‚îÄ COGNITIVE_SKILLS
‚îÇ   ‚îú‚îÄ‚îÄ Analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Data_Analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Statistical_Analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Pattern_Recognition
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Anomaly_Detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Text_Analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Semantic_Analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Sentiment_Analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Entity_Extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Code_Analysis
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Static_Analysis
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Complexity_Analysis
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Dependency_Analysis
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Synthesis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Content_Generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Technical_Writing
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Creative_Writing
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Code_Generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Algorithm_Implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Boilerplate_Generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Test_Generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Design_Synthesis
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Architecture_Design
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ API_Design
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ UI_UX_Design
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Planning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Task_Decomposition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Resource_Allocation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dependency_Mapping
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Risk_Assessment
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Reasoning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Logical_Reasoning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Causal_Reasoning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Probabilistic_Reasoning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Constraint_Satisfaction
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Learning
‚îÇ       ‚îú‚îÄ‚îÄ Pattern_Learning
‚îÇ       ‚îú‚îÄ‚îÄ Contextual_Adaptation
‚îÇ       ‚îî‚îÄ‚îÄ Error_Correction
‚îÇ
‚îú‚îÄ‚îÄ TECHNICAL_SKILLS
‚îÇ   ‚îú‚îÄ‚îÄ Programming
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Languages
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Python
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Core_Python
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Async_Programming
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Package_Management
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ JavaScript_TypeScript
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ES6_Modern_JS
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Node_Runtime
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Browser_APIs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Systems_Languages
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Rust
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ C_Cpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Domain_Specific
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ SQL
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Shell_Scripting
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Configuration_Languages
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Paradigms
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Object_Oriented
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Functional
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Reactive
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Concurrent_Parallel
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Practices
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Testing
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Debugging
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Refactoring
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Code_Review
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Data_Engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Storage
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Relational_Databases
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NoSQL_Databases
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Graph_Databases
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Vector_Databases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Processing
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ETL_Pipelines
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Stream_Processing
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Batch_Processing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Modeling
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Schema_Design
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Data_Normalization
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Data_Warehousing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cloud_Platforms
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AWS_Services
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Azure_Services
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GCP_Services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Containerization
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Docker
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Kubernetes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Infrastructure_as_Code
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Terraform
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ CloudFormation
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Ansible
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Security
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Authentication_Authorization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Encryption
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Vulnerability_Assessment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Secure_Coding
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ APIs_Integration
‚îÇ       ‚îú‚îÄ‚îÄ REST_APIs
‚îÇ       ‚îú‚îÄ‚îÄ GraphQL
‚îÇ       ‚îú‚îÄ‚îÄ WebSockets
‚îÇ       ‚îî‚îÄ‚îÄ Message_Queues
‚îÇ
‚îú‚îÄ‚îÄ DOMAIN_KNOWLEDGE
‚îÇ   ‚îú‚îÄ‚îÄ Machine_Learning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Supervised_Learning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Unsupervised_Learning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Reinforcement_Learning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Deep_Learning
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ NLP_Understanding
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Language_Models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Text_Processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Information_Extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dialogue_Systems
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Computer_Vision
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Image_Processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Object_Detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Image_Generation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Business_Intelligence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Metrics_KPIs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Reporting
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Forecasting
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Scientific_Computing
‚îÇ       ‚îú‚îÄ‚îÄ Numerical_Methods
‚îÇ       ‚îú‚îÄ‚îÄ Simulation
‚îÇ       ‚îî‚îÄ‚îÄ Optimization
‚îÇ
‚îú‚îÄ‚îÄ TOOL_PROFICIENCY
‚îÇ   ‚îú‚îÄ‚îÄ Development_Tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ IDEs_Editors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Version_Control
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Build_Systems
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Package_Managers
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Data_Tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Data_Analysis_Libraries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Visualization_Tools
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ETL_Tools
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Monitoring_Observability
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Metrics_Collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tracing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Alerting
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Collaboration_Tools
‚îÇ       ‚îú‚îÄ‚îÄ Project_Management
‚îÇ       ‚îú‚îÄ‚îÄ Documentation_Systems
‚îÇ       ‚îî‚îÄ‚îÄ Communication_Platforms
‚îÇ
‚îú‚îÄ‚îÄ MCP_CAPABILITIES
‚îÇ   ‚îú‚îÄ‚îÄ Context_Management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Context_Switching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Context_Persistence
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Context_Retrieval
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Tool_Integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tool_Discovery
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tool_Invocation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Tool_Composition
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Resource_Access
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Filesystem_Access
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Database_Access
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ API_Access
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Memory_Access
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ State_Management
‚îÇ       ‚îú‚îÄ‚îÄ Session_State
‚îÇ       ‚îú‚îÄ‚îÄ Workflow_State
‚îÇ       ‚îî‚îÄ‚îÄ Knowledge_State
‚îÇ
‚îú‚îÄ‚îÄ SPECIALIZATIONS
‚îÇ   ‚îú‚îÄ‚îÄ Frontend_Development
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ React_Ecosystem
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Vue_Ecosystem
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Responsive_Design
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Performance_Optimization
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Backend_Development
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ API_Development
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Microservices
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Database_Design
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Caching_Strategies
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ DevOps_SRE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CI_CD_Pipelines
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Deployment_Automation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Incident_Response
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Capacity_Planning
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Data_Science
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Exploratory_Analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Model_Development
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Feature_Engineering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Model_Deployment
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ AI_ML_Engineering
‚îÇ       ‚îú‚îÄ‚îÄ Model_Training
‚îÇ       ‚îú‚îÄ‚îÄ Prompt_Engineering
‚îÇ       ‚îú‚îÄ‚îÄ Agent_Development
‚îÇ       ‚îî‚îÄ‚îÄ LLM_Integration
‚îÇ
‚îú‚îÄ‚îÄ TASK_FOCUS_AREAS
‚îÇ   ‚îú‚îÄ‚îÄ Build_Create
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Prototype_Development
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Feature_Implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ System_Architecture
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Content_Creation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Debug_Fix
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Bug_Identification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Root_Cause_Analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Fix_Implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Regression_Prevention
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Optimize_Improve
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Performance_Tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Code_Refactoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Architecture_Refinement
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Process_Improvement
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Research_Explore
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Technology_Evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Proof_of_Concept
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Feasibility_Study
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Competitive_Analysis
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Maintain_Support
‚îÇ       ‚îú‚îÄ‚îÄ Monitoring_Alerting
‚îÇ       ‚îú‚îÄ‚îÄ Documentation_Updates
‚îÇ       ‚îú‚îÄ‚îÄ Dependency_Management
‚îÇ       ‚îî‚îÄ‚îÄ User_Support
‚îÇ
‚îî‚îÄ‚îÄ MEMORY_BLOCKS
    ‚îú‚îÄ‚îÄ Project_Context
    ‚îÇ   ‚îú‚îÄ‚îÄ Project_Goals
    ‚îÇ   ‚îú‚îÄ‚îÄ Constraints_Requirements
    ‚îÇ   ‚îú‚îÄ‚îÄ Architecture_Decisions
    ‚îÇ   ‚îî‚îÄ‚îÄ Team_Conventions
    ‚îÇ
    ‚îú‚îÄ‚îÄ Interaction_History
    ‚îÇ   ‚îú‚îÄ‚îÄ Previous_Decisions
    ‚îÇ   ‚îú‚îÄ‚îÄ Attempted_Solutions
    ‚îÇ   ‚îú‚îÄ‚îÄ User_Preferences
    ‚îÇ   ‚îî‚îÄ‚îÄ Error_Patterns
    ‚îÇ
    ‚îú‚îÄ‚îÄ Knowledge_Base
    ‚îÇ   ‚îú‚îÄ‚îÄ Domain_Facts
    ‚îÇ   ‚îú‚îÄ‚îÄ Best_Practices
    ‚îÇ   ‚îú‚îÄ‚îÄ Code_Patterns
    ‚îÇ   ‚îî‚îÄ‚îÄ Troubleshooting_Guides
    ‚îÇ
    ‚îî‚îÄ‚îÄ Skill_Evolution
        ‚îú‚îÄ‚îÄ Learned_Patterns
        ‚îú‚îÄ‚îÄ Performance_Metrics
        ‚îú‚îÄ‚îÄ Capability_Growth
        ‚îî‚îÄ‚îÄ Adaptation_Logs
```

## Dynamic Skill Mounting Strategy

### 1. **Planner Agent Workflow**
```
Input: User Task
  ‚Üì
Parse Task Intent
  ‚Üì
Identify Required Skill Branches
  ‚Üì
Calculate Skill Dependencies
  ‚Üì
Optimize Skill Set (minimize overhead)
  ‚Üì
Mount Skills to Worker Agent(s)
  ‚Üì
Monitor & Adapt
```

### 2. **Skill Selection Criteria**
- **Primary Skills**: Direct task requirements
- **Supporting Skills**: Dependencies and enablers
- **Context Skills**: Project-specific knowledge
- **Adaptive Skills**: Based on interaction history

### 3. **Mounting Levels**
- **Level 1 - Core**: Always loaded (reasoning, basic I/O)
- **Level 2 - Task-Specific**: Mounted for current task
- **Level 3 - Contextual**: Available on-demand
- **Level 4 - Dormant**: Cached but not loaded

### 4. **State Management**
```json
{
  "session_id": "unique_session",
  "mounted_skills": [
    "COGNITIVE_SKILLS.Analysis.Code_Analysis",
    "TECHNICAL_SKILLS.Programming.Python",
    "TOOL_PROFICIENCY.Development_Tools.Version_Control"
  ],
  "context_stack": [
    {
      "task": "debug_authentication",
      "active_memory_blocks": ["Project_Context", "Interaction_History"],
      "skill_usage_metrics": {}
    }
  ],
  "evolution_log": []
}
```

## Implementation Example

### Scenario: Debug API Authentication Issue

**Planner Analysis:**
- Primary Task: Debug_Fix.Bug_Identification
- Required Skills:
  - TECHNICAL_SKILLS.Security.Authentication_Authorization
  - TECHNICAL_SKILLS.Programming.Python.Core_Python
  - TECHNICAL_SKILLS.APIs_Integration.REST_APIs
  - COGNITIVE_SKILLS.Analysis.Code_Analysis
  - TOOL_PROFICIENCY.Development_Tools.IDEs_Editors

**Memory Blocks Activated:**
- Project_Context (API architecture, auth patterns)
- Interaction_History (previous auth issues)
- Knowledge_Base (OAuth best practices)

**Worker Configuration:**
```json
{
  "worker_type": "debugging_specialist",
  "loaded_skills": ["minimal_set_from_taxonomy"],
  "memory_access": ["relevant_blocks"],
  "tool_access": ["git", "debugger", "api_client"],
  "context_window": "optimized_for_task"
}
```

## Benefits

1. **Efficiency**: Load only necessary skills
2. **Scalability**: Multiple workers with different skill sets
3. **Adaptability**: Skills evolve based on project needs
4. **State Continuity**: Memory blocks maintain context
5. **Specialization**: Workers can have distinct expertise profiles

This taxonomy enables flexible, efficient agentic systems that grow with project complexity while maintaining optimal resource usage.

============================================================
END FILE: plans/archive/taxonomy-system.md
============================================================

============================================================
FILE: pyproject.toml
============================================================

[project]
name = "skill-fleet"
version = "0.3.5"
description = "Hierarchical skills taxonomy + DSPy workflow prototype"
readme = "README.md"
requires-python = ">=3.12"
authors = [{ name = "Qredence" }]
license = { text = "Apache-2.0" }
keywords = ["agents", "skills", "dspy", "fastapi", "cli", "workflow"]
dependencies = [
  "asyncpg>=0.31.0",
  "aiosqlite>=0.17.0",
  "click>=8.3.1",
  "datasets>=4.4.2",
  "dspy>=3.1.2,<4",
  "fastapi[standard]>=0.128.0",
  "google-genai>=1.56.0",
  "httpx>=0.28.1",
  "litellm[proxy]>=1.80.16",
  "mlflow>=3.8.1",
  "openai>=2.14.0",
  "psycopg[binary]>=3.2.5",
  "pydantic>=2.12.5",
  "pyyaml>=6.0.3",
  "python-dotenv>=1.2.1",
  "rich>=13.7.1",
  "sqlalchemy>=2.0.45",
  "tomli>=2.0",
  "typer>=0.21.1",
  "typing-extensions>=4.0",
  "uvicorn[standard]>=0.31.1",
  "prompt-toolkit>=3.0.52",
  "ty>=0.0.12",
  "gepa>=0.0.24",
  "questionary>=2.0.0",
  "greenlet>=3.3.0",
]
classifiers = [
  "Development Status :: 3 - Alpha",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: Apache Software License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.12",
  "Topic :: Software Development :: Libraries :: Application Frameworks",
]

[project.urls]
Homepage = "https://github.com/Qredence/skill-fleet"
Repository = "https://github.com/Qredence/skill-fleet"
"Bug Tracker" = "https://github.com/Qredence/skill-fleet/issues"
Documentation = "https://github.com/Qredence/skill-fleet/tree/main/docs"

[project.scripts]
fleet-agent = "skill_fleet.cli:cli_entrypoint"
skill-fleet = "skill_fleet.cli:cli_entrypoint"

[project.optional-dependencies]
dev = [
  "httpx>=0.28.1",
  "pytest>=8.0.0",
  "pytest-asyncio>=1.3.0",
  "pytest-cov>=7.0.0",
  "ruff>=0.8.0",
]

[dependency-groups]
dev = [
  "bandit[toml]>=1.8.0",
  "pre-commit>=4.0.1",
  "pytest>=8.0.0",
  "pytest-asyncio>=1.3.0",
  "pytest-cov>=7.0.0",
  "ruff>=0.8.0",
]

[build-system]
requires = ["hatchling>=1.26.0"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/skill_fleet"]
include = ["src/skill_fleet/config/**", "src/skill_fleet/_seed/**"]

[tool.pytest.ini_options]
pythonpath = ["src"]
testpaths = ["tests"]
asyncio_mode = "auto"
markers = [
  "integration: marks tests as integration tests (may require LLM access)",
  "slow: marks tests as slow running",
]
filterwarnings = [
  "ignore:Pydantic serializer warnings:UserWarning",
  # DeprecationWarning from aiohttp/connector.py in python 3.13
  "ignore:enable_cleanup_closed ignored:DeprecationWarning",
]

[tool.ruff]
src = ["src", "skills"]
target-version = "py312"
line-length = 100
exclude = ["skills/**"]

[tool.ruff.lint]
# Enable comprehensive rule set for 2026 best practices
select = [
  "E",     # pycodestyle errors
  "F",     # pyflakes
  "I",     # isort (import sorting)
  "UP",    # pyupgrade (modernize Python syntax)
  "B",     # flake8-bugbear (detect common bugs)
  "D",     # pydocstyle (docstring conventions)
  "N",     # pep8-naming (naming conventions)
  "C4",    # flake8-comprehensions (optimize comprehensions)
  "SIM",   # flake8-simplify (simplify code)
  "TC001", # Type checking - typing imports in runtime code
  "TC002", # Type checking - immutable default in mutable context
  "TC003", # Type checking - import from typing instead of collections.abc
  "D100",  # Missing module docstring
  "D101",  # Missing class docstring
  "D102",  # Missing method docstring
  "D103",  # Missing function docstring
  "D104",  # Missing package docstring
]
ignore = [
  "E501", # Line length handled by formatter
  "D212", # Alternative to D213 (multi-line docstring style)
  "D203", # Alternative to D211 (blank line before class)
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["D"] # Disable docstring checks for tests
# B008: Do not perform function call in default arguments
# Standard pattern for CLI libraries (typer, click)
"src/skill_fleet/cli/*.py" = [
  "B008",
  "D413",
  "D401",
  "SIM102",
] # Allow flexible docstring formatting and nested ifs in CLI
"src/skill_fleet/cli/**/*.py" = [
  "D413",
  "D401",
  "TC002",
  "TC003",
  "SIM102",
] # Relax docstring, type-checking, and simplification rules in CLI submodules
"src/skill_fleet/cli/interactive_cli.py" = [
  "TC001",
] # Allow runtime imports for type hints
"src/skill_fleet/cli/commands/db_sync.py" = [
  "D400",
  "D415",
] # Relax docstring ending rules for module docstrings
"src/skill_fleet/cli/exceptions.py" = ["N818"] # Allow CLIExit exception name

# API and core modules - relax type-checking rules for runtime imports
"src/skill_fleet/api/*.py" = [
  "TC001",
  "TC002",
  "TC003",
  "SIM105",
  "D401",
  "D107",
  "N818",
  "SIM110",
]
"src/skill_fleet/api/**/*.py" = ["TC001", "TC002", "TC003", "D417", "SIM110"]
"src/skill_fleet/api/exceptions.py" = ["N818"] # Allow APIException naming
"src/skill_fleet/analytics/*.py" = ["D107", "N803", "N806"]
"src/skill_fleet/core/**/*.py" = [
  "TC003",
  "SIM105",
  "D401",
  "D107",
  "D301",
  "SIM108",
]
"src/skill_fleet/common/*.py" = ["TC003", "D107", "D301"]
"src/skill_fleet/taxonomy/*.py" = [
  "SIM105",
  "SIM108",
  "SIM110",
  "SIM222",
  "D301",
]
"src/skill_fleet/validators/*.py" = ["D107", "D301"]

# Scripts - relaxed rules for utility scripts
"scripts/**/*.py" = [
  "D",
  "TC001",
  "TC002",
  "TC003",
  "SIM105",
  "SIM108",
  "SIM110",
  "SIM222",
  "D401",
  "E402",
]

# Tests - relaxed rules
"tests/**/*.py" = ["D", "TC001", "TC003", "SIM114", "SIM117", "SIM222", "N806"]

[tool.ruff.format]
# Explicit formatting configuration
quote-style = "double"
indent-style = "space"
line-ending = "auto"
skip-magic-trailing-comma = false

[tool.bandit]
# Bandit security linter configuration
exclude_dirs = ["tests", ".venv", "venv", ".git"]
skips = [
  # Skip assert_used - we use assertions for internal consistency checks
  "B101",
  # Skip hardcoded_password_func - we use environment variables for secrets
  "B105",
  "B106",
]
assert_used = false

[tool.ty.environment]
# Python version to assume (affects syntax and stdlib types)
python-version = "3.12"

# Path to Python environment for third-party modules (auto-detected by ty)
# python = "./.venv"

# Target platform ("win32", "darwin", "linux", "android", "ios", "all")
# ty will auto-detect from current platform
# python-platform = "linux"

# First-party module search paths (priority order)
root = ["./src"]

[tool.ty.rules]
# Set rule severities: "error", "warn", or "ignore"

# Critical errors
division-by-zero = "error"
possibly-unresolved-reference = "error"
invalid-argument-type = "error"
unsupported-operator = "error"

# Code quality warnings
unused-ignore-comment = "warn"
redundant-cast = "warn"

# Relaxed rules for false positives
index-out-of-bounds = "warn"

[tool.ty.src]
# Include specific files/directories
include = ["src", "tests"]

[tool.ty.terminal]
output-format = "full"
error-on-warning = false

# Relaxed rules for tests
[[tool.ty.overrides]]
include = ["tests/**"]

[tool.ty.overrides.rules]
possibly-unresolved-reference = "warn"


============================================================
END FILE: pyproject.toml
============================================================

============================================================
FILE: scripts/README.md
============================================================

# Skill Fleet Scripts

This directory contains utility scripts for the Skill Fleet framework, organized by function.

## üöÄ Entry Points

These are the high-level tools you should use first:

| Script | Description |
|--------|-------------|
| `manage_db.py` | Database management (init, migrate, seed, verify). |
| `manage_data.py` | Training data management (extract, synthetic, prepare). |
| `manage_opt.py` | DSPy optimization management (mipro, gepa, benchmark). |
| `check.py` | Development tools (quality, dev, cleanup). |

## üóÇÔ∏è Internal Scripts

Implementation scripts are organized in `scripts/internal/`:

| Directory | Purpose |
|-----------|---------|
| `internal/db/` | Database utilities (migrations, taxonomy). |
| `internal/opt/` | DSPy optimization scripts. |
| `internal/data/` | Training data generation. |
| `internal/tests/` | Test runner and validation scripts. |
| `internal/dev/` | Development utilities (dev server, quality checks). |
| `internal/setup/` | Setup and configuration scripts. |

## Usage Examples

### Database Setup
```bash
# Initialize schema and seed data
uv run python scripts/manage_db.py init

# Verify database state
uv run python scripts/manage_db.py verify
```

### Training Data Workflow
```bash
# Prepare training data (extract from skills + synthetic)
uv run python scripts/manage_data.py prepare

```

### Optimization Workflow
```bash
# Run MIPROv2 optimization
uv run python scripts/manage_opt.py mipro

# Run GEPA optimization
uv run python scripts/manage_opt.py gepa

# Compare optimizers
uv run python scripts/manage_opt.py benchmark
```

### Development
```bash
# Run full quality suite (lint, test, typecheck)
uv run python scripts/check.py quality

# Start development server & TUI
uv run python scripts/check.py dev

# Run technical debt cleanup
uv run python scripts/check.py cleanup
```

### Legacy Scripts
The `scripts/archive/` directory contains older scripts that have been consolidated into the new manager tools. These are kept for reference but should not be used directly.


============================================================
END FILE: scripts/README.md
============================================================

============================================================
FILE: scripts/archive/apply_migration.py
============================================================

#!/usr/bin/env python3
"""
Skills-Fleet Migration Script

This script applies the database migration to your Neon database.
Run this script to initialize the database schema.
"""

import os
import sys

import psycopg
from dotenv import load_dotenv

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from skill_fleet.db.database import DATABASE_URL

# Load environment variables
load_dotenv()


