<!-- Chunk 277: bytes 351116-352261, type=function -->
def find_duplicates(paths: list[Path], min_lines: int = 5) -> dict[str, list[dict]]:
    """
    Find duplicate code blocks across files.

    Args:
        paths: List of Python file paths to analyze
        min_lines: Minimum lines for a block to be considered (default: 5)

    Returns:
        Dictionary mapping hash to list of duplicate blocks
    """
    hash_map: dict[str, list[dict]] = defaultdict(list)

    for path in paths:
        try:
            with open(path, encoding="utf-8") as f:
                source = f.read()

            tree = ast.parse(source, filename=str(path))
            extractor = CodeBlockExtractor(str(path))
            extractor.visit(tree)

            for block in extractor.blocks:
                # Filter out small blocks
                if block["code"].count("\n") >= min_lines:
                    hash_map[block["hash"]].append(block)

        except (SyntaxError, UnicodeDecodeError) as e:
            print(f"⚠️  Skipping {path}: {e}", file=sys.stderr)
            continue

    # Keep only hashes with duplicates
    return {h: blocks for h, blocks in hash_map.items() if len(blocks) > 1}


