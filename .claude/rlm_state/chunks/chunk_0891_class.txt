<!-- Chunk 891: bytes 3147845-3160449, type=class -->
class HITLStrategyModule(dspy.Module):
    """Determine optimal HITL strategy for a task."""
```

**Implementation**: `dspy.ChainOfThought(DetermineHITLStrategy)`

**Input:**
```python
task_description: str
task_complexity: str  # simple/moderate/complex
user_preferences: str
```

**Output:**
```python
{
    "strategy": str,  # minimal/standard/thorough
    "checkpoints": list[str],
    "reasoning": str,
}
```

**Strategies:**
- **minimal**: 2 checkpoints (essential only)
- **standard**: 4 checkpoints (recommended)
- **thorough**: 6 checkpoints (comprehensive)

---

## Async Support

All modules support async execution via `aforward()`:

```python
# Preferred for API/CLI (non-blocking)
result = await module.aforward(input="...")

# Sync wrapper (for scripting)
result = module.forward(input="...")
```

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
The `aforward()` methods use `asyncio.to_thread()` to run DSPy's synchronous operations in a thread pool. This allows the FastAPI server and CLI to handle multiple concurrent skill creation requests without blocking.
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

## Module Selection Guide

| Task | Recommended Module | Rationale |
|------|-------------------|-----------|
| Direct transformation | `dspy.Predict` | Fastest, no reasoning needed |
| Complex decision making | `dspy.ChainOfThought` | Explicit reasoning improves quality |
| Iterative improvement | `dspy.Refine` | Comparison-based refinement |
| Maximizing quality | `dspy.BestOfN` | Generate N candidates, pick best |

## Metrics Module

**File**: `src/skill_fleet/core/dspy/metrics/skill_quality.py`

The metrics module provides quality assessment functions for evaluating generated skills.

### assess_skill_quality

Main entry point for skill quality evaluation.

```python
from skill_fleet.core.dspy.metrics import assess_skill_quality, SkillQualityScores

scores: SkillQualityScores = assess_skill_quality(skill_content)
print(f"Overall: {scores.overall_score}")
print(f"Issues: {scores.issues}")
```

**Input:**
```python
skill_content: str  # Raw SKILL.md content
weights: dict[str, float] | None = None  # Optional custom weights
```

**Output:**
```python
SkillQualityScores(
    # Structure scores
    frontmatter_completeness: float,
    has_overview: bool,
    has_when_to_use: bool,
    has_quick_reference: bool,
    
    # Pattern scores
    pattern_count: int,
    has_anti_patterns: bool,
    has_production_patterns: bool,
    has_key_insights: bool,
    
    # Practical value scores
    has_common_mistakes: bool,
    has_red_flags: bool,
    has_real_world_impact: bool,
    
    # Code quality scores
    code_examples_count: int,
    code_examples_quality: float,
    
    # Obra/superpowers quality indicators
    has_core_principle: bool,
    has_strong_guidance: bool,
    has_good_bad_contrast: bool,
    description_quality: float,
    
    # Overall
    overall_score: float,
    issues: list[str],
    strengths: list[str],
)
```

### skill_quality_metric

DSPy-compatible metric function for use with optimizers.

```python
from skill_fleet.core.dspy.metrics import skill_quality_metric
from dspy.teleprompt import MIPROv2

optimizer = MIPROv2(
    metric=skill_quality_metric,
    auto="medium",
)
```

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
The metrics are calibrated against golden skills from [Obra/superpowers](https://github.com/obra/superpowers). A penalty multiplier is applied for missing critical elements (core principle, strong guidance, good/bad contrast), ensuring stricter scoring that differentiates excellent skills from mediocre ones.
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

See [Evaluation Documentation](evaluation.md) for detailed metrics information.

---

## See Also

- **[Signatures Documentation](signatures.md)** - Input/output contracts
- **[Programs Documentation](programs.md)** - Module orchestration
- **[Evaluation Documentation](evaluation.md)** - Quality metrics and assessment
- **[DSPy Overview](index.md)** - Architecture and concepts


============================================================
END FILE: docs/dspy/modules.md
============================================================

============================================================
FILE: docs/dspy/optimization.md
============================================================

# Optimizer Auto-Selection Guide

The Optimizer Auto-Selection Engine intelligently recommends the best DSPy optimizer based on your task characteristics, budget, and quality goals.

## Quick Start

### CLI Usage

```bash
# Auto-select optimizer based on trainset and budget
uv run skill-fleet optimize --auto-select --budget 10.0 --quality-target 0.85

# With time constraint
uv run skill-fleet optimize --auto-select --time-limit 5 --budget 20.0

# Specify trainset
uv run skill-fleet optimize --auto-select \
  --trainset config/training/trainset_v4.json \
  --budget 15.0
```

### API Usage

```bash
curl -X POST http://localhost:8000/api/v1/optimization/recommend \
  -H "Content-Type: application/json" \
  -d '{
    "trainset_size": 50,
    "budget_dollars": 10.0,
    "quality_target": 0.85,
    "domain": "testing"
  }'
```

### Python Usage

```python
from skill_fleet.core.dspy.optimization.selector import (
    OptimizerContext,
    OptimizerSelector,
)

# Create context
context = OptimizerContext(
    trainset_size=50,
    budget_dollars=10.0,
    quality_target=0.85,
    domain="testing",
)

# Get recommendation
selector = OptimizerSelector()
result = selector.recommend(context)

print(f"Recommended: {result.recommended.value}")
print(f"Config: auto={result.config.auto}")
print(f"Cost: ${result.estimated_cost:.2f}")
print(f"Time: {result.estimated_time_minutes} min")
print(f"Confidence: {result.confidence:.0%}")
print(f"Reasoning: {result.reasoning}")
```

## Decision Rules

| Condition | Recommended Optimizer | Why |
|-----------|----------------------|-----|
| time < 2 min | Reflection Metrics | Fastest (<1 sec) |
| budget < $1 | Reflection Metrics | Cheapest ($0.01-0.05) |
| trainset < 100 AND budget < $5 | GEPA | Fast + cheap for small data |
| trainset < 500 AND budget < $20 | MIPROv2 (light) | Good balance |
| trainset >= 500 AND budget >= $20 | MIPROv2 (medium) | Best quality |
| budget >= $100 | MIPROv2 (heavy) | Maximum quality |
| budget >= $100 AND quality >= 0.95 | BootstrapFinetune | Weight tuning |
| Fallback | BootstrapFewShot | Always works |

## Cost Estimates

| Optimizer | Cost per 100 examples | Time per 100 examples |
|-----------|----------------------|----------------------|
| Reflection Metrics | $0.05 | < 1 min |
| BootstrapFewShot | $0.10 | 1 min |
| GEPA | $0.50 | 5 min |
| MIPROv2 (light) | $1.25 | 7 min |
| MIPROv2 (medium) | $2.50 | 15 min |
| MIPROv2 (heavy) | $6.25 | 40 min |
| BootstrapFinetune | $20.00 | 60 min |

## Metrics Tracking

Results are automatically recorded to `config/selector_metrics.jsonl` for future learning. The system uses historical data to improve confidence in recommendations.

## API Response Example

```json
{
  "recommended": "miprov2",
  "config": {
    "auto": "light",
    "max_bootstrapped_demos": 2,
    "max_labeled_demos": 4,
    "num_threads": 8
  },
  "estimated_cost": 1.25,
  "estimated_time_minutes": 7,
  "confidence": 0.82,
  "reasoning": "Medium trainset (50) + moderate budget ($10.00). MIPROv2 'light' provides good quality/cost balance.",
  "alternatives": [
    {
      "optimizer": "reflection_metrics",
      "cost": "$0.03",
      "time": "< 1 min",
      "quality_risk": 0.15,
      "note": "Fastest option, may sacrifice some quality"
    },
    {
      "optimizer": "bootstrap_fewshot",
      "cost": "$0.05",
      "time": "1 min",
      "quality_risk": 0.10,
      "note": "Safe fallback, always works"
    }
  ]
}
```

## CLI Output Example

```
============================================================
ðŸ¤– Optimizer Auto-Selection
============================================================
Trainset size: 50
Budget: $10.00
Quality target: 0.85

âœ… Recommended: miprov2
   Config: auto=light
   Estimated cost: $1.25
   Estimated time: 7 minutes
   Confidence: 75%

ðŸ“ Reasoning: Medium trainset (50) + moderate budget ($10.00). 
   MIPROv2 'light' provides good quality/cost balance.

ðŸ”„ Alternatives:
   - reflection_metrics: $0.03 | < 1 min | Risk: +15%
   - bootstrap_fewshot: $0.05 | 1 min | Risk: +10%
============================================================

Proceed with recommended optimizer? [Y/n]:
```

## Advanced Usage

### Recording Results for Learning

The selector can learn from past optimization results:

```python
# After optimization completes
selector.record_result(
    context=context,
    optimizer=OptimizerType.MIPROV2,
    actual_cost=1.50,
    actual_time_minutes=8,
    quality_score=0.87,
)
```

### Custom Metrics Path

```python
selector = OptimizerSelector(
    metrics_path="path/to/custom_metrics.jsonl"
)
```

### Context Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `trainset_size` | int | required | Number of training examples |
| `budget_dollars` | float | 10.0 | Maximum budget in USD |
| `quality_target` | float | 0.85 | Target quality (0.0-1.0) |
| `complexity_score` | float | 0.5 | Task complexity (0.0-1.0) |
| `domain` | str | "general" | Skill domain/category |
| `time_constraint_minutes` | int | None | Max time allowed |
| `previous_optimizer` | str | None | Last optimizer used |
| `historical_quality` | float | None | Previous quality score |

## Troubleshooting

### "Recommendation confidence is low"

This happens when:
- Trainset is very small (< 20 examples)
- No historical data matches your domain
- Edge case parameters

**Solution**: Consider collecting more training data or using the recommended optimizer as a baseline.

### "Estimated cost too high"

**Solutions**:
1. Use `--time-limit` to force faster/cheaper options
2. Reduce `--budget` to get cheaper recommendations
3. Use `--optimizer reflection_metrics` for instant, cheap optimization

### "Wrong optimizer was selected"

The selector learns from results. After optimization:
1. Record the result with `record_result()`
2. Future recommendations will factor in your domain's history


============================================================
END FILE: docs/dspy/optimization.md
============================================================

============================================================
FILE: docs/dspy/programs.md
============================================================

# DSPy Programs Reference

**Last Updated**: 2026-01-12
**Location**: `src/skill_fleet/core/programs/`

## Overview

DSPy programs **orchestrate** multiple modules to accomplish complex workflows. While modules implement individual steps, programs coordinate the entire skill creation process across phases, with integrated HITL (Human-in-the-Loop) checkpoints.

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
Programs are the "glue code" of the workflow. They handle orchestration logic like: which phase to run next, when to pause for human input, how to handle errors, and how to combine outputs from multiple modules.
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

## Available Programs

| Program                      | Purpose                                           | File                                                 |
| ---------------------------- | ------------------------------------------------- | ---------------------------------------------------- |
| `SkillCreationProgram`       | New 3-phase orchestrator (current default)        | `skill_creator.py`                                   |
| `LegacySkillCreationProgram` | Legacy 4-phase skill creation workflow            | `programs.py`                                        |
| ~~`GuidedCreatorProgram`~~   | ~~Chat-based guided skill creation~~ (DEPRECATED) | ~~`conversational.py`~~                              |
| `ConversationalOrchestrator` | Current chat-based workflow                       | `workflows/conversational_interface/orchestrator.py` |

## SkillCreationProgram

**File**: `src/skill_fleet/core/dspy/skill_creator.py`

The modern, unified orchestrator for the 3-phase skill creation workflow.

## LegacySkillCreationProgram

**File**: `src/skill_fleet/core/dspy/programs.py`

The original orchestrator for the complete 4-phase skill creation workflow with integrated HITL. Renamed from `SkillCreationProgram` to avoid naming conflicts with the new unified architecture.

### Class Definition

```python
