<!-- Chunk 1335: bytes 4994000-5030719, type=function -->
def mcts_search(root_node, iterations=50):
    for _ in range(iterations):
        node = select_leaf(root_node)
        child = expand(node)
        reward = simulate(child) # Rollout or PRM
        backpropagate(child, reward)
    return best_child(root_node).state

============================================================
END FILE: skills/_drafts/recur/llm-inference/recursive-language-models/examples/example_3.py
============================================================

============================================================
FILE: skills/_drafts/recur/llm-inference/recursive-language-models/metadata.json
============================================================

{
  "skill_id": "llm-inference/recursive-language-models",
  "name": "recursive-language-models",
  "description": "Use when standard LLM generation fails on complex logic, math, or coding tasks due to 'one-pass' limitations. This skill provides the framework for implementing iterative reasoning loops, process-based verification (PRMs), and path optimization strategies like MCTS to scale inference-time compute for high-reasoning accuracy.",
  "version": "1.0.0",
  "type": "technical",
  "weight": "heavyweight",
  "load_priority": "task_specific",
  "dependencies": [
    "_core/reasoning",
    "_core/state_management"
  ],
  "capabilities": [
    "recursive-step-generation",
    "process-based-verification",
    "inference-search-optimization",
    "compute-budget-management"
  ],
  "category": "llm-inference",
  "keywords": [
    "rlm",
    "inference-time-compute",
    "mcts",
    "prm",
    "reasoning-loops",
    "o1-style-inference"
  ],
  "scope": "Covers the implementation of recursive inference loops, search algorithms (MCTS, Best-of-N), and step-wise verification models. Excludes model training, fine-tuning, and low-level hardware optimizations like CUDA kernels.",
  "see_also": [
    "_core/reasoning",
    "mcp_capabilities/tool_integration"
  ],
  "tags": [
    "ai-engineering",
    "inference-scaling",
    "search-algorithms",
    "reasoning"
  ],
  "created_at": "2026-01-24T18:26:05.755103+00:00",
  "last_modified": "2026-01-24T18:26:05.755103+00:00",
  "evolution": {
    "created_by": "skill-fleet-api",
    "workflow": "SkillCreationProgram",
    "validation_score": 0.96
  }
}


============================================================
END FILE: skills/_drafts/recur/llm-inference/recursive-language-models/metadata.json
============================================================

============================================================
FILE: skills/_drafts/recur/llm-inference/recursive-language-models/tests/test_1.json
============================================================

{
  "name": "Backtracking Verification",
  "description": "Verify the model can recover when the PRM rejects a candidate step.",
  "input_data": "Task: Solve a math riddle where the first logical step is intentionally deceptive.",
  "expected_result": "The model should generate Step 1, have it rejected by the verifier, and successfully generate an alternative Step 1a."
}

============================================================
END FILE: skills/_drafts/recur/llm-inference/recursive-language-models/tests/test_1.json
============================================================

============================================================
FILE: skills/_drafts/recur/llm-inference/recursive-language-models/tests/test_2.json
============================================================

{
  "name": "Budget Termination",
  "description": "Ensure the system stops when the token budget is reached, returning the best path found so far.",
  "input_data": "Task: Complex optimization problem with budget=1000 tokens.",
  "expected_result": "System terminates at ~1000 tokens and returns the path with the highest PRM score."
}

============================================================
END FILE: skills/_drafts/recur/llm-inference/recursive-language-models/tests/test_2.json
============================================================

============================================================
FILE: skills/_drafts/recur/taxonomy_meta.json
============================================================

{
  "total_skills": 1,
  "generation_count": 1,
  "statistics": {
    "by_type": {
      "technical": 1
    },
    "by_weight": {
      "heavyweight": 1
    },
    "by_priority": {
      "task_specific": 1
    }
  },
  "last_updated": "2026-01-24T18:26:05.766461+00:00"
}


============================================================
END FILE: skills/_drafts/recur/taxonomy_meta.json
============================================================

============================================================
FILE: skills/_templates/skill_template.json
============================================================

{
  "$comment": "skill-fleet v2 Golden Standard Template",
  "skill_styles": {
    "navigation_hub": {
      "description": "Short SKILL.md (~100-300 lines) with rich subdirectories",
      "best_for": ["Complex technical skills", "Many patterns to document"],
      "examples": ["dspy-basics", "neon-drizzle"]
    },
    "comprehensive": {
      "description": "Long detailed SKILL.md (~400-800 lines) self-contained",
      "best_for": ["Procedural skills", "Workflows", "Checklists"],
      "examples": ["vibe-coding", "product-management"]
    },
    "minimal": {
      "description": "Concise SKILL.md (~50-150 lines) focused on one thing",
      "best_for": ["Simple tools", "Single patterns"],
      "examples": ["utility skills"]
    }
  },
  "directory_structure": [
    "references/",
    "guides/",
    "templates/",
    "scripts/",
    "examples/"
  ],
  "required_files": ["SKILL.md"],
  "optional_files": [
    "references/*.md",
    "guides/*.md",
    "templates/*",
    "scripts/*",
    "examples/*/README.md"
  ],
  "subdirectory_purposes": {
    "references": "Deep technical documentation, API references, pattern catalogs",
    "guides": "Step-by-step workflows, troubleshooting guides, tutorials",
    "templates": "Boilerplate code files for copy-paste reuse",
    "scripts": "Runnable utility scripts, automation helpers",
    "examples": "Complete runnable demo projects with READMEs"
  },
  "deprecated": {
    "directories": ["capabilities/", "resources/", "tests/"],
    "files": ["metadata.json"],
    "note": "Existing skills with these structures are grandfathered; new skills should use v2 structure"
  }
}


============================================================
END FILE: skills/_templates/skill_template.json
============================================================

============================================================
FILE: skills/mcp_capabilities/context_management.json
============================================================

{
  "skill_id": "mcp_capabilities/context_management",
  "version": "1.0.0",
  "type": "mcp",
  "weight": "lightweight",
  "load_priority": "always",
  "always_loaded": true,
  "dependencies": [],
  "capabilities": [
    "context_switching",
    "context_persistence",
    "context_retrieval"
  ],
  "description": "Essential context management capabilities for stateful operations",
  "created_at": "2026-01-06T00:00:00Z",
  "last_modified": "2026-01-06T00:00:00Z"
}



============================================================
END FILE: skills/mcp_capabilities/context_management.json
============================================================

============================================================
FILE: skills/mcp_capabilities/state_management.json
============================================================

{
  "skill_id": "mcp_capabilities/state_management",
  "version": "1.0.0",
  "type": "mcp",
  "weight": "lightweight",
  "load_priority": "always",
  "always_loaded": true,
  "dependencies": [
    "_core/state_management"
  ],
  "capabilities": [
    "session_state",
    "workflow_state",
    "knowledge_state"
  ],
  "description": "State management for MCP-driven workflows",
  "created_at": "2026-01-06T00:00:00Z",
  "last_modified": "2026-01-06T00:00:00Z"
}



============================================================
END FILE: skills/mcp_capabilities/state_management.json
============================================================

============================================================
FILE: skills/mcp_capabilities/tool_integration.json
============================================================

{
  "skill_id": "mcp_capabilities/tool_integration",
  "version": "1.0.0",
  "type": "mcp",
  "weight": "lightweight",
  "load_priority": "always",
  "always_loaded": true,
  "dependencies": [],
  "capabilities": [
    "tool_discovery",
    "tool_invocation",
    "tool_composition"
  ],
  "description": "Essential tool integration capabilities for MCP toolchains",
  "created_at": "2026-01-06T00:00:00Z",
  "last_modified": "2026-01-06T00:00:00Z"
}



============================================================
END FILE: skills/mcp_capabilities/tool_integration.json
============================================================

============================================================
FILE: skills/memory_blocks/interaction_history.json
============================================================

{
  "skill_id": "memory_blocks/interaction_history",
  "version": "1.0.0",
  "type": "memory",
  "weight": "lightweight",
  "load_priority": "always",
  "always_loaded": true,
  "dependencies": [],
  "capabilities": [
    "interaction_history_capture",
    "interaction_history_retrieval"
  ],
  "description": "Memory block for interaction history continuity",
  "created_at": "2026-01-06T00:00:00Z",
  "last_modified": "2026-01-06T00:00:00Z"
}



============================================================
END FILE: skills/memory_blocks/interaction_history.json
============================================================

============================================================
FILE: skills/memory_blocks/project_context.json
============================================================

{
  "skill_id": "memory_blocks/project_context",
  "version": "1.0.0",
  "type": "memory",
  "weight": "lightweight",
  "load_priority": "always",
  "always_loaded": true,
  "dependencies": [],
  "capabilities": [
    "project_context_capture",
    "project_context_retrieval"
  ],
  "description": "Memory block for project-level context continuity",
  "created_at": "2026-01-06T00:00:00Z",
  "last_modified": "2026-01-06T00:00:00Z"
}



============================================================
END FILE: skills/memory_blocks/project_context.json
============================================================

============================================================
FILE: skills/practices/rlm/SKILL.md
============================================================

---
name: rlm
description: Process very large context files (logs, docs, transcripts, scraped webpages) that exceed context limits by chunking content, delegating analysis to subagents, and synthesizing results. Use when files are >100k characters and require iterative inspection and extraction across the entire document.
allowed-tools:
  - Read
  - Write
  - Edit
  - Grep
  - Glob
  - Bash
  - Task
---

# RLM (Recursive Language Model Workflow)

A workflow for processing large context files that won't fit in a single conversation by chunking content, delegating chunk analysis to subagents, and synthesizing results.

## When to Use This Skill

Use RLM when:
- The user provides or references a **very large context file** (>100k chars) - logs, documentation, transcripts, scraped webpages
- The file is too large to load entirely into the conversation context
- You need to **iteratively inspect, search, chunk, and extract** information from the entire file
- The task requires **analyzing multiple sections** independently before synthesis
- You need to maintain state across multiple operations on the same file

**Trigger phrases:**
- "Analyze this large log file..."
- "Extract all mentions of X from this transcript..."
- "Summarize this 500-page documentation..."
- "Find patterns across this entire scraped website dump..."

## Mental Model

The RLM workflow has three key components:

1. **Root LM** (you) - Orchestrates the workflow, manages chunking strategy, synthesizes final results
2. **Persistent REPL** (`rlm_repl.py`) - Maintains state, provides helpers for chunking/searching/extraction
3. **Sub-LM** (subagent) - Analyzes individual chunks and returns structured results

## Quick Start

### Minimal Example

```bash
# 1. Initialize with your large file
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init /path/to/large_file.txt

# 2. Scout the content
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(f'Total chars: {len(content):,}')"
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 1000))"

# 3. Create chunks
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=150000, overlap=5000)
print(f"Created {len(paths)} chunks")
for i, p in enumerate(paths[:3]):
    print(f"  {i}: {p}")
PY

# 4. Process chunks with subagent (see Subagent Approaches section)

# 5. Synthesize results in main conversation
```

## Complete Workflow

### 1. Parse Arguments

This skill expects `$ARGUMENTS` in one of these formats:
- `context=/path/to/file.txt query="What are the main errors?"`
- `context=/path/to/file.txt query="Extract all usernames" chunk_chars=200000`

Optional parameters:
- `chunk_chars=<int>` - Chunk size in characters (default: ~200000)
- `overlap_chars=<int>` - Overlap between chunks (default: 0)

If the user didn't provide arguments, ask for:
1. The context file path
2. The query/task

### 2. Initialize REPL State

```bash
# Initialize with the context file
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init /path/to/context.txt

# Check status
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py status
```

The REPL creates a persistent state file at `.letta/rlm_state/state.pkl` containing:
- The full context content
- Buffers for accumulating results
- Any persisted variables from exec commands

### 3. Scout the Context

Get a quick sense of the content structure:

```bash
# Peek at start
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 3000))"

# Peek at end
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(len(content)-3000, len(content)))"

# Quick search for structure
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "hits = grep(r'^#+ ', max_matches=10); print([h['match'] for h in hits])"
```

### 4. Choose Chunking Strategy

**Semantic chunking** (preferred when structure is clear):
- Markdown files: Split on heading boundaries (`^#+ `)
- JSON: Split on top-level objects/arrays
- Logs: Split on timestamp boundaries
- Code: Split on function/class boundaries

**Character-based chunking** (fallback):
- Use when no clear semantic boundaries exist
- Recommended size: 150,000-250,000 chars
- Add overlap (5,000-10,000 chars) to preserve context at boundaries

### 5. Materialize Chunks

Write chunks to disk so subagents can read them:

```bash
# Character-based chunking
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=200000, overlap=10000)
print(f"Created {len(paths)} chunks:")
for i, p in enumerate(paths):
    print(f"  chunk_{i}: {p}")
PY

# Semantic chunking example (markdown headings)
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
import re
# Find all heading positions
headings = list(re.finditer(r'^#+ .+$', content, re.MULTILINE))
# Create chunks between headings
# ... (custom logic based on content structure)
PY
```

### 6. Process Chunks with Subagent

You have two approaches for delegating chunk analysis:

#### Approach A: Letta Task Tool (Recommended)

```python
# For each chunk, spawn a subagent task
Task({
    "subagent_type": "general-purpose",
    "description": f"Analyze chunk {i}",
    "prompt": f"""Analyze this chunk and extract all error messages.

Read the chunk: {chunk_path}

Return a JSON object with:
- errors: list of error messages found
- severity: list of severity levels
- context: brief context for each error

Be concise and structured."""
})
```

#### Approach B: Custom rlm-subcall Subagent

If you've set up a dedicated `rlm-subcall` agent (see `references/subagent-setup.md`):

```bash
# Use your preferred method to invoke rlm-subcall with:
# - The user's query
# - The chunk file path
# - Specific extraction instructions
```

**Key principles for subagent calls:**
- Keep instructions specific and task-focused
- Request structured output (JSON preferred)
- Limit subagent output size (they should extract/summarize, not dump)
- Pass chunk file path, don't paste content into prompt

### 7. Accumulate Results

Collect subagent outputs in REPL buffers or in the main conversation:

```bash
# Add subagent result to buffers
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
result = """
{chunk analysis from subagent}
"""
add_buffer(result)
print(f"Buffer count: {len(buffers)}")
PY
```

Or manually track results in the conversation as you iterate through chunks.

### 8. Synthesize Final Answer

Once all chunks are processed:

1. Review accumulated results from buffers/conversation
2. Identify patterns, trends, or comprehensive answers
3. Synthesize a coherent final response
4. Optionally: Use a subagent one final time to merge/format the collected evidence

```bash
# Export buffers for final synthesis
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py export-buffers /tmp/rlm_results.txt
```

## REPL Helper Functions

The `rlm_repl.py` script provides these helpers in exec mode:

### Variables
- `context` - Dict with keys: `path`, `loaded_at`, `content`
- `content` - String alias for `context['content']`
- `buffers` - List[str] for storing intermediate results

### Functions
- `peek(start=0, end=1000)` - Extract a substring from content
- `grep(pattern, max_matches=20, window=120, flags=0)` - Search with regex, returns matches with context
- `chunk_indices(size=200000, overlap=0)` - Calculate chunk boundaries (returns list of (start, end) tuples)
- `write_chunks(out_dir, size=200000, overlap=0, prefix='chunk')` - Write chunks to files, returns list of paths
- `add_buffer(text)` - Append text to buffers list

### Commands
- `init <context_path>` - Load a context file and create state
- `status` - Show current state (file size, buffers, vars)
- `exec -c "<code>"` - Execute Python code (state persists)
- `exec` - Execute Python code from stdin (use with heredoc)
- `export-buffers <out_path>` - Write all buffers to a file
- `reset` - Delete state file

## Guardrails

**DO:**
- Use REPL to locate and extract specific excerpts
- Keep subagent prompts focused and structured
- Request JSON or other structured formats from subagents
- Quote only necessary snippets in the main conversation

**DON'T:**
- Paste large raw chunks into the main chat context
- Spawn subagents from within subagents (orchestration stays in root)
- Store sensitive data in REPL state files
- Forget to clean up `.letta/rlm_state/` and `.letta/rlm_chunks/` when done

## File Locations

By default, RLM uses these paths:
- State: `.letta/rlm_state/state.pkl`
- Chunks: `.letta/rlm_chunks/chunk_*.txt`
- Buffers export: User-specified path

You can override the state path with `--state /custom/path.pkl` on any command.

## Advanced Usage

See `references/examples.md` for complete workflow examples including:
- Analyzing large log files
- Extracting structured data from transcripts
- Summarizing documentation
- Finding patterns in scraped web content

For subagent setup details, see `references/subagent-setup.md`.

## Troubleshooting

**"No state found" error:**
- Run `init` first with your context file path

**REPL state getting too large:**
- Use `reset` to clear state
- Initialize with a fresh context
- Consider filtering content before init if possible

**Subagent outputs too verbose:**
- Be more specific in subagent prompts
- Request structured/summarized output
- Limit output with explicit constraints

**Memory issues with very large files:**
- Use `--max-bytes` with init to cap file size
- Pre-filter or split files before processing
- Process in multiple RLM sessions if needed


============================================================
END FILE: skills/practices/rlm/SKILL.md
============================================================

============================================================
FILE: skills/practices/rlm/references/examples.md
============================================================

# RLM Workflow Examples

Complete examples of using the RLM workflow for different types of large-context tasks.

## Example 1: Analyzing Large Log Files

**Scenario:** User provides a 5MB application log file and wants all critical errors with context.

**Query:** "Extract all CRITICAL errors from app.log with timestamps and surrounding context"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init /path/to/app.log

# 2. Scout the structure
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 1000))"
# Output shows: [2024-01-23 10:15:32] INFO ...
# Logs are timestamp-prefixed, line-based

# 3. Search for critical errors
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
critical = grep(r'CRITICAL', max_matches=100, window=200)
print(f"Found {len(critical)} CRITICAL entries")
print("First 3:")
for i, hit in enumerate(critical[:3]):
    print(f"\n{i}. {hit['snippet'][:200]}...")
PY

# 4. If too many hits, chunk the file
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=150000, overlap=5000)
print(f"Created {len(paths)} chunks")
PY

# 5. Process each chunk with subagent
```

**Subagent prompt template:**
```
Read: {chunk_path}

Task: Extract all CRITICAL errors.

For each error, return JSON:
{
  "timestamp": "...",
  "message": "...",
  "surrounding_lines": "..." (3 lines before and after)
}

Return: {"errors": [...]}
```

**Synthesis:**
- Collect all error objects from subagent responses
- Sort by timestamp
- Group by error type/pattern if requested
- Present summary + full list

---

## Example 2: Extracting Structured Data from Transcripts

**Scenario:** 200-page meeting transcript, user wants all action items and decisions.

**Query:** "Extract all action items and decisions from transcript.txt"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init transcript.txt

# 2. Check format
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 2000))"
# Transcript has speaker labels: "Alice: ...", "Bob: ..."

# 3. Search for keywords
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
action_words = grep(r'\b(action item|TODO|will do|responsible for)\b', max_matches=50, flags=2)
decision_words = grep(r'\b(decided|decision|we\'ll go with|agreed)\b', max_matches=50, flags=2)
print(f"Action keywords: {len(action_words)}")
print(f"Decision keywords: {len(decision_words)}")
PY

# 4. Semantic chunking (by speaker turns or time markers)
# Or character-based if no clear structure
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=180000, overlap=8000)
print(f"{len(paths)} chunks created")
PY

# 5. Process chunks
```

**Subagent prompt:**
```
Read: {chunk_path}

Extract:
1. Action items (who, what, when)
2. Decisions made (what was decided, context)

Return JSON:
{
  "action_items": [
    {"person": "...", "task": "...", "deadline": "..."}
  ],
  "decisions": [
    {"decision": "...", "context": "...", "speaker": "..."}
  ]
}

Only include clear, explicit items. Skip vague mentions.
```

**Synthesis:**
- Deduplicate similar action items across chunks
- Organize by person/team
- Present decisions chronologically
- Flag any incomplete information

---

## Example 3: Summarizing Documentation

**Scenario:** 1000-page technical specification, user wants high-level summary and specific section.

**Query:** "Summarize the authentication section from spec.md"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init spec.md

# 2. Find authentication section
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
auth_hits = grep(r'^## Authentication', max_matches=5, window=500)
if auth_hits:
    match = auth_hits[0]
    print(f"Found at position: {match['span']}")
    print(match['snippet'])
PY

# 3. Extract just authentication section
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
# Find section boundaries (between ## headers)
import re
headers = [(m.start(), m.group(0)) for m in re.finditer(r'^## .+$', content, re.MULTILINE)]

# Find authentication header
auth_idx = None
for i, (pos, title) in enumerate(headers):
    if 'Authentication' in title:
        auth_idx = i
        break

if auth_idx is not None:
    start = headers[auth_idx][0]
    end = headers[auth_idx + 1][0] if auth_idx + 1 < len(headers) else len(content)
    auth_section = content[start:end]
    
    # Write to separate file
    from pathlib import Path
    out = Path('.letta/rlm_chunks/auth_section.txt')
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(auth_section)
    print(f"Extracted {len(auth_section):,} chars to {out}")
PY

# 4. If section is still large, chunk it
# Otherwise, process directly with subagent
```

**Subagent prompt:**
```
Read: .letta/rlm_chunks/auth_section.txt

Create a comprehensive summary of the authentication approach:

1. Overview (2-3 sentences)
2. Key components (bullet list)
3. Authentication flow (step-by-step)
4. Important notes/warnings

Format as markdown. Be thorough but concise.
```

---

## Example 4: Finding Patterns in Scraped Web Content

**Scenario:** Scraped content from 500 web pages (single concatenated file), find all mentions of pricing.

**Query:** "Find all pricing information from scraped_pages.txt"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init scraped_pages.txt

# 2. Quick search for price patterns
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
import re
price_patterns = [
    r'\$\d+(?:,\d{3})*(?:\.\d{2})?',  # $123.45
    r'\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:USD|dollars)',  # 123 dollars
    r'(?:price|cost|pricing):\s*\$?\d+',  # price: $50
]

all_prices = []
for pattern in price_patterns:
    hits = grep(pattern, max_matches=100, window=150, flags=2)
    all_prices.extend(hits)

print(f"Found {len(all_prices)} price mentions")
print("\nSample:")
for hit in all_prices[:5]:
    print(f"  {hit['match']} -> ...{hit['snippet'][:100]}...")
PY

# 3. Chunk for detailed extraction
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=200000, overlap=5000)
print(f"Created {len(paths)} chunks")
PY

# 4. Process with subagent
```

**Subagent prompt:**
```
Read: {chunk_path}

Extract all pricing information.

For each price found, return:
{
  "price": "...",
  "currency": "...",
  "context": "..." (what is being priced),
  "page_title": "..." (if identifiable)
}

Return: {"prices": [...]}

Focus on explicit pricing. Skip vague mentions.
```

**Synthesis:**
- Deduplicate identical price mentions
- Group by product/service
- Create price range summary
- Flag any ambiguous/conflicting pricing

---

## Example 5: Multi-Pass Analysis

**Scenario:** Large dataset where you need to first identify patterns, then extract based on those patterns.

**Pass 1: Discover pattern types**

```bash
# Sample randomly from content
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
import random
n = len(content)
samples = [
    peek(random.randint(0, n-5000), random.randint(0, n-5000)+5000)
    for _ in range(5)
]
for i, s in enumerate(samples):
    print(f"\n=== Sample {i} ===")
    print(s[:500])
PY
```

Use subagent to analyze samples and identify patterns.

**Pass 2: Extract based on discovered patterns**

```bash
# Chunk with strategy informed by Pass 1
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
# Use pattern-specific chunking
# Then write chunks
PY
```

Process chunks with updated extraction logic.

---

## Tips for All Workflows

1. **Start with reconnaissance**: Use `peek()` and `grep()` to understand structure before chunking
2. **Choose appropriate chunk size**: 
   - Smaller (100k) for dense, important content
   - Larger (250k) for sparse, repetitive content
3. **Add overlap for safety**: 5-10k chars prevents losing context at boundaries
4. **Validate early**: Process 1-2 chunks first, check quality before processing all
5. **Structured output**: Always request JSON or structured format from subagents
6. **Incremental synthesis**: Don't wait for all chunks - synthesize partial results as you go
7. **Clean up**: Delete `.letta/rlm_chunks/` and state files when done

## Common Patterns

### Pattern: Frequency Analysis
1. Quick grep to count occurrences
2. If high count, chunk and aggregate
3. Present sorted frequency table

### Pattern: Timeline Extraction
1. Chunk by time boundaries (if logs/events)
2. Extract chronological events from each chunk
3. Merge into master timeline
4. Sort and filter by date range

### Pattern: Hierarchical Summary
1. Chunk by document structure (sections/chapters)
2. Summarize each chunk individually
3. Create high-level summary from chunk summaries
4. Present multi-level summary (overview â†’ details)

### Pattern: Entity Extraction
1. Sample to identify entity types
2. Chunk content
3. Extract entities from each chunk
4. Deduplicate and normalize
5. Present entity catalog with frequencies


============================================================
END FILE: skills/practices/rlm/references/examples.md
============================================================

============================================================
FILE: skills/practices/rlm/references/subagent-setup.md
============================================================

# Setting Up the rlm-subcall Subagent

This guide explains how to create a dedicated `rlm-subcall` subagent for use with the RLM workflow.

## Why a Dedicated Subagent?

The `rlm-subcall` agent is optimized for:
- Analyzing individual chunks of content
- Extracting structured information
- Returning concise, focused results
- Operating with minimal context overhead

## Option 1: Using Letta Task Tool (Recommended)

The simplest approach is to use Letta's built-in Task tool without creating a dedicated agent:

```python
# Spawn a one-off subagent for each chunk
Task({
    "subagent_type": "general-purpose",
    "description": f"Extract errors from chunk {i}",
    "prompt": f"""Read and analyze: {chunk_path}

Task: {user_query}

Return structured JSON with your findings.
Be concise - extract and summarize, don't dump raw content."""
})
```

**Advantages:**
- No setup required
- Works immediately
- Flexible - adjust prompt per chunk

**Disadvantages:**
- Less specialized than a dedicated agent
- No persistent memory between chunks
- May need more detailed instructions each time

## Option 2: Creating a Dedicated rlm-subcall Agent

For repeated RLM workflows, create a specialized subagent:

### 1. Create the Agent

```bash
# Via Letta CLI
letta create agent \
  --name rlm-subcall \
  --description "Specialized agent for RLM chunk analysis" \
  --preset default
```

Or programmatically if using Letta as a library.

### 2. Configure the Agent's System Prompt

Edit the agent to include this specialized system guidance:

```markdown
You are rlm-subcall, a specialized subagent for RLM (Recursive Language Model) workflows.

## Your Role

You analyze individual chunks of a larger document and return focused, structured results.

## Key Principles

1. **Be Concise**: You're analyzing one chunk of a larger file. Return only extracted/summarized information, never dump raw content.

2. **Follow Structure**: The root agent will specify what format to use (usually JSON). Follow it exactly.

3. **No Meta-commentary**: Don't say "I analyzed the chunk and found..." Just return the requested structure.

4. **Read the Chunk**: You'll be given a file path. Read it with the Read tool.

5. **Extract, Don't Dump**: Your job is extraction and light analysis, not wholesale copying.

## Typical Task Format

You'll receive prompts like:

"""
Read: /path/to/chunk_0042.txt

Task: Extract all error messages with their timestamps.

Return JSON:
{
  "errors": [
    {"timestamp": "...", "message": "...", "severity": "..."}
  ]
}
"""

## Output Format

Prefer structured output:
- JSON for data extraction
- Markdown lists for summaries
- CSV for tabular data

Keep output under 2000 characters unless explicitly requested otherwise.
```

### 3. Test the Agent

```bash
# Test with a sample chunk
letta send --agent rlm-subcall \
  --message "Read: /tmp/test_chunk.txt\n\nTask: Count occurrences of 'ERROR' and 'WARN'.\n\nReturn JSON with counts."
```

### 4. Use in RLM Workflow

When processing chunks, invoke your rlm-subcall agent:

```python
# If using Letta Task tool with your pre-configured agent
Task({
    "agent_id": "rlm-subcall",  # Your agent ID
    "subagent_type": "general-purpose",
    "description": f"Process chunk {i}",
    "prompt": f"""Read: {chunk_path}

Task: {user_query}

Return structured JSON."""
})
```

Or use your platform's method for invoking a specific agent.

## Agent Memory Considerations

**Per-Chunk Analysis (Default):**
- Agent forgets between chunks
- Clean slate for each analysis
- Prevents context pollution

**Cross-Chunk Memory (Advanced):**
- If you need the subagent to remember findings across chunks, maintain a conversation thread
- Use conversation_id to continue the same conversation
- Be cautious of context accumulation

## Best Practices

1. **Clear Instructions**: Tell the subagent exactly what to extract and in what format
2. **Validate Output**: Check that subagent responses match expected structure
3. **Error Handling**: Provide fallback logic if subagent returns unexpected format
4. **Chunk Size**: Keep chunks small enough that subagent analysis is focused
5. **Prompt Consistency**: Use similar prompt structure for all chunks in a workflow

## Alternative: Deploy an Existing Agent

If you already have an agent optimized for analysis tasks, you can deploy it for RLM workflows:

```python
Task({
    "agent_id": "your-existing-agent-id",
    "subagent_type": "explore",  # or "general-purpose"
    "description": "Analyze chunk",
    "prompt": "..."
})
```

See the main SKILL.md for complete workflow integration.


============================================================
END FILE: skills/practices/rlm/references/subagent-setup.md
============================================================

============================================================
FILE: skills/practices/rlm/scripts/rlm_repl.py
============================================================

#!/usr/bin/env python3
"""
Persistent mini-REPL for RLM-style workflows in Claude Code.

This script provides a *stateful* Python environment across invocations by
saving a pickle file to disk. It is intentionally small and dependency-free.

Typical flow:
  1) Initialise context:
       python rlm_repl.py init path/to/context.txt
  2) Execute code repeatedly (state persists):
       python rlm_repl.py exec -c 'print(len(content))'
       python rlm_repl.py exec <<'PYCODE'
       # you can write multi-line code
       hits = grep('TODO')
       print(hits[:3])
       PYCODE

The script injects these variables into the exec environment:
  - context: dict with keys {path, loaded_at, content}
  - content: string alias for context['content']
  - buffers: list[str] for storing intermediate text results

It also injects helpers:
  - peek(start=0, end=1000) -> str
  - grep(pattern, max_matches=20, window=120, flags=0) -> list[dict]
  - chunk_indices(size=200000, overlap=0) -> list[(start,end)]
  - write_chunks(out_dir, size=200000, overlap=0, prefix='chunk') -> list[str]
  - add_buffer(text: str) -> None

Security note:
  This runs arbitrary Python via exec. Treat it like running code you wrote.
"""

from __future__ import annotations

import argparse
import io
import os
import pickle
import re
import sys
import textwrap
import time
import traceback
from contextlib import redirect_stderr, redirect_stdout
from pathlib import Path
from typing import Any

DEFAULT_STATE_PATH = Path(".claude/rlm_state/state.pkl")
DEFAULT_MAX_OUTPUT_CHARS = 8000


