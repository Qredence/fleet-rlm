<!-- Chunk 645: bytes 1014708-1018701, type=function -->
def validate_trainset(trainset: list[dict]) -> bool:
    """Validate trainset structure."""
    required_fields = ["task_description"]
    
    for i, example in enumerate(trainset):
        # Check required fields
        missing = [f for f in required_fields if f not in example]
        if missing:
            print(f"Example {i}: Missing fields {missing}")
            return False
        
        # Check field types
        if not isinstance(example["task_description"], str):
            print(f"Example {i}: task_description must be string")
            return False
    
    return True
```

**Why**: Invalid training data causes cryptic errors during optimization.

## Optimization Strategy

### Start with GEPA for Quick Iteration

```python
# Phase 1: Quick baseline with GEPA (2-5 minutes)
gepa = dspy.GEPA(metric=metric, num_candidates=5, num_iters=2)
baseline = gepa.compile(program, trainset=trainset)

# Evaluate
baseline_score = evaluate(baseline, testset)
print(f"GEPA baseline: {baseline_score:.3f}")
```

**Why**: GEPA is 5-10x faster than MIPROv2, good for initial experiments.

### Use MIPROv2 auto="medium" for Production

```python
# Phase 2: Production optimization with MIPROv2
mipro = dspy.MIPROv2(
    metric=metric,
    auto="medium",  # Balanced cost/quality
    num_threads=8,
)

optimized = mipro.compile(
    program,
    trainset=trainset,
    max_bootstrapped_demos=2,  # Conservative for speed
    max_labeled_demos=2,
    num_candidate_programs=8,  # Reduced from default 16
)
```

**Why**: `auto="medium"` provides 80% of the quality gains at 30% of the cost of `auto="heavy"`.

### Always Use Separate Test Set

```python
# ✅ Good: Separate train/test
trainset, testset = train_test_split(examples, test_size=0.2, random_state=42)

optimizer.compile(program, trainset=trainset)
test_score = evaluate(program, testset)  # Unbiased estimate
```

```python
# ❌ Bad: Evaluate on training set
optimizer.compile(program, trainset=examples)
score = evaluate(program, examples)  # Overly optimistic!
```

**Why**: Evaluating on training data gives inflated scores that don't reflect real performance.

## Monitoring & Observability

### Wrap Critical Modules with ModuleMonitor

```python
from skill_fleet.core.dspy.monitoring import ModuleMonitor, ExecutionTracer

tracer = ExecutionTracer(max_traces=1000)

# Wrap expensive modules
generator = GenerateSkillContentModule()
monitored_generator = ModuleMonitor(
    generator,
    name="skill_content_generator",
    tracer=tracer,
    quality_metric=lambda x: score_skill_quality(x.skill_content),
)

# Use normally
result = monitored_generator(task="Create async skill")

# Check metrics
metrics = monitored_generator.get_metrics()
print(f"Success rate: {metrics['success_rate']:.2%}")
print(f"Avg quality: {metrics['avg_quality_score']:.3f}")
```

**Why**: Production visibility helps identify bottlenecks and quality issues.

### Export Traces for Analysis

```python
# Collect traces during operation
for task in tasks:
    result = monitored_module(task=task)

# Periodically export
if execution_count % 100 == 0:
    tracer.export_traces(f"traces_{execution_count}.json")
    tracer.clear()  # Prevent memory growth
```

**Why**: Trace analysis helps identify patterns in failures and quality issues.

## Error Handling

### Use RobustModule for Critical Operations

```python
from skill_fleet.core.dspy.modules.error_handling import RobustModule

# Wrap unreliable module
unreliable = SomeExternalAPIModule()
robust = RobustModule(
    unreliable,
    name="external_api",
    max_retries=3,
    retry_delay=1.0,  # Exponential backoff
    fallback_fn=lambda **kwargs: safe_default_response(),
)

# Handles transient failures gracefully
result = robust(query="test")
```

**Why**: External APIs, LMs, and network calls can fail. Retries with backoff handle transient failures.

### Validate Outputs Before Returning

```python
from skill_fleet.core.dspy.modules.error_handling import ValidatedModule

