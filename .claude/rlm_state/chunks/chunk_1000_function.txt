<!-- Chunk 1000: bytes 3509608-3537229, type=function -->
def skill_quality_metric(example: dspy.Example, prediction: dspy.Prediction, trace=None) -> float:
    """Multi-dimensional skill quality assessment."""
    scores = {
        'pattern_count': min(prediction.pattern_count / 5, 1.0),  # Target: 5+ patterns
        'has_anti_patterns': 1.0 if prediction.has_anti_patterns else 0.0,
        'has_key_insights': 1.0 if prediction.has_key_insights else 0.0,
        'has_real_world_impact': 1.0 if prediction.has_real_world_impact else 0.0,
        'has_quick_reference': 1.0 if prediction.has_quick_reference else 0.0,
        'has_common_mistakes': 1.0 if prediction.has_common_mistakes else 0.0,
        'has_red_flags': 1.0 if prediction.has_red_flags else 0.0,
        'frontmatter_completeness': prediction.frontmatter_score,
        'code_examples_quality': prediction.code_quality_score,
    }
    return sum(scores.values()) / len(scores)
```

#### 2.2.2 Create Gold-Standard Training Set

Use excellent skills as training examples:
```python
gold_skills = [
    dspy.Example(
        task_description="Create a skill for FastAPI production patterns",
        expected_skill=load_skill("skills/technical_skills/.../fastapi/SKILL.md"),
        quality_score=0.95
    ),
    # Add more gold-standard examples from:
    # - Anthropics skills (skill-creator, mcp-builder, etc.)
    # - Obra/Superpowers skills (writing-skills, test-driven-development, etc.)
]
```

#### 2.2.3 Implement MIPROv2 Optimization

```python
from dspy.teleprompt import MIPROv2

# Configure optimizer
optimizer = MIPROv2(
    metric=skill_quality_metric,
    auto="medium",  # Balance between optimization depth and cost
    num_threads=8,
)

# Optimize the skill creation program
optimized_creator = optimizer.compile(
    SkillCreationProgram(),
    trainset=gold_skills,
    max_bootstrapped_demos=4,  # Auto-generated examples
    max_labeled_demos=4,       # Human-curated examples
)

# Save optimized program
optimized_creator.save("optimized_skill_creator")
```

#### 2.2.4 Implement Evaluation Pipeline

```python
from dspy import Evaluate

# Create evaluation function
evaluate_skills = Evaluate(
    devset=test_skills,
    metric=skill_quality_metric,
    num_threads=16,
    display_progress=True,
    display_table=True
)

# Evaluate before/after optimization
baseline_score = evaluate_skills(SkillCreationProgram())
optimized_score = evaluate_skills(optimized_creator)
```

---

## Part 3: Skill Structure Improvements

### 3.1 Adopt Anthropics/Obra Best Practices

#### 3.1.1 Simplified Directory Structure

**Current (Over-engineered):**
```
skill-name/
‚îú‚îÄ‚îÄ SKILL.md
‚îú‚îÄ‚îÄ assets/README.md          # Placeholder
‚îú‚îÄ‚îÄ capabilities/README.md    # Placeholder
‚îú‚îÄ‚îÄ examples/example_1.py     # Often low quality
‚îú‚îÄ‚îÄ references/README.md      # Placeholder
‚îú‚îÄ‚îÄ resources/README.md       # Placeholder
‚îú‚îÄ‚îÄ scripts/README.md         # Placeholder
‚îú‚îÄ‚îÄ tests/test_1.py           # Text, not executable
‚îú‚îÄ‚îÄ best_practices.md
‚îú‚îÄ‚îÄ metadata.json
‚îî‚îÄ‚îÄ taxonomy_meta.json
```

**Recommended (Lean):**
```
skill-name/
‚îú‚îÄ‚îÄ SKILL.md                  # Required - main content
‚îú‚îÄ‚îÄ scripts/                  # Only if executable tools needed
‚îÇ   ‚îî‚îÄ‚îÄ tool.py
‚îî‚îÄ‚îÄ references/               # Only if heavy reference (100+ lines)
    ‚îî‚îÄ‚îÄ api-reference.md
```

#### 3.1.2 SKILL.md Structure (Anthropics + Obra Hybrid)

```markdown
---
name: skill-name
description: Use when [specific triggering conditions and symptoms]
---
# Skill Name

## Overview
What is this? Core principle in 1-2 sentences.

## When to Use
**Use when:**
- [Symptom/situation 1]
- [Symptom/situation 2]

**When NOT to use:**
- [Anti-use case 1]

## Quick Reference
| Problem | Solution | Keywords |
|---------|----------|----------|
| ... | ... | ... |

## Core Patterns

### Pattern 1: [Name]
**The problem:** [Description]

**‚ùå Common mistake:**
```code
[Anti-pattern]
```

**‚úÖ Production pattern:**
```code
[Correct approach]
```

**Key insight:** [Critical learning in 1-2 sentences]

### Pattern 2: [Name]
[Repeat structure...]

## Common Mistakes
| Mistake | Why It's Wrong | Fix |
|---------|----------------|-----|
| ...     | ...            | ... |

## Red Flags
- [Warning sign 1]
- [Warning sign 2]

**All of these mean: [Action to take]**

## Real-World Impact
- [Quantified benefit 1]
- [Quantified benefit 2]
```

### 3.2 Key Principles to Enforce

From Anthropics:
1. **Concise is Key** - Only add what the model doesn't already know
2. **Progressive Disclosure** - Metadata (~100 words) ‚Üí SKILL.md (<5k words) ‚Üí References (as needed)
3. **No Placeholder Files** - Only create files with real content

From Obra/Superpowers:
1. **Description = Triggering Conditions ONLY** - Never summarize workflow
2. **TDD for Skills** - Test with pressure scenarios before deployment
3. **One Excellent Example** - Not multi-language dilution
4. **No Narrative Storytelling** - Reusable patterns, not session logs

---

## Part 4: Implementation Roadmap

### Phase 1: Foundation (Week 1-2)

1. **Define Quality Metrics Module**
   - Create `src/skill_fleet/core/dspy/metrics/skill_quality.py`
   - Implement multi-dimensional quality scoring
   - Add structural validation (pattern count, sections present, etc.)

2. **Create Gold-Standard Dataset**
   - Extract excellent skills (FastAPI, Anthropics examples, Obra examples)
   - Create `config/training/gold_skills.json`
   - Document quality criteria for each example

3. **Update SKILL.md Template**
   - Simplify `config/templates/SKILL_md_template.md`
   - Remove placeholder file generation
   - Add Key Insight prompts after each pattern

### Phase 2: DSPy Integration (Week 3-4)

4. **Implement Evaluation Pipeline**
   - Create `src/skill_fleet/core/dspy/evaluation.py`
   - Add `dspy.Evaluate` integration
   - Create baseline metrics for current generation

5. **Implement MIPROv2 Optimization**
   - Create `src/skill_fleet/core/dspy/optimization.py`
   - Configure optimizer with gold-standard examples
   - Run optimization and save optimized program

6. **A/B Testing Framework**
   - Compare baseline vs optimized generation
   - Track quality metrics over time
   - Create optimization feedback loop

### Phase 3: Validation & Refinement (Week 5-6)

7. **TDD-Based Skill Validation**
   - Create pressure scenario framework
   - Test generated skills with subagents
   - Document baseline failures and improvements

8. **Continuous Improvement Pipeline**
   - Auto-collect high-quality generated skills
   - Expand training set over time
   - Re-optimize periodically

---

## Part 5: Success Metrics

### 5.1 Quality Metrics to Track

| Metric | Current Baseline | Target |
|--------|------------------|--------|
| Average pattern count | 2 | 5+ |
| Skills with Key Insights | 0% | 100% |
| Skills with Real-World Impact | 0% | 80% |
| Skills with executable tests | 0% | 100% |
| Placeholder files generated | Many | 0 |
| SKILL.md under 500 lines | Variable | 100% |
| Description follows "Use when..." | Variable | 100% |

### 5.2 Evaluation Cadence

- **Per-generation:** Run quality metric on each generated skill
- **Weekly:** Aggregate quality scores, identify trends
- **Monthly:** Re-run MIPROv2 optimization with expanded training set

---

## Part 6: Technical Implementation Details

### 6.1 New Files to Create

```
src/skill_fleet/core/dspy/
‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ skill_quality.py      # Quality metric functions
‚îú‚îÄ‚îÄ evaluation.py              # dspy.Evaluate integration
‚îú‚îÄ‚îÄ optimization.py            # MIPROv2/BootstrapFewShot setup
‚îî‚îÄ‚îÄ training/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ gold_standards.py      # Gold-standard skill loader

config/
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ gold_skills.json       # Curated excellent skills
‚îÇ   ‚îî‚îÄ‚îÄ quality_criteria.yaml  # Quality scoring rubric
‚îî‚îÄ‚îÄ templates/
    ‚îî‚îÄ‚îÄ SKILL_md_template.md   # Updated lean template
```

### 6.2 Integration Points

1. **Phase2GenerationModule** - Add few-shot examples from gold standards
2. **Phase3ValidationModule** - Use quality metrics for validation
3. **SkillCreationProgram** - Load optimized program if available
4. **CLI** - Add `--optimize` flag to run optimization
5. **API** - Add `/optimize` endpoint for training

---

## Appendix A: External Resources

### Anthropics Skills
- Repository: https://github.com/anthropics/skills
- Key skill: `skill-creator` - Official guidance on skill creation
- Principles: Concise, Progressive Disclosure, Degrees of Freedom

### Obra/Superpowers
- Repository: https://github.com/obra/superpowers
- Key skill: `writing-skills` - TDD approach to skill creation
- Principles: RED-GREEN-REFACTOR, Pressure Testing, CSO (Claude Search Optimization)

### DSPy Documentation
- Evaluations: `dspy.Evaluate` with custom metrics
- Optimizers: `MIPROv2` (recommended), `BootstrapFewShot`
- Best practice: Use stronger model (GPT-4o) as teacher for optimization

---

## Appendix B: Quality Checklist for Generated Skills

### Structure Checklist
- [ ] YAML frontmatter with name and description
- [ ] Description starts with "Use when..."
- [ ] Description does NOT summarize workflow
- [ ] Overview with core principle (1-2 sentences)
- [ ] When to Use / When NOT to use sections
- [ ] Quick Reference table
- [ ] 5+ Core Patterns
- [ ] Each pattern has ‚ùå anti-pattern and ‚úÖ production pattern
- [ ] Each pattern has Key Insight
- [ ] Common Mistakes table
- [ ] Red Flags section
- [ ] Real-World Impact with quantified benefits

### Content Checklist
- [ ] No placeholder files
- [ ] No narrative storytelling
- [ ] One excellent example per pattern (not multi-language)
- [ ] Code examples are copy-paste ready
- [ ] Labels have semantic meaning (not helper1, step2)
- [ ] Total SKILL.md under 500 lines (or split to references)

### Validation Checklist
- [ ] Tested with pressure scenarios
- [ ] Baseline failures documented
- [ ] Skill addresses specific failures
- [ ] Re-tested after refinement


============================================================
END FILE: docs/internal/plans/archive/2026-01-15-skill-creation-improvement-plan.md
============================================================

============================================================
FILE: docs/internal/plans/archive/2026-01-16-taxonomy-simplification-plan.md
============================================================

# Taxonomy Simplification & Skill Learning Plan

**Date:** 2026-01-16  
**Status:** Draft  
**Purpose:** Rework and simplify the taxonomy hierarchy without breaking existing behavior, while adding a skill-learning strategy aligned with trajectory/feedback baselines.

---

## Executive Summary

The current taxonomy is defined implicitly by directory structure under `skills/`, with multiple inconsistent paths for similar domains and a deep hierarchy that increases routing complexity. This plan proposes a **backward-compatible redesign** that introduces a canonical taxonomy index (logical tree + facets + aliases), decouples storage paths from logical taxonomy, and integrates a skill-learning pipeline inspired by Letta‚Äôs Skill Learning and recent metacognitive reuse work. The intent is to **simplify hierarchy** without breaking any API, CLI, validation, training, or DSPy workflows.

---

## Goals

1. **Simplify taxonomy hierarchy** (reduce depth and redundancy).
2. **Preserve compatibility**: old paths remain valid via alias resolution.
3. **Improve skill selection**: enable agent skill picker to use a logical taxonomy + metadata facets.
4. **Add skill-learning framework**: trajectory-only vs trajectory+feedback baselines.
5. **Minimize empty directories** by making skill content materialization template-driven and lazy.
6. **Keep taxonomy definition even when empty** so skill creation can place new skills correctly without pre-creating directories or ‚Äúfilling‚Äù the taxonomy with placeholder skills.

## Non-Goals

- No immediate bulk movement of existing skill directories.
- No breaking changes to existing skill IDs, dependencies, or drafts.
- No removal of current validation logic until the alias/index layer is proven.
- No attempt to author ‚Äúas many skills as the taxonomy schema allows.‚Äù The taxonomy definition must be evolutive/dynamic and remain relevant over time; skill inventory grows only when skills are explicitly created, migrated, or learned.

---

## Current State (Findings)

- **Taxonomy is filesystem-defined**: directory path is treated as taxonomy path (`skills/‚Ä¶`).
- **Stats only**: `skills/taxonomy_meta.json` tracks counts/usage, not taxonomy structure.
- **Deep + redundant paths**: e.g. `technical_skills/programming/languages/python/*` vs `technical_skills/programming/python/*` vs `technical_skills/programming/web_frameworks/python/*`.
- **Inconsistent `skill_id` vs directory path** already exists (e.g., `docker` dir vs `docker-best-practices` skill_id).
- **Skill picker is path-centric**: DSPy modules, rewards, evaluation datasets, and API tree all assume path structure.

---

## Constraints & Safety Requirements

- **No breaking API/CLI/validation**: all existing paths must resolve.
- **Alias-first migration**: do not move directories until all call sites are alias-aware.
- **Deterministic validation**: validators must accept old + new paths for a deprecation window.
- **Traceable updates**: record which changes affect datasets, rewards, and prompts.

---

## Proposed Strategy (High-Level)

### A) Introduce a Canonical Taxonomy Index (Taxonomy = Logical Model)

Create a single source of truth (e.g., `skills/_taxonomy/index.json` or `skills/taxonomy.yaml`) that defines the taxonomy structure.

- **Primary Key**: `skill_id` (derived from `SKILL.md` frontmatter name). This is the stable identifier and must be unique.
- **Fields**: `skill_id`, `canonical_path` (new simplified path), `aliases` (old paths), `facets` (e.g., `language:python`), `tags`.
- **Hybrid Strategy (Index Authority)**: The Index is the authority for tree structure and routing. The filesystem is valid storage but secondary.
  - **Reads**: `TaxonomyManager` loads the tree from `index.json` (O(1)).
  - **Validation**: Lazy check against filesystem only when content is requested.
  - **Dev Safety**: Startup warning if filesystem directories exist that are not in the index (to catch un-indexed manual creations).

### A.1) Separate Taxonomy Definition from Skill Inventory

Maintain a taxonomy definition that can include ‚Äúempty categories‚Äù (nodes with no skills yet) without requiring directories or placeholder skills.

Key rule: **taxonomy nodes exist in the index even if no `skills/...` directories exist for them.**

This supports:

- consistent placement for skill creation (the planner can target a canonical path that does not yet exist),
- long-lived taxonomy stability, and
- evolutive updates to the taxonomy definition without forcing content churn.

### B) Decouple Identity from Location

- **Stable IDs**: Stop using file path as identity. `skill_id` is the key.
- **Resolving Paths**: To find `skills/deep/nested/docker`, the system looks up the alias in the Index to find `skill_id=docker-guidelines`, then retrieves content from the `canonical_path`.
- **API Tree**: Generated from the index, not filesystem traversal.

### C) Simplify Hierarchy + Use Facets

- Target depth: **2‚Äì3 segments** max.
- Replace deep nesting with facets + tags for precision.

### D) Lazy Materialization of Skill Content

Only create the `skills/<path>/...` directories and their internal structure when explicitly:

- created (skill creation workflow),
- migrated, or
- learned/promoted.

The index remains the canonical taxonomy definition and routing layer; the filesystem is a cache/materialized view for skills that exist.

### E) Skill Learning Pipeline

- Baselines:
  - **Baseline**: no skills
  - **Skills (Trajectory)**
  - **Skills (Trajectory + Feedback)**
- Use trajectory logs + verifier feedback to synthesize skill cards and update taxonomy index.

---

## Phased Execution Plan

### Phase 1 ‚Äî Taxonomy Inventory & Canonical Map (No Code Changes)

- Create a **full inventory** of existing skills, paths, and metadata mismatches.
- Draft a **canonical taxonomy map** that proposes simplified paths.
- Produce an **alias mapping** for every existing skill.
- Output: `taxonomy-map.md` (proposed canonical structure + alias table).

**Acceptance Criteria**

- 100% of existing skills have a canonical path + alias.
- No path collisions in canonical set.
- Taxonomy definition can include categories with zero skills without requiring filesystem directories.

### Phase 2 ‚Äî Taxonomy Index Spec & Manager Update

- Define the index schema (JSON/YAML).
- Update `TaxonomyManager` to load from Index first.
- Implement lazy validation (warn on startup for unindexed directories).
- **Decision**: Enforce `skill_id` uniqueness immediately.

**Acceptance Criteria**

- Schema reviewed + approved.
- `TaxonomyManager` prefers Index but falls back gracefully (or warns) for unindexed files during transition.

### Phase 3 ‚Äî Backward-Compatible Resolution Plan

- Specify alias resolution logic for:
  - `TaxonomyManager` lookups
  - Dependency resolution
  - Validator expectations
  - API tree generation
- Define **deprecation policy** (how long old paths remain valid).

**Acceptance Criteria**

- Documented resolution rules and edge cases.
- No breaking change risks unaddressed.

### Phase 4 ‚Äî Skill Learning Integration Plan

- Define pipeline stages:
  1. Baseline trajectory capture
  2. Reflection/extraction (skill candidate)
  3. Synthesis into skill card
  4. Validation + scoring
  5. Promotion into taxonomy index
- Map to storage: where trajectory + feedback live.

**Acceptance Criteria**

- Clear data flow for trajectory ‚Üí skill.
- Defined evaluation metrics for ‚Äúlearned skill quality.‚Äù

### Phase 5 ‚Äî Migration & Update Checklist

- Identify all call sites to update:
  - `TaxonomyManager`
  - Validators
  - DSPy rewards + eval data
  - API taxonomy tree
  - Training datasets
- Define test matrix.

**Acceptance Criteria**

- Complete list of files and tests to update.
- Explicit rollback strategy.

---

## Impact Analysis (Known Couplings)

1. **Validators**: currently enforce underscore-only segments; will need to support hyphens if canonical paths include them.
2. **DSPy Rewards**: `taxonomy_path_reward` assumes depth 2‚Äì6 and specific root buckets.
3. **Datasets**: training + optimized configs encode `expected_taxonomy_path`.
4. **API Tree**: built from filesystem; must move to index.
5. **Skill Dependencies**: `dependencies`, `see_also`, `category` fields include old paths.

---

## Risks & Mitigations

- **Risk: breaking legacy paths** ‚Üí Mitigation: alias resolution layer before any moves.
- **Risk: dataset drift** ‚Üí Mitigation: keep dual path acceptance + update datasets gradually.
- **Risk: skill picker confusion** ‚Üí Mitigation: add facets + ranking with explicit triggers.
- **Risk: validator rejects new structure** ‚Üí Mitigation: update validator to accept index-defined paths.

---

## Deliverables

1. `plans/2026-01-16-taxonomy-simplification-plan.md` (this doc)
2. `taxonomy-map.md` (proposed canonical paths + aliases)
3. `taxonomy-index-schema.md` (index schema spec)
4. Skill Learning pipeline spec + evaluation rubric

---

## References

- Letta Skill Learning: https://www.letta.com/blog/skill-learning
- Metacognitive Reuse: https://arxiv.org/pdf/2509.13237
- Dynamic Cheatsheet / Test-Time Memory: https://arxiv.org/pdf/2504.07952


============================================================
END FILE: docs/internal/plans/archive/2026-01-16-taxonomy-simplification-plan.md
============================================================

============================================================
FILE: docs/internal/plans/archive/2026-01-20-dspy-optimization-comprehensive.md
============================================================

# DSPy Platform Optimization Plan (Comprehensive)

**Date**: January 20, 2026  
**Status**: Ready for Implementation  
**Scope**: skill-fleet FastAPI + CLI architecture with DSPy workflow optimization  
**Target Impact**: Quality 0.70-0.75 ‚Üí 0.85-0.90 | Performance +30-50% | Reliability: Production-ready

---

## üéØ Local Development Focus

**Platform**: Local development only (no distributed infrastructure)

**Key Simplifications**:
- ‚úÖ **Job Storage**: SQLite only (no Redis option)
- ‚úÖ **Caching**: Filesystem only (no distributed cache)
- ‚úÖ **Similarity Index**: On-demand rebuild (no scheduled nightly job)
- ‚úÖ **Streaming**: Local single-process simplification (no multi-server backpressure)
- ‚úÖ **Infrastructure**: Localhost focus (no cloud deployment concerns)

**Timeline Impact**:
- Original: 12 weeks
- **Local-only**: 8-10 weeks (-20%)
- Quick wins: 4-7 days ‚Üí **2-4 days** (-50%)

---

## Executive Summary

This plan optimizes the skill-fleet platform's **core value proposition**: creating agent skills via intelligent DSPy workflows (signatures ‚Üí modules ‚Üí programs ‚Üí evaluation ‚Üí optimization).

**Key insight**: The platform is meta-learning system‚Äîit learns to create better skills through DSPy. Current gaps prevent unlocking full potential:

- **Optimizer Selection**: Manual (user guesses MIPROv2 vs GEPA vs Bootstrap)
- **Metric Feedback**: Evaluated separately from optimization; doesn't improve signatures
- **Training Data**: Fixed 50 examples; no feedback on which help optimization
- **Job Persistence**: In-memory only; lost on API restart
- **Module Composition**: Ad-hoc instantiation; hard to version/test/swap

**This plan addresses all three layers**:
1. **Tier 1** (DSPy Core): Feedback loops, auto-selection, adaptive weighting
2. **Tier 2** (API/CLI): Persistence, streaming, caching
3. **Tier 3-5** (Quality, DX): Drift detection, ensemble, dashboards

**Estimated effort**: 12 weeks (Phases 0-4)  
**Quick wins**: 4-7 days for Tier 1A + 2A + 1C = 20% quality gain

---

## Part 1: Current State Analysis

### Architecture Overview

**150 Python files organized into 6 layers**:

```
Entry Points (CLI + API)
    ‚Üì
Core DSPy Engine (3-phase workflow)
    ‚Üì
Advanced Patterns (Ensemble, Versioning, Caching, Error Handling)
    ‚Üì
Quality System (Metrics, Monitoring, Evaluation, Training Data)
    ‚Üì
Configuration & Infrastructure
```

### What's Strong ‚úÖ

| Component | Status | Evidence |
|-----------|--------|----------|
| API-First Architecture | ‚úÖ | CLI correctly delegates to FastAPI |
| 3-Phase Workflow | ‚úÖ | Understanding ‚Üí Generation ‚Üí Validation (modular) |
| Signature Design | ‚úÖ | Using Literal types, quality constraints |
| Training Data | ‚úÖ | 50 examples across 19 categories (threshold met) |
| Advanced Patterns | ‚úÖ | Ensemble, versioning, caching, error handling implemented |
| Dual Interfaces | ‚úÖ | REST API functional, CLI modular (15 commands) |
| Type Safety | ‚úÖ | Pydantic v2, unified models (900+ lines) |

### Critical Gaps ‚ö†Ô∏è

| Gap | Impact | Addressed By |
|-----|--------|--------------|
| Job Persistence: In-memory only | Lost on API restart | Tier 2A |
| Optimizer Selection: Manual | Wrong choice = wasted budget | Tier 1A |
| Metric Feedback: Disconnected | Doesn't improve signatures | Tier 1D |
| Training Data: Static | No feedback on optimizer effectiveness | Tier 1C |
| Module Registry: Ad-hoc | Hard to version/swap | Tier 3A |
| Signature Versioning: None | No schema evolution path | Tier 3B |
| Phase Branching: All-or-nothing | Simple skills waste time | Tier 3C |
| Streaming Buffering: Unclear | Real-time visibility might lag | Tier 2B |
| Golden Standard Drift: Not monitored | Optimizer may degrade over time | Tier 4A |
| Schema Guarantees: Inference-based | No structured output guarantee | Tier 2A (stretch) |

---

## Part 2: Opportunity Details (Tiered)

### TIER 1: Core DSPy Value Prop (Highest ROI)

#### **1A: Optimizer Auto-Selection Engine** üöÄ

**Problem**: User manually chooses optimizer (MIPROv2 vs GEPA vs BootstrapFewShot). Wrong choice = wasted budget/time.

**Solution**: Intelligent selector based on task characteristics.

**Implementation**:
- New module: `src/skill_fleet/core/dspy/optimization/selector.py`
  - Class: `OptimizerSelector`
  - Input: `OptimizerContext` (trainset_size, budget, quality_target, complexity, domain)
  - Logic: Decision tree + rule-based selection
  - Output: `OptimizerRecommendation` (optimizer, config, cost_estimate, confidence)

**Decision Rules**:
```
IF trainset_size < 100 AND budget < $5:
    RECOMMEND GEPA (fast, reflection-based)
ELSE IF trainset_size < 500 AND budget < $20:
    RECOMMEND MIPROv2 auto="light"
ELSE IF trainset_size >= 500 AND budget >= $20:
    RECOMMEND MIPROv2 auto="medium"
ELSE IF budget > $100:
    RECOMMEND MIPROv2 auto="heavy" or BootstrapFinetune
ELSE:
    RECOMMEND BootstrapFewShot (fallback, always works)
```

**New API Endpoint**:
```
POST /api/v1/optimization/recommend
Request: {
  "trainset_size": 50,
  "quality_target": 0.85,
  "budget": "$10",
  "domain": "testing",
  "complexity_score": 0.7
}
Response: {
  "recommended": "MIPROv2",
  "config": {"auto": "light", "num_threads": 8},
  "estimated_cost": "$2.50",
  "estimated_time": "15 min",
  "confidence": 0.92,
  "alternatives": [
    {"optimizer": "GEPA", "cost": "$0.50", "time": "5 min", "quality_risk": 0.15}
  ]
}
```

**CLI Integration**:
```bash
uv run skill-fleet optimize --auto-select
# Uses OptimizerSelector to pick best optimizer automatically
```

**Metrics to Track**:
- Selection accuracy: Did recommendation match actual best choice?
- Cost accuracy: Was estimate within 10%?
- Quality improvement: Compare auto-selected vs manual selection
- Coverage: What % of runs use auto-select vs manual

**Files to Create**:
1. `src/skill_fleet/core/dspy/optimization/selector.py` (~250 lines)
2. `src/skill_fleet/api/routes/optimization.py` new endpoint
3. `src/skill_fleet/cli/commands/optimize.py` --auto-select flag

**Testing**:
- Unit: Test decision tree with synthetic contexts
- Integration: Run actual optimization with recommended config
- A/B: Compare auto vs manual selection results

**Timeline**: 1-2 days | **Impact**: 20-30% faster optimization (fewer wrong choices)

---

#### **1B: Adaptive Metric Weighting** üéØ

**Problem**: 6 metrics exist (skill_quality, semantic_f1, entity_f1, readability, coverage, composite) but weights are static (all equal). Different skill types need different emphasis.

**Example**: 
- Navigation hub: Prioritize clarity + coverage, de-emphasize entity extraction
- Comprehensive: Balanced approach
- Minimal: Prioritize semantic correctness over readability

**Solution**: Detect skill style, adjust metric weights accordingly.

**Implementation**:
- New module: `src/skill_fleet/core/dspy/metrics/adaptive_weighting.py`
  - Class: `AdaptiveMetricWeighting`
  - Input: Skill draft, style classification
  - Output: Adjusted metric weights dict

**Style-to-Weight Mapping**:
```python
METRIC_WEIGHTS = {
    "navigation_hub": {
        "skill_quality": 0.30,       # ‚Üë prioritize structure
        "semantic_f1": 0.15,
        "entity_f1": 0.05,          # ‚Üì de-emphasize
        "readability": 0.35,        # ‚Üë must be clear
        "coverage": 0.15            # ‚Üë must have examples
    },
    "comprehensive": {
        "skill_quality": 0.25,
        "semantic_f1": 0.25,        # balanced
        "entity_f1": 0.20,
        "readability": 0.20,
        "coverage": 0.10
    },
    "minimal": {
        "skill_quality": 0.20,
        "semantic_f1": 0.50,        # ‚Üë‚Üë pure correctness
        "entity_f1": 0.15,
        "readability": 0.10,
        "coverage": 0.05            # ‚Üì few examples ok
    }
}
```

**New Signature in phase3_validation.py**:
```python
