<!-- Chunk 692: bytes 1329600-1365972, type=function -->
def main(argv: List[str]) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    try:
        return int(args.func(args))
    except RlmReplError as e:
        sys.stderr.write(f"ERROR: {e}\n")
        return 2


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))


============================================================
END FILE: .github/skills/rlm/scripts/rlm_repl.py
============================================================

============================================================
FILE: .github/workflows/ci.yml
============================================================

name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read
  pull-requests: read

jobs:
  lint:
    name: Lint with Ruff
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync --group dev
        # For production deployments, use: uv sync --frozen --no-dev

      - name: Run ruff check
        run: uv run ruff check src/ tests/

      - name: Run ruff format check
        run: uv run ruff format --check src/ tests/

  type-check:
    name: Type Check with ty
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync --group dev
        # For production deployments, use: uv sync --frozen --no-dev

      - name: Run ty type check
        run: uv run ty check src/ tests/

  test:
    name: Run Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12", "3.13"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --group dev
        # For production deployments, use: uv sync --frozen --no-dev

      - name: Run unit tests
        run: uv run pytest tests/unit/ -v --tb=short

      - name: Run integration tests (if API key available)
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          if [ -n "$GOOGLE_API_KEY" ]; then
            uv run pytest tests/integration/ -v --tb=short || true
          else
            echo "Skipping integration tests - GOOGLE_API_KEY not set"
          fi

  build:
    name: Build Verification
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync --group dev
        # For production deployments, use: uv sync --frozen --no-dev

      - name: Verify CLI entrypoint
        run: uv run skill-fleet --help

      - name: Validate skill structure
        run: |
          if [ -d "skills" ]; then
            uv run skill-fleet validate skills/general/testing || true
            echo "Skill validation completed"
          fi

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync --group dev
        # For production deployments, use: uv sync --frozen --no-dev

      - name: Install security scanning tools
        run: uv pip install pip-audit safety bandit[toml]

      - name: Run Bandit security linter
        run: uv run bandit -r src/skill_fleet -f json -o bandit-report.json || true
        continue-on-error: true

      - name: Display Bandit results
        run: |
          if [ -f bandit-report.json ]; then
            echo "### Bandit Security Scan Results ###"
            uv run bandit -r src/skill_fleet || true
          fi

      - name: Run pip-audit for dependency vulnerabilities
        run: uv run pip-audit --desc || true

      - name: Run safety check
        run: |
          if [ -f "requirements.txt" ]; then
            uv run safety check --file requirements.txt || true
          else
            echo "No requirements.txt found, checking uv lock file"
            uv pip list > /tmp/requirements.txt && uv run safety check --file /tmp/requirements.txt || true
          fi

      - name: Check for secrets in code
        run: |
          echo "Checking for potential secrets..."
          if grep -r -i -E "(api[_-]?key|secret|password|token)\s*=\s*['\"][^'\"]{16,}" src/ --exclude-dir=__pycache__; then
            echo "⚠️  Warning: Potential secrets found in source code"
            exit 1
          else
            echo "✅ No obvious secrets detected"
          fi

      - name: Summary
        run: echo "✅ Security scanning completed"

  all-checks:
    name: All Checks Passed
    runs-on: ubuntu-latest
    needs: [lint, type-check, test, build, security]
    steps:
      - name: Confirm all checks passed
        run: echo "All CI checks passed successfully!"


============================================================
END FILE: .github/workflows/ci.yml
============================================================

============================================================
FILE: .github/workflows/copilot-setup-steps.yml
============================================================

name: "Copilot Setup Steps"

on:
  workflow_dispatch:
  push:
    paths:
      - .github/workflows/copilot-setup-steps.yml
  pull_request:
    paths:
      - .github/workflows/copilot-setup-steps.yml

jobs:
  copilot-setup-steps:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync --group dev

      - name: Verify installation
        run: uv run skill-fleet --help


============================================================
END FILE: .github/workflows/copilot-setup-steps.yml
============================================================

============================================================
FILE: .github/workflows/junie.yml
============================================================

name: Junie
run-name: Junie run ${{ inputs.run_id }}

permissions:
  contents: write

on:
  workflow_dispatch:
    inputs:
      run_id:
        description: "id of workflow process"
        required: true
      workflow_params:
        description: "stringified params"
        required: true

jobs:
  call-workflow-passing-data:
    uses: jetbrains-junie/junie-workflows/.github/workflows/ej-issue.yml@main
    with:
      workflow_params: ${{ inputs.workflow_params }}


============================================================
END FILE: .github/workflows/junie.yml
============================================================

============================================================
FILE: .github/workflows/release.yaml
============================================================

name: Release

on:
  push:
    tags:
      - "v*"
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

jobs:
  build:
    name: Build Package
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install uv and Python
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
        with:
          enable-cache: true
          python-version: "3.12"

      - name: Build package
        run: uv build

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/

  release:
    name: Create GitHub Release
    needs: build
    runs-on: ubuntu-latest
    environment: pypi
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - name: Download build artifact
        uses: actions/download-artifact@v4
        with:
          name: dist
          path: dist/

      - name: Request OIDC token for PyPI
        id: request-oidc
        uses: actions/github-script@v6
        with:
          script: |
            const core = require('@actions/core');
            try {
              const token = await core.getIDToken('api://pypi');
              core.setOutput('token', token);
            } catch (err) {
              core.warning('Failed to obtain OIDC token: ' + err.message);
              core.setOutput('token', '');
            }
      - name: Publish distributions to PyPI
        uses: pypa/gh-action-pypi-publish@v1.13.0
        with:
          user: __token__
          # The action accepts either an API token or an OIDC-backed ephemeral token. Use OIDC token if present, otherwise fall back to stored secret.
          password: ${{ steps.request-oidc.outputs.token || secrets.PYPI_API_TOKEN }}
          packages-dir: dist/

      - name: Create Release
        uses: softprops/action-gh-release@a06a81a03ee405af7f2048a818ed3f03bbf83c7b # v2
        with:
          files: dist/*
          generate_release_notes: true

      - name: Publish distributions to PyPI
        uses: pypa/gh-action-pypi-publish@v1.13.0
        with:
          user: __token__
          password: ${{ secrets.PYPI_API_TOKEN }}
          packages-dir: dist/


============================================================
END FILE: .github/workflows/release.yaml
============================================================

============================================================
FILE: .letta/rlm/SKILL.md
============================================================

---
name: rlm
description: Process very large context files (logs, docs, transcripts, scraped webpages) that exceed context limits by chunking content, delegating analysis to subagents, and synthesizing results. Use when files are >100k characters and require iterative inspection and extraction across the entire document.
allowed-tools:
  - Read
  - Write
  - Edit
  - Grep
  - Glob
  - Bash
  - Task
---

# RLM (Recursive Language Model Workflow)

A workflow for processing large context files that won't fit in a single conversation by chunking content, delegating chunk analysis to subagents, and synthesizing results.

## When to Use This Skill

Use RLM when:
- The user provides or references a **very large context file** (>100k chars) - logs, documentation, transcripts, scraped webpages
- The file is too large to load entirely into the conversation context
- You need to **iteratively inspect, search, chunk, and extract** information from the entire file
- The task requires **analyzing multiple sections** independently before synthesis
- You need to maintain state across multiple operations on the same file

**Trigger phrases:**
- "Analyze this large log file..."
- "Extract all mentions of X from this transcript..."
- "Summarize this 500-page documentation..."
- "Find patterns across this entire scraped website dump..."

## Mental Model

The RLM workflow has three key components:

1. **Root LM** (you) - Orchestrates the workflow, manages chunking strategy, synthesizes final results
2. **Persistent REPL** (`rlm_repl.py`) - Maintains state, provides helpers for chunking/searching/extraction
3. **Sub-LM** (subagent) - Analyzes individual chunks and returns structured results

## Quick Start

### Minimal Example

```bash
# 1. Initialize with your large file
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init /path/to/large_file.txt

# 2. Scout the content
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(f'Total chars: {len(content):,}')"
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 1000))"

# 3. Create chunks
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=150000, overlap=5000)
print(f"Created {len(paths)} chunks")
for i, p in enumerate(paths[:3]):
    print(f"  {i}: {p}")
PY

# 4. Process chunks with subagent (see Subagent Approaches section)

# 5. Synthesize results in main conversation
```

## Complete Workflow

### 1. Parse Arguments

This skill expects `$ARGUMENTS` in one of these formats:
- `context=/path/to/file.txt query="What are the main errors?"`
- `context=/path/to/file.txt query="Extract all usernames" chunk_chars=200000`

Optional parameters:
- `chunk_chars=<int>` - Chunk size in characters (default: ~200000)
- `overlap_chars=<int>` - Overlap between chunks (default: 0)

If the user didn't provide arguments, ask for:
1. The context file path
2. The query/task

### 2. Initialize REPL State

```bash
# Initialize with the context file
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init /path/to/context.txt

# Check status
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py status
```

The REPL creates a persistent state file at `.letta/rlm_state/state.pkl` containing:
- The full context content
- Buffers for accumulating results
- Any persisted variables from exec commands

### 3. Scout the Context

Get a quick sense of the content structure:

```bash
# Peek at start
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 3000))"

# Peek at end
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(len(content)-3000, len(content)))"

# Quick search for structure
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "hits = grep(r'^#+ ', max_matches=10); print([h['match'] for h in hits])"
```

### 4. Choose Chunking Strategy

**Semantic chunking** (preferred when structure is clear):
- Markdown files: Split on heading boundaries (`^#+ `)
- JSON: Split on top-level objects/arrays
- Logs: Split on timestamp boundaries
- Code: Split on function/class boundaries

**Character-based chunking** (fallback):
- Use when no clear semantic boundaries exist
- Recommended size: 150,000-250,000 chars
- Add overlap (5,000-10,000 chars) to preserve context at boundaries

### 5. Materialize Chunks

Write chunks to disk so subagents can read them:

```bash
# Character-based chunking
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=200000, overlap=10000)
print(f"Created {len(paths)} chunks:")
for i, p in enumerate(paths):
    print(f"  chunk_{i}: {p}")
PY

# Semantic chunking example (markdown headings)
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
import re
# Find all heading positions
headings = list(re.finditer(r'^#+ .+$', content, re.MULTILINE))
# Create chunks between headings
# ... (custom logic based on content structure)
PY
```

### 6. Process Chunks with Subagent

You have two approaches for delegating chunk analysis:

#### Approach A: Letta Task Tool (Recommended)

```python
# For each chunk, spawn a subagent task
Task({
    "subagent_type": "general-purpose",
    "description": f"Analyze chunk {i}",
    "prompt": f"""Analyze this chunk and extract all error messages.

Read the chunk: {chunk_path}

Return a JSON object with:
- errors: list of error messages found
- severity: list of severity levels
- context: brief context for each error

Be concise and structured."""
})
```

#### Approach B: Custom rlm-subcall Subagent

If you've set up a dedicated `rlm-subcall` agent (see `references/subagent-setup.md`):

```bash
# Use your preferred method to invoke rlm-subcall with:
# - The user's query
# - The chunk file path
# - Specific extraction instructions
```

**Key principles for subagent calls:**
- Keep instructions specific and task-focused
- Request structured output (JSON preferred)
- Limit subagent output size (they should extract/summarize, not dump)
- Pass chunk file path, don't paste content into prompt

### 7. Accumulate Results

Collect subagent outputs in REPL buffers or in the main conversation:

```bash
# Add subagent result to buffers
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
result = """
{chunk analysis from subagent}
"""
add_buffer(result)
print(f"Buffer count: {len(buffers)}")
PY
```

Or manually track results in the conversation as you iterate through chunks.

### 8. Synthesize Final Answer

Once all chunks are processed:

1. Review accumulated results from buffers/conversation
2. Identify patterns, trends, or comprehensive answers
3. Synthesize a coherent final response
4. Optionally: Use a subagent one final time to merge/format the collected evidence

```bash
# Export buffers for final synthesis
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py export-buffers /tmp/rlm_results.txt
```

## REPL Helper Functions

The `rlm_repl.py` script provides these helpers in exec mode:

### Variables
- `context` - Dict with keys: `path`, `loaded_at`, `content`
- `content` - String alias for `context['content']`
- `buffers` - List[str] for storing intermediate results

### Functions
- `peek(start=0, end=1000)` - Extract a substring from content
- `grep(pattern, max_matches=20, window=120, flags=0)` - Search with regex, returns matches with context
- `chunk_indices(size=200000, overlap=0)` - Calculate chunk boundaries (returns list of (start, end) tuples)
- `write_chunks(out_dir, size=200000, overlap=0, prefix='chunk')` - Write chunks to files, returns list of paths
- `add_buffer(text)` - Append text to buffers list

### Commands
- `init <context_path>` - Load a context file and create state
- `status` - Show current state (file size, buffers, vars)
- `exec -c "<code>"` - Execute Python code (state persists)
- `exec` - Execute Python code from stdin (use with heredoc)
- `export-buffers <out_path>` - Write all buffers to a file
- `reset` - Delete state file

## Guardrails

**DO:**
- Use REPL to locate and extract specific excerpts
- Keep subagent prompts focused and structured
- Request JSON or other structured formats from subagents
- Quote only necessary snippets in the main conversation

**DON'T:**
- Paste large raw chunks into the main chat context
- Spawn subagents from within subagents (orchestration stays in root)
- Store sensitive data in REPL state files
- Forget to clean up `.letta/rlm_state/` and `.letta/rlm_chunks/` when done

## File Locations

By default, RLM uses these paths:
- State: `.letta/rlm_state/state.pkl`
- Chunks: `.letta/rlm_chunks/chunk_*.txt`
- Buffers export: User-specified path

You can override the state path with `--state /custom/path.pkl` on any command.

## Advanced Usage

See `references/examples.md` for complete workflow examples including:
- Analyzing large log files
- Extracting structured data from transcripts
- Summarizing documentation
- Finding patterns in scraped web content

For subagent setup details, see `references/subagent-setup.md`.

## Troubleshooting

**"No state found" error:**
- Run `init` first with your context file path

**REPL state getting too large:**
- Use `reset` to clear state
- Initialize with a fresh context
- Consider filtering content before init if possible

**Subagent outputs too verbose:**
- Be more specific in subagent prompts
- Request structured/summarized output
- Limit output with explicit constraints

**Memory issues with very large files:**
- Use `--max-bytes` with init to cap file size
- Pre-filter or split files before processing
- Process in multiple RLM sessions if needed


============================================================
END FILE: .letta/rlm/SKILL.md
============================================================

============================================================
FILE: .letta/rlm/references/examples.md
============================================================

# RLM Workflow Examples

Complete examples of using the RLM workflow for different types of large-context tasks.

## Example 1: Analyzing Large Log Files

**Scenario:** User provides a 5MB application log file and wants all critical errors with context.

**Query:** "Extract all CRITICAL errors from app.log with timestamps and surrounding context"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init /path/to/app.log

# 2. Scout the structure
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 1000))"
# Output shows: [2024-01-23 10:15:32] INFO ...
# Logs are timestamp-prefixed, line-based

# 3. Search for critical errors
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
critical = grep(r'CRITICAL', max_matches=100, window=200)
print(f"Found {len(critical)} CRITICAL entries")
print("First 3:")
for i, hit in enumerate(critical[:3]):
    print(f"\n{i}. {hit['snippet'][:200]}...")
PY

# 4. If too many hits, chunk the file
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=150000, overlap=5000)
print(f"Created {len(paths)} chunks")
PY

# 5. Process each chunk with subagent
```

**Subagent prompt template:**
```
Read: {chunk_path}

Task: Extract all CRITICAL errors.

For each error, return JSON:
{
  "timestamp": "...",
  "message": "...",
  "surrounding_lines": "..." (3 lines before and after)
}

Return: {"errors": [...]}
```

**Synthesis:**
- Collect all error objects from subagent responses
- Sort by timestamp
- Group by error type/pattern if requested
- Present summary + full list

---

## Example 2: Extracting Structured Data from Transcripts

**Scenario:** 200-page meeting transcript, user wants all action items and decisions.

**Query:** "Extract all action items and decisions from transcript.txt"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init transcript.txt

# 2. Check format
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec -c "print(peek(0, 2000))"
# Transcript has speaker labels: "Alice: ...", "Bob: ..."

# 3. Search for keywords
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
action_words = grep(r'\b(action item|TODO|will do|responsible for)\b', max_matches=50, flags=2)
decision_words = grep(r'\b(decided|decision|we\'ll go with|agreed)\b', max_matches=50, flags=2)
print(f"Action keywords: {len(action_words)}")
print(f"Decision keywords: {len(decision_words)}")
PY

# 4. Semantic chunking (by speaker turns or time markers)
# Or character-based if no clear structure
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=180000, overlap=8000)
print(f"{len(paths)} chunks created")
PY

# 5. Process chunks
```

**Subagent prompt:**
```
Read: {chunk_path}

Extract:
1. Action items (who, what, when)
2. Decisions made (what was decided, context)

Return JSON:
{
  "action_items": [
    {"person": "...", "task": "...", "deadline": "..."}
  ],
  "decisions": [
    {"decision": "...", "context": "...", "speaker": "..."}
  ]
}

Only include clear, explicit items. Skip vague mentions.
```

**Synthesis:**
- Deduplicate similar action items across chunks
- Organize by person/team
- Present decisions chronologically
- Flag any incomplete information

---

## Example 3: Summarizing Documentation

**Scenario:** 1000-page technical specification, user wants high-level summary and specific section.

**Query:** "Summarize the authentication section from spec.md"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init spec.md

# 2. Find authentication section
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
auth_hits = grep(r'^## Authentication', max_matches=5, window=500)
if auth_hits:
    match = auth_hits[0]
    print(f"Found at position: {match['span']}")
    print(match['snippet'])
PY

# 3. Extract just authentication section
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
# Find section boundaries (between ## headers)
import re
headers = [(m.start(), m.group(0)) for m in re.finditer(r'^## .+$', content, re.MULTILINE)]

# Find authentication header
auth_idx = None
for i, (pos, title) in enumerate(headers):
    if 'Authentication' in title:
        auth_idx = i
        break

if auth_idx is not None:
    start = headers[auth_idx][0]
    end = headers[auth_idx + 1][0] if auth_idx + 1 < len(headers) else len(content)
    auth_section = content[start:end]
    
    # Write to separate file
    from pathlib import Path
    out = Path('.letta/rlm_chunks/auth_section.txt')
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(auth_section)
    print(f"Extracted {len(auth_section):,} chars to {out}")
PY

# 4. If section is still large, chunk it
# Otherwise, process directly with subagent
```

**Subagent prompt:**
```
Read: .letta/rlm_chunks/auth_section.txt

Create a comprehensive summary of the authentication approach:

1. Overview (2-3 sentences)
2. Key components (bullet list)
3. Authentication flow (step-by-step)
4. Important notes/warnings

Format as markdown. Be thorough but concise.
```

---

## Example 4: Finding Patterns in Scraped Web Content

**Scenario:** Scraped content from 500 web pages (single concatenated file), find all mentions of pricing.

**Query:** "Find all pricing information from scraped_pages.txt"

### Workflow

```bash
# 1. Initialize
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py init scraped_pages.txt

# 2. Quick search for price patterns
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
import re
price_patterns = [
    r'\$\d+(?:,\d{3})*(?:\.\d{2})?',  # $123.45
    r'\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:USD|dollars)',  # 123 dollars
    r'(?:price|cost|pricing):\s*\$?\d+',  # price: $50
]

all_prices = []
for pattern in price_patterns:
    hits = grep(pattern, max_matches=100, window=150, flags=2)
    all_prices.extend(hits)

print(f"Found {len(all_prices)} price mentions")
print("\nSample:")
for hit in all_prices[:5]:
    print(f"  {hit['match']} -> ...{hit['snippet'][:100]}...")
PY

# 3. Chunk for detailed extraction
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
paths = write_chunks('.letta/rlm_chunks', size=200000, overlap=5000)
print(f"Created {len(paths)} chunks")
PY

# 4. Process with subagent
```

**Subagent prompt:**
```
Read: {chunk_path}

Extract all pricing information.

For each price found, return:
{
  "price": "...",
  "currency": "...",
  "context": "..." (what is being priced),
  "page_title": "..." (if identifiable)
}

Return: {"prices": [...]}

Focus on explicit pricing. Skip vague mentions.
```

**Synthesis:**
- Deduplicate identical price mentions
- Group by product/service
- Create price range summary
- Flag any ambiguous/conflicting pricing

---

## Example 5: Multi-Pass Analysis

**Scenario:** Large dataset where you need to first identify patterns, then extract based on those patterns.

**Pass 1: Discover pattern types**

```bash
# Sample randomly from content
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
import random
n = len(content)
samples = [
    peek(random.randint(0, n-5000), random.randint(0, n-5000)+5000)
    for _ in range(5)
]
for i, s in enumerate(samples):
    print(f"\n=== Sample {i} ===")
    print(s[:500])
PY
```

Use subagent to analyze samples and identify patterns.

**Pass 2: Extract based on discovered patterns**

```bash
# Chunk with strategy informed by Pass 1
python3 ~/.letta/skills/rlm/scripts/rlm_repl.py exec <<'PY'
# Use pattern-specific chunking
# Then write chunks
PY
```

Process chunks with updated extraction logic.

---

## Tips for All Workflows

1. **Start with reconnaissance**: Use `peek()` and `grep()` to understand structure before chunking
2. **Choose appropriate chunk size**: 
   - Smaller (100k) for dense, important content
   - Larger (250k) for sparse, repetitive content
3. **Add overlap for safety**: 5-10k chars prevents losing context at boundaries
4. **Validate early**: Process 1-2 chunks first, check quality before processing all
5. **Structured output**: Always request JSON or structured format from subagents
6. **Incremental synthesis**: Don't wait for all chunks - synthesize partial results as you go
7. **Clean up**: Delete `.letta/rlm_chunks/` and state files when done

## Common Patterns

### Pattern: Frequency Analysis
1. Quick grep to count occurrences
2. If high count, chunk and aggregate
3. Present sorted frequency table

### Pattern: Timeline Extraction
1. Chunk by time boundaries (if logs/events)
2. Extract chronological events from each chunk
3. Merge into master timeline
4. Sort and filter by date range

### Pattern: Hierarchical Summary
1. Chunk by document structure (sections/chapters)
2. Summarize each chunk individually
3. Create high-level summary from chunk summaries
4. Present multi-level summary (overview → details)

### Pattern: Entity Extraction
1. Sample to identify entity types
2. Chunk content
3. Extract entities from each chunk
4. Deduplicate and normalize
5. Present entity catalog with frequencies


============================================================
END FILE: .letta/rlm/references/examples.md
============================================================

============================================================
FILE: .letta/rlm/references/subagent-setup.md
============================================================

# Setting Up the rlm-subcall Subagent

This guide explains how to create a dedicated `rlm-subcall` subagent for use with the RLM workflow.

## Why a Dedicated Subagent?

The `rlm-subcall` agent is optimized for:
- Analyzing individual chunks of content
- Extracting structured information
- Returning concise, focused results
- Operating with minimal context overhead

## Option 1: Using Letta Task Tool (Recommended)

The simplest approach is to use Letta's built-in Task tool without creating a dedicated agent:

```python
# Spawn a one-off subagent for each chunk
Task({
    "subagent_type": "general-purpose",
    "description": f"Extract errors from chunk {i}",
    "prompt": f"""Read and analyze: {chunk_path}

Task: {user_query}

Return structured JSON with your findings.
Be concise - extract and summarize, don't dump raw content."""
})
```

**Advantages:**
- No setup required
- Works immediately
- Flexible - adjust prompt per chunk

**Disadvantages:**
- Less specialized than a dedicated agent
- No persistent memory between chunks
- May need more detailed instructions each time

## Option 2: Creating a Dedicated rlm-subcall Agent

For repeated RLM workflows, create a specialized subagent:

### 1. Create the Agent

```bash
# Via Letta CLI
letta create agent \
  --name rlm-subcall \
  --description "Specialized agent for RLM chunk analysis" \
  --preset default
```

Or programmatically if using Letta as a library.

### 2. Configure the Agent's System Prompt

Edit the agent to include this specialized system guidance:

```markdown
You are rlm-subcall, a specialized subagent for RLM (Recursive Language Model) workflows.

## Your Role

You analyze individual chunks of a larger document and return focused, structured results.

## Key Principles

1. **Be Concise**: You're analyzing one chunk of a larger file. Return only extracted/summarized information, never dump raw content.

2. **Follow Structure**: The root agent will specify what format to use (usually JSON). Follow it exactly.

3. **No Meta-commentary**: Don't say "I analyzed the chunk and found..." Just return the requested structure.

4. **Read the Chunk**: You'll be given a file path. Read it with the Read tool.

5. **Extract, Don't Dump**: Your job is extraction and light analysis, not wholesale copying.

## Typical Task Format

You'll receive prompts like:

"""
Read: /path/to/chunk_0042.txt

Task: Extract all error messages with their timestamps.

Return JSON:
{
  "errors": [
    {"timestamp": "...", "message": "...", "severity": "..."}
  ]
}
"""

## Output Format

Prefer structured output:
- JSON for data extraction
- Markdown lists for summaries
- CSV for tabular data

Keep output under 2000 characters unless explicitly requested otherwise.
```

### 3. Test the Agent

```bash
# Test with a sample chunk
letta send --agent rlm-subcall \
  --message "Read: /tmp/test_chunk.txt\n\nTask: Count occurrences of 'ERROR' and 'WARN'.\n\nReturn JSON with counts."
```

### 4. Use in RLM Workflow

When processing chunks, invoke your rlm-subcall agent:

```python
# If using Letta Task tool with your pre-configured agent
Task({
    "agent_id": "rlm-subcall",  # Your agent ID
    "subagent_type": "general-purpose",
    "description": f"Process chunk {i}",
    "prompt": f"""Read: {chunk_path}

Task: {user_query}

Return structured JSON."""
})
```

Or use your platform's method for invoking a specific agent.

## Agent Memory Considerations

**Per-Chunk Analysis (Default):**
- Agent forgets between chunks
- Clean slate for each analysis
- Prevents context pollution

**Cross-Chunk Memory (Advanced):**
- If you need the subagent to remember findings across chunks, maintain a conversation thread
- Use conversation_id to continue the same conversation
- Be cautious of context accumulation

## Best Practices

1. **Clear Instructions**: Tell the subagent exactly what to extract and in what format
2. **Validate Output**: Check that subagent responses match expected structure
3. **Error Handling**: Provide fallback logic if subagent returns unexpected format
4. **Chunk Size**: Keep chunks small enough that subagent analysis is focused
5. **Prompt Consistency**: Use similar prompt structure for all chunks in a workflow

## Alternative: Deploy an Existing Agent

If you already have an agent optimized for analysis tasks, you can deploy it for RLM workflows:

```python
Task({
    "agent_id": "your-existing-agent-id",
    "subagent_type": "explore",  # or "general-purpose"
    "description": "Analyze chunk",
    "prompt": "..."
})
```

See the main SKILL.md for complete workflow integration.


============================================================
END FILE: .letta/rlm/references/subagent-setup.md
============================================================

============================================================
FILE: .letta/rlm/scripts/rlm_repl.py
============================================================

#!/usr/bin/env python3
"""
Persistent mini-REPL for RLM-style workflows in Claude Code.

This script provides a *stateful* Python environment across invocations by
saving a pickle file to disk. It is intentionally small and dependency-free.

Typical flow:
  1) Initialise context:
       python rlm_repl.py init path/to/context.txt
  2) Execute code repeatedly (state persists):
       python rlm_repl.py exec -c 'print(len(content))'
       python rlm_repl.py exec <<'PYCODE'
       # you can write multi-line code
       hits = grep('TODO')
       print(hits[:3])
       PYCODE

The script injects these variables into the exec environment:
  - context: dict with keys {path, loaded_at, content}
  - content: string alias for context['content']
  - buffers: list[str] for storing intermediate text results

It also injects helpers:
  - peek(start=0, end=1000) -> str
  - grep(pattern, max_matches=20, window=120, flags=0) -> list[dict]
  - chunk_indices(size=200000, overlap=0) -> list[(start,end)]
  - write_chunks(out_dir, size=200000, overlap=0, prefix='chunk') -> list[str]
  - add_buffer(text: str) -> None

Security note:
  This runs arbitrary Python via exec. Treat it like running code you wrote.
"""

from __future__ import annotations

import argparse
import io
import os
import pickle
import re
import sys
import textwrap
import time
import traceback
from contextlib import redirect_stderr, redirect_stdout
from pathlib import Path
from typing import Any, Dict, List, Tuple

DEFAULT_STATE_PATH = Path(".claude/rlm_state/state.pkl")
DEFAULT_MAX_OUTPUT_CHARS = 8000


