<!-- Chunk 1259: bytes 4681371-4693412, type=class -->
class DSPyTracker:
    """Comprehensive DSPy tracking for production monitoring."""

    def __init__(self, output_path: str = "config/dspy_tracking"):
        self.output_path = Path(output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)

        self.executions: list[ExecutionMetrics] = []
        self.optimizations: list[OptimizationMetrics] = []
        self.session_start = time.time()

        # LM call tracking
        self.lm_call_count = 0
        self.total_tokens = 0
        self.total_cost = 0.0

        logger.info(f"âœ… DSPyTracker initialized - Output: {self.output_path}")

    def track_execution(
        self,
        model_name: str,
        task_type: str,
        execution_fn,
        **kwargs,
    ) -> Any:
        """
        Track a DSPy execution with detailed metrics.

        Args:
            model_name: Name of LM model being used
            task_type: Type of DSPy task (prediction, optimization, etc.)
            execution_fn: Function to execute (will be timed and tracked)
            **kwargs: Arguments to pass to execution function

        Returns:
            Result of execution function

        """
        start_time = time.time()

        try:
            # Execute function
            result = execution_fn(**kwargs)
            execution_time = time.time() - start_time

            # Estimate tokens (rough estimate: 1 token ~ 4 characters)
            estimated_tokens = max(1, int(100 * execution_time))
            estimated_cost = estimated_tokens * 0.00003  # $0.03 per 1K tokens (approx)

            # Update totals
            self.lm_call_count += 1
            self.total_tokens += estimated_tokens
            self.total_cost += estimated_cost

            # Create metrics record
            metrics = ExecutionMetrics(
                timestamp=datetime.now().isoformat(),
                model_name=model_name,
                task_type=task_type,
                execution_time_seconds=execution_time,
                input_tokens=int(estimated_tokens * 0.4),  # Approx 40% input
                output_tokens=int(estimated_tokens * 0.6),  # Approx 60% output
                total_tokens=estimated_tokens,
                estimated_cost_usd=estimated_cost,
                success=True,
                error=None,
            )

            self.executions.append(metrics)

            logger.info(
                f"âœ… Execution tracked: {task_type} "
                f"({execution_time:.2f}s, {estimated_tokens} tokens, ${estimated_cost:.4f})"
            )

            return result

        except Exception as e:
            execution_time = time.time() - start_time

            metrics = ExecutionMetrics(
                timestamp=datetime.now().isoformat(),
                model_name=model_name,
                task_type=task_type,
                execution_time_seconds=execution_time,
                input_tokens=0,
                output_tokens=0,
                total_tokens=0,
                estimated_cost_usd=0.0,
                success=False,
                error=str(e),
            )

            self.executions.append(metrics)
            logger.error(f"âŒ Execution failed: {task_type} - {e}")

            raise

    def track_optimization(
        self,
        optimizer_type: str,
        trainset: list[dspy.Example],
        testset: list[dspy.Example],
        baseline_score: float,
        optimized_score: float,
        parameters: dict[str, Any],
        execution_time_seconds: float,
    ) -> OptimizationMetrics:
        """
        Track optimization results with detailed metrics.

        Args:
            optimizer_type: Type of optimizer (MIPROv2, GEPA, BootstrapFewShot, etc.)
            trainset: Training examples used
            testset: Test examples used
            baseline_score: Score before optimization
            optimized_score: Score after optimization
            parameters: Optimizer parameters (auto level, demos, etc.)
            execution_time_seconds: Time taken for optimization

        Returns:
            OptimizationMetrics object

        """
        improvement = optimized_score - baseline_score
        improvement_percent = (improvement / baseline_score * 100) if baseline_score > 0 else 0

        metrics = OptimizationMetrics(
            timestamp=datetime.now().isoformat(),
            optimizer_type=optimizer_type,
            trainset_size=len(trainset),
            testset_size=len(testset),
            baseline_score=baseline_score,
            optimized_score=optimized_score,
            improvement=improvement,
            improvement_percent=improvement_percent,
            execution_time_seconds=execution_time_seconds,
            lm_calls=self.lm_call_count,
            total_cost_usd=self.total_cost,
            iterations=parameters.get("iterations", 1),
            parameters=parameters,
        )

        self.optimizations.append(metrics)

        logger.info(
            f"âœ… Optimization tracked: {optimizer_type} "
            f"(Î”{improvement:+.1%} in {execution_time_seconds:.1f}s, "
            f"{len(trainset)} train examples)"
        )

        return metrics

    def save_session(self) -> None:
        """Save all tracked metrics to JSON files."""
        session_duration = time.time() - self.session_start

        # Create session summary
        summary = {
            "session_start": datetime.fromtimestamp(self.session_start).isoformat(),
            "session_end": datetime.now().isoformat(),
            "duration_seconds": session_duration,
            "total_executions": len(self.executions),
            "total_optimizations": len(self.optimizations),
            "total_lm_calls": self.lm_call_count,
            "total_tokens": self.total_tokens,
            "total_cost_usd": round(self.total_cost, 4),
            "executions_by_type": self._group_by_task_type(),
            "optimizations_by_type": self._group_by_optimizer_type(),
            "cost_breakdown": {
                "per_execution_avg": round(self.total_cost / max(len(self.executions), 1), 4),
                "per_optimization_avg": round(self.total_cost / max(len(self.optimizations), 1), 4),
            },
        }

        # Save to files
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Session summary
        session_file = self.output_path / f"session_{timestamp}.json"
        with open(session_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2)

        # Executions
        executions_file = self.output_path / f"executions_{timestamp}.json"
        with open(executions_file, "w", encoding="utf-8") as f:
            json.dump([e.__dict__ for e in self.executions], f, indent=2)

        # Optimizations
        optimizations_file = self.output_path / f"optimizations_{timestamp}.json"
        with open(optimizations_file, "w", encoding="utf-8") as f:
            json.dump([o.__dict__ for o in self.optimizations], f, indent=2)

        logger.info(f"ðŸ’¾ Saved tracking data to {self.output_path}/")
        logger.info(f"  - Session: {session_file}")
        logger.info(f"  - Executions: {executions_file}")
        logger.info(f"  - Optimizations: {optimizations_file}")
        logger.info(f"  - Total cost: ${summary['total_cost_usd']:.2f}")
        logger.info(f"  - Total time: {session_duration:.1f}s")

    def _group_by_task_type(self) -> dict[str, Any]:
        """Group executions by task type."""
        groups = defaultdict(list)
        for execution in self.executions:
            groups[execution.task_type].append(
                {
                    "count": len(
                        [e for e in self.executions if e.task_type == execution.task_type]
                    ),
                    "avg_time": sum(
                        e.execution_time_seconds
                        for e in self.executions
                        if e.task_type == execution.task_type
                    )
                    / len([e for e in self.executions if e.task_type == execution.task_type]),
                    "total_time": sum(
                        e.execution_time_seconds
                        for e in self.executions
                        if e.task_type == execution.task_type
                    ),
                    "success_rate": sum(
                        1
                        for e in self.executions
                        if e.task_type == execution.task_type and e.success
                    )
                    / len([e for e in self.executions if e.task_type == execution.task_type])
                    if self.executions
                    else 0,
                }
            )
        return dict(groups)

    def _group_by_optimizer_type(self) -> dict[str, Any]:
        """Group optimizations by optimizer type."""
        groups = defaultdict(list)
        for opt in self.optimizations:
            groups[opt.optimizer_type].append(
                {
                    "count": len(
                        [o for o in self.optimizations if o.optimizer_type == opt.optimizer_type]
                    ),
                    "avg_improvement": sum(
                        o.improvement
                        for o in self.optimizations
                        if o.optimizer_type == opt.optimizer_type
                    )
                    / len([o for o in self.optimizations if o.optimizer_type == opt.optimizer_type])
                    if self.optimizations
                    else 0,
                    "avg_time": sum(
                        o.execution_time_seconds
                        for o in self.optimizations
                        if o.optimizer_type == opt.optimizer_type
                    )
                    / len([o for o in self.optimizations if o.optimizer_type == opt.optimizer_type])
                    if self.optimizations
                    else 0,
                    "avg_cost": sum(
                        o.total_cost_usd
                        for o in self.optimizations
                        if o.optimizer_type == opt.optimizer_type
                    )
                    / len([o for o in self.optimizations if o.optimizer_type == opt.optimizer_type])
                    if self.optimizations
                    else 0,
                    "avg_lmcalls": sum(
                        o.lm_calls
                        for o in self.optimizations
                        if o.optimizer_type == opt.optimizer_type
                    )
                    / len([o for o in self.optimizations if o.optimizer_type == opt.optimizer_type])
                    if self.optimizations
                    else 0,
                }
            )
        return dict(groups)

    def print_summary(self) -> None:
        """Print summary of tracked metrics."""
        print("\n" + "=" * 70)
        print("ðŸ“Š DSPy TRACKING SUMMARY")
        print("=" * 70)

        print(f"\nSession Duration: {time.time() - self.session_start:.1f}s")
        print(f"Total Executions: {len(self.executions)}")
        print(f"Total Optimizations: {len(self.optimizations)}")
        print(f"Total LM Calls: {self.lm_call_count}")
        print(f"Total Tokens: {self.total_tokens:,}")
        print(f"Total Cost: ${self.total_cost:.2f}")

        # Group by optimizer
        if self.optimizations:
            print("\n" + "-" * 70)
            print("ðŸ”· OPTIMIZER PERFORMANCE")
            print("-" * 70)

            groups = self._group_by_optimizer_type()
            for opt_type, stats in sorted(groups.items(), key=lambda x: -x[1]["avg_improvement"]):
                print(f"\n{opt_type}:")
                print(f"  Runs: {stats['count']}")
                print(f"  Avg Improvement: {stats['avg_improvement']:+.1%}")
                print(f"  Avg Time: {stats['avg_time']:.2f}s")
                print(f"  Avg Cost: ${stats['avg_cost']:.4f}")
                print(f"  Avg LM Calls: {stats['avg_lmcalls']:.0f}")

        print("\n" + "=" * 70)


