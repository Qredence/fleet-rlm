<!-- Chunk 1790: bytes 7460876-7463594, type=function -->
def phase2_checkpoint_score(
    skill_metadata: str | dict,
    capabilities: list,
    dependencies: list,
    confirmed_type: str,
    weight: str,
    load_priority: str,
) -> float:
    """
    Calculate Phase 2 checkpoint score.

    This is a convenience function for calculating the checkpoint score
    directly from Phase 2 outputs, without requiring a Prediction object.

    Args:
        skill_metadata: Skill metadata (dict or JSON string)
        capabilities: List of capabilities
        dependencies: List of dependencies
        confirmed_type: Confirmed skill type
        weight: Weight assignment
        load_priority: Load priority decision

    Returns:
        Float score from 0.0 to 1.0

    Examples:
        >>> score = phase2_checkpoint_score(
        ...     skill_metadata={"skill_id": "test", "name": "test"},
        ...     capabilities=["cap1", "cap2", "cap3"],
        ...     dependencies=[],
        ...     confirmed_type="technical",
        ...     weight="lightweight",
        ...     load_priority="task_specific"
        ... )
        >>> print(score)  # Should be 1.0

    """
    pred = {
        "confirmed_type": confirmed_type,
        "weight": weight,
        "load_priority": load_priority,
        "capabilities": capabilities,
        "dependencies_valid": True,  # Assume valid if we got this far
    }

    return phase2_validity_metric({}, pred)


__all__ = [
    "phase2_validity_metric",
    "phase2_checkpoint_score",
    "VALID_SKILL_TYPES",
    "VALID_WEIGHTS",
    "VALID_PRIORITIES",
]


============================================================
END FILE: src/skill_fleet/core/optimization/rewards/phase2_rewards.py
============================================================

============================================================
FILE: src/skill_fleet/core/optimization/rewards/step_rewards.py
============================================================

"""
Reward functions for DSPy Refine and BestOfN modules.

These functions score LLM outputs for quality assurance, enabling:
- dspy.Refine: Automatic feedback loop to improve outputs
- dspy.BestOfN: Select best output from multiple attempts

All reward functions follow the signature:
    def reward_fn(args, pred: dspy.Prediction) -> float:
        # Returns score between 0.0 and 1.0
"""

from __future__ import annotations

import json
import logging
import re
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    import dspy

logger = logging.getLogger(__name__)


# =============================================================================
# Step 1: Understand - Taxonomy Path Validation
# =============================================================================


