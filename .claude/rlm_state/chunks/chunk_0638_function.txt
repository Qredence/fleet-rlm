<!-- Chunk 638: bytes 995935-999064, type=function -->
def create_phase_specific_metric(phase: str):
    """Factory for creating appropriate metric for each phase.
    
    Different workflow phases need different evaluation focus:
    - Phase 1 (Understanding): Taxonomy + intent accuracy
    - Phase 2 (Generation): Comprehensive quality
    - Phase 3 (Validation): Metadata + compliance
    
    Args:
        phase: Phase name ("understanding", "generation", "validation")
    
    Returns:
        Metric function appropriate for that phase
    """
    if phase == "understanding":
        # Focus on intent and taxonomy
        def understanding_metric(ex, pred, trace=None):
            tax_score = taxonomy_accuracy_metric(ex, pred, trace)
            
            # Check topics identified
            topics_score = 0.0
            if hasattr(pred, "topics") and len(pred.topics) >= 3:
                topics_score = 0.5
            
            return 0.7 * tax_score + 0.3 * topics_score
        
        return understanding_metric
    
    elif phase == "generation":
        # Focus on comprehensive quality
        return composite_metric
    
    elif phase == "validation":
        # Focus on compliance
        from skill_fleet.core.dspy.metrics.enhanced_metrics import metadata_quality_metric
        return metadata_quality_metric
    
    else:
        # Default: composite
        return composite_metric


# Usage example
if __name__ == "__main__":
    import dspy
    
    # Create mock example and prediction
    example = dspy.Example(
        task_description="Create async Python skill",
        expected_taxonomy_path="python/async",
        expected_skill_style="comprehensive",
    ).with_inputs("task_description")
    
    prediction = dspy.Prediction(
        domain="python",
        category="async",
        taxonomy_path="python/async",
        topics=["asyncio", "await", "tasks", "event-loop"],
        skill_style="comprehensive",
    )
    
    # Test different metrics
    print("Testing metrics...")
    print(f"Simple: {simple_quality_metric(example, prediction):.3f}")
    print(f"Taxonomy: {taxonomy_accuracy_metric(example, prediction):.3f}")
    print(f"Composite: {composite_metric(example, prediction):.3f}")
    
    # Test phase-specific metrics
    understanding_metric = create_phase_specific_metric("understanding")
    print(f"Understanding: {understanding_metric(example, prediction):.3f}")


============================================================
END FILE: .fleet/skills/dspy-optimization-workflow/examples/example_metric.py
============================================================

============================================================
FILE: .fleet/skills/dspy-optimization-workflow/examples/example_signature.py
============================================================

"""Example of a well-structured DSPy signature with best practices.

This example demonstrates:
- Literal types for constrained outputs
- Specific OutputField descriptions with quality indicators
- Concise, actionable docstring
- Proper type hints
"""

from __future__ import annotations

from typing import Literal

import dspy


