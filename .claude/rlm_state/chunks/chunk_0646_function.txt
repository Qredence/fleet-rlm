<!-- Chunk 646: bytes 1018701-1023826, type=function -->
def validate_skill_output(result):
    """Validate skill generation output."""
    if not hasattr(result, "skill_content"):
        raise ValueError("Missing skill_content field")
    
    if len(result.skill_content) < 100:
        raise ValueError("Skill content too short")
    
    if "# " not in result.skill_content:
        raise ValueError("Skill missing header")
    
    return True

validated = ValidatedModule(
    generator,
    validator=validate_skill_output,
    raise_on_invalid=True,
)
```

**Why**: Catch invalid outputs early before they propagate through the system.

## Ensemble Methods

### Use BestOfN for Critical Generations

```python
from skill_fleet.core.dspy.modules.ensemble import BestOfN

generator = dspy.ChainOfThought("task -> skill_content")

best_of_3 = BestOfN(
    module=generator,
    n=3,
    quality_fn=lambda x: score_skill_quality(x.skill_content),
)

# Generates 3 candidates, returns highest quality
result = best_of_3(task="Create async Python skill")
```

**Why**: Multiple attempts improve quality for critical outputs. 3-5 is optimal (diminishing returns after that).

### Use MajorityVote for Classification

```python
from skill_fleet.core.dspy.modules.ensemble import MajorityVote

classifiers = [model1, model2, model3]

voter = MajorityVote(
    modules=classifiers,
    vote_field="category",
    min_agreement=0.66,  # Require 2/3 agreement
)

result = voter(text="Sample skill description")
```

**Why**: Consensus reduces errors in classification tasks.

## Caching

### Cache Expensive Operations

```python
from skill_fleet.core.dspy.caching import CachedModule

expensive_module = LargeModelInference()

cached = CachedModule(
    expensive_module,
    cache_dir=".cache/inference",
    ttl_seconds=86400,  # 24 hour TTL
    use_memory=True,  # Memory + disk caching
)

# First call: executes module (slow)
result1 = cached(input="test")

# Second call with same input: cached (instant)
result2 = cached(input="test")

# Check cache stats
stats = cached.get_stats()
print(f"Hit rate: {stats['hit_rate']:.2%}")
```

**Why**: Caching can improve performance by 30-50% for deterministic operations.

## Versioning & A/B Testing

### Use ProgramRegistry for Version Management

```python
from skill_fleet.core.dspy.versioning import ProgramRegistry

registry = ProgramRegistry("config/optimized")

# Register new version after optimization
registry.register(
    program=optimized_v2,
    name="skill_generator_v2",
    optimizer="miprov2",
    quality_score=0.87,
    training_examples=50,
)

# Load for production
production_program = registry.load("skill_generator_v2")

# Compare versions
comparison = registry.compare(
    "skill_generator_v1",
    "skill_generator_v2",
)
print(f"Quality improvement: {comparison['quality_diff']:+.3f}")
```

**Why**: Track multiple versions, enable rollback, compare performance.

### Gradual Rollout with ABTestRouter

```python
from skill_fleet.core.dspy.versioning import ABTestRouter

router = ABTestRouter(
    variants={
        "v1": program_v1,  # Current production
        "v2": program_v2,  # New optimized version
    },
    weights={
        "v1": 0.9,  # 90% of traffic
        "v2": 0.1,  # 10% of traffic
    },
    strategy="weighted",
)

# Route requests
result = router(task="Generate skill", user_id=user_id)

# Monitor performance
stats = router.get_stats()
for variant, metrics in stats.items():
    print(f"{variant}: {metrics['success_rate']:.2%} success")
```

**Why**: Gradual rollout reduces risk when deploying new optimized versions.

## Common Pitfalls to Avoid

❌ **Don't optimize on <50 examples** - Leads to overfitting  
❌ **Don't skip baseline evaluation** - You need a comparison point  
❌ **Don't use auto="heavy" by default** - Usually not worth the cost  
❌ **Don't ignore type errors** - They cause runtime failures  
❌ **Don't skip monitoring** - You're flying blind without it  
❌ **Don't cache non-deterministic operations** - Defeats the purpose  
❌ **Don't deploy without version management** - Makes rollback impossible  
❌ **Don't forget train/test split** - Evaluation will be biased


============================================================
END FILE: .fleet/skills/dspy-optimization-workflow/references/best-practices.md
============================================================

============================================================
FILE: .fleet/skills/dspy-optimization-workflow/references/phase1-implementation.md
============================================================

# Phase 1 Implementation Guide

Complete walkthrough of Phase 1: Foundation (signatures, training data, monitoring).

## Overview

**Goal**: Prepare DSPy programs for effective optimization  
**Duration**: 1-2 days  
**Prerequisites**: Basic DSPy knowledge, Python 3.12+

**Deliverables**:
1. Enhanced signatures with Literal types
2. Training dataset with 50-100 examples
3. Monitoring infrastructure for production

## Task 1: Signature Enhancements

### Step 1: Add Literal Types

**Files to modify**: `src/skill_fleet/core/dspy/signatures/*.py`

**Pattern**:
```python
# Before
