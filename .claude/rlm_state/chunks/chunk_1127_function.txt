<!-- Chunk 1127: bytes 4222568-4224790, type=function -->
def ensemble_forward(input):
    results = [p(input) for p in ensemble.programs]
    # Custom selection logic
    return select_best(results)
```

### Validation with Separate Test Set

```python
# Split data properly
train_examples = trainset[:80]
test_examples = trainset[80:]

# Optimize on train
optimizer = dspy.MIPROv2(metric=metric, auto="medium")
optimized = optimizer.compile(program, trainset=train_examples)

# Evaluate on separate test
from dspy.evaluate import Evaluate

evaluator = Evaluate(devset=test_examples, metric=metric)
test_score = evaluator(optimized)

print(f"Train performance: {train_score:.2%}")
print(f"Test performance: {test_score:.2%}")
```

## Troubleshooting

### Issue: "No improvement detected"

**Possible causes**:
1. Metric is too simple or permissive
2. Training data not representative
3. Program already near-optimal
4. Insufficient training data

**Solutions**:
- Review metric design for stricter evaluation
- Expand training data with diverse examples
- Check if baseline is already high (>90%)
- Try MIPROv2 instead of BootstrapFewShot

### Issue: "Optimization runs very slowly"

**Possible causes**:
1. Using MIPROv2 with `auto="heavy"`
2. Very large training set
3. Complex program with many modules

**Solutions**:
- Use MIPROv2 `auto="light"` instead
- Reduce trainset to 50-100 examples
- Optimize individual modules separately
- Try GEPA for faster reflection-based optimization

### Issue: "OutOfMemory during optimization"

**Possible causes**:
1. Training set too large
2. LM context window exceeded
3. Caching too many traces

**Solutions**:
- Reduce training set size to 50-100 examples
- Reduce `max_bootstrapped_demos` to 1-2
- Use lighter optimizer (BootstrapFewShot or GEPA)
- Clear DSPy cache: `rm -rf .dspy_cache`

## Best Practices

### 1. Always Use Separate Train/Test Sets

```python
# Good
trainset = examples[:80]
testset = examples[80:]
optimized = optimizer.compile(program, trainset=trainset)
final_score = evaluate(optimized, testset)

# Bad - overfitting
optimized = optimizer.compile(program, trainset=examples)
final_score = evaluate(optimized, examples)  # Same data!
```

### 2. Define Clear Success Metrics

```python
# Good - clear criteria
