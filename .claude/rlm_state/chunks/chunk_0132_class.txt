<!-- Chunk 132: bytes 181467-187643, type=class -->
class CacheStats:
    def __init__(self):
        self.hits = 0
        self.misses = 0

    def record_hit(self):
        self.hits += 1

    def record_miss(self):
        self.misses += 1

    def hit_rate(self):
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

# Use cache stats
stats = CacheStats()
# ... monitor cache operations ...
print(f"Cache hit rate: {stats.hit_rate():.2%}")
```

### 5. Use Disk Cache for Persistent Storage

```python
# Good: Enable disk cache for persistent storage
dspy.configure_cache(
    enable_disk_cache=True,
    disk_cache_dir="/path/to/cache",
)

# Bad: Only use memory cache (lost on restart)
dspy.configure_cache(
    enable_disk_cache=False,
    enable_memory_cache=True,
)
```

## Common Issues and Solutions

### Issue: Cache Not Working

**Problem**: Caching seems to have no effect

**Solution**:
1. Verify caching is enabled (it's enabled by default)
2. Check cache directory has write permissions
3. Verify request is being cached (check cache_key())
4. Check cache size (may be full or expired)

### Issue: Stale Cache Results

**Problem**: Cache returns old, outdated results

**Solution**:
1. Clear cache after program changes
2. Implement TTL (time-to-live) for cache entries
3. Use custom cache keys to include version information
4. Regularly clear cache during development

### Issue: Cache Too Large

**Problem**: Cache consumes too much disk or memory

**Solution**:
1. Disable disk cache if memory is sufficient
2. Implement TTL to expire old entries
3. Regularly clear cache
4. Use custom cache with limited size

### Issue: Cache Keys Collide

**Problem**: Different requests return same cached result

**Solution**:
1. Review custom cache key implementation
2. Ensure all relevant arguments are included in key
3. Add unique identifiers (e.g., model name, timestamp)
4. Test cache key generation with varied inputs


============================================================
END FILE: .fleet/factory/skills/dspy-configuration/references/caching.md
============================================================

============================================================
FILE: .fleet/factory/skills/dspy-configuration/references/lm-config.md
============================================================

# DSPy Language Model Configuration

Configure which language models to use and how to switch between them. This guide covers LM setup, multi-provider configuration, and local overrides.

## Table of Contents

- [LM Configuration Overview](#lm-configuration-overview)
- [Basic LM Configuration](#basic-lm-configuration)
- [Multi-Provider Setup](#multi-provider-setup)
- [Local LM Overrides](#local-lm-overrides)
- [Responses API](#responses-api)
- [Best Practices](#best-practices)

## LM Configuration Overview

### What is LM Configuration?

DSPy uses the `dspy.LM` class to specify which language model to use for program execution. The `dspy.configure()` method sets this language model globally across all DSPy program invocations.

### Why Configure LMs?

- **Flexibility**: Easily switch between different models
- **Cost control**: Use cheaper models for simple tasks
- **Quality**: Use better models for complex reasoning
- **Multi-provider**: Access LLMs from multiple providers

## Basic LM Configuration

### Configure OpenAI LM

```python
import dspy

# Configure OpenAI LM
lm = dspy.LM('openai/gpt-4o-mini')
dspy.configure(lm=lm)
```

### Configure with API Key

```python
# Configure with API key directly
lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')
dspy.configure(lm=lm)
```

### Configure with Temperature

```python
# Configure with temperature
lm = dspy.LM('openai/gpt-4o-mini', temperature=0.7)
dspy.configure(lm=lm)
```

### Configure with Max Tokens

```python
# Configure with max tokens
lm = dspy.LM('openai/gpt-4o-mini', max_tokens=1000)
dspy.configure(lm=lm)
```

## Multi-Provider Setup

### Anthropic LM

```python
# Configure Anthropic LM
lm = dspy.LM("anthropic/claude-3-opus-20240229")
dspy.configure(lm=lm)
```

### Together AI LM

```python
# Configure Together AI LM
lm = dspy.LM("together_ai/meta-llama/Llama-2-70b-chat-hf")
dspy.configure(lm=lm)
```

### General Provider Configuration

```python
# Configure with general provider (via LiteLLM)
lm = dspy.LM(
    "openai/your-model-name",
    api_key="PROVIDER_API_KEY",
    api_base="YOUR_PROVIDER_URL"
)
dspy.configure(lm=lm)
```

### Multiple Providers Example

```python
# OpenAI
openai_lm = dspy.LM('openai/gpt-4o-mini')

# Anthropic
anthropic_lm = dspy.LM("anthropic/claude-3-opus-20240229")

# Use different providers for different tasks
fast_lm = openai_lm
quality_lm = anthropic_lm

# Switch between them (see Local LM Overrides section)
```

## Local LM Overrides

### Global Configuration

Configure LM globally for entire session:

```python
import dspy

# Configure LM globally
dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))

# All programs use this LM
response = qa(question="Test question")
print('GPT-4o-mini:', response.answer)
```

### Local Context Override

Switch LM within a specific code block:

```python
import dspy

# Configure LM globally
dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))

# Use global LM
response = qa(question="Test question")
print('GPT-4o-mini:', response.answer)

# Change LM within a context block
with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):
    response = qa(question="Test question")
    print('GPT-3.5-turbo:', response.answer)

# Back to global LM
response = qa(question="Test question")
print('GPT-4o-mini:', response.answer)
```

### Task-Specific LM Configuration

```python
# Fast LM for simple tasks
fast_lm = dspy.LM('openai/gpt-3.5-turbo')

# Quality LM for complex tasks
quality_lm = dspy.LM('openai/gpt-4o')

# Use fast LM for classification
with dspy.context(lm=fast_lm):
    result = classify(text="This is a test")

# Use quality LM for reasoning
with dspy.context(lm=quality_lm):
    result = reason(task="Solve this complex problem")
```

### Thread-Safe Configuration

Both `dspy.configure()` and `dspy.context()` are thread-safe:

```python
import threading
import dspy

