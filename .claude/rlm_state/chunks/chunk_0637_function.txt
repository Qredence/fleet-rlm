<!-- Chunk 637: bytes 994073-995935, type=function -->
def composite_metric(example: dspy.Example, prediction: dspy.Prediction, trace=None) -> float:
    """Composite metric combining multiple dimensions.
    
    Weights different quality aspects for balanced optimization.
    This is the recommended approach for production use.
    
    Dimensions:
    - 40%: Taxonomy accuracy (correct placement)
    - 30%: Content quality (structure, examples)
    - 20%: Metadata quality (name, description, tags)
    - 10%: Style alignment (comprehensive vs minimal)
    
    Args:
        example: Training example
        prediction: Model prediction
        trace: Optional trace
    
    Returns:
        Weighted score 0.0-1.0
    """
    # Import sub-metrics
    from skill_fleet.core.dspy.metrics.enhanced_metrics import (
        taxonomy_accuracy_metric,
        metadata_quality_metric,
        skill_style_alignment_metric,
    )
    
    # Calculate component scores
    taxonomy_score = taxonomy_accuracy_metric(example, prediction, trace)
    metadata_score = metadata_quality_metric(example, prediction, trace)
    style_score = skill_style_alignment_metric(example, prediction, trace)
    
    # Content quality (simplified - in production, use full skill_quality_metric)
    content_score = 0.0
    if hasattr(prediction, "skill_content"):
        content = prediction.skill_content
        
        # Check length
        if 500 <= len(content) <= 5000:
            content_score += 0.3
        
        # Check has examples
        if "```" in content:
            content_score += 0.4
        
        # Check has structure
        if content.count("#") >= 3:
            content_score += 0.3
    
    # Weighted combination
    weighted_score = (
        0.4 * taxonomy_score +
        0.3 * content_score +
        0.2 * metadata_score +
        0.1 * style_score
    )
    
    return weighted_score


