<!-- Chunk 1928: bytes 7748861-7753021, type=function -->
def log_validation_results(
    validation_report: dict[str, Any],
    prefix: str = "validation",
) -> None:
    """
    Log validation results as metrics and artifacts.

    Extracts key validation metrics from the report and logs them both
    as metrics (for charting) and as a JSON artifact (for details).

    Args:
        validation_report: Validation report dictionary
        prefix: Prefix for metric names (default: "validation")

    Examples:
        >>> log_validation_results({
        ...     "passed": True,
        ...     "score": 0.95,
        ...     "issues": [{"severity": "error", "message": "..."}]
        ... })
        # Logs: validation_passed=1.0, validation_score=0.95, validation_issues_count=1

    """
    try:
        import json

        import mlflow

        metrics: dict[str, float] = {}

        # Extract common validation metrics
        passed = validation_report.get("passed")
        if passed is not None:
            metrics[f"{prefix}_passed"] = float(passed)

        score = validation_report.get("score")
        if score is not None:
            metrics[f"{prefix}_score"] = float(score)

        issues = validation_report.get("issues", [])
        if isinstance(issues, list):
            metrics[f"{prefix}_issues_count"] = float(len(issues))
            metrics[f"{prefix}_errors_count"] = float(
                sum(1 for i in issues if i.get("severity") == "error")
            )
            metrics[f"{prefix}_warnings_count"] = float(
                sum(1 for i in issues if i.get("severity") == "warning")
            )

        mlflow.log_metrics(metrics)  # type: ignore[attr-defined]

        # Log full report as artifact
        report_json = json.dumps(validation_report, indent=2, default=str)
        mlflow.log_text(report_json, artifact_file=f"{prefix}_report.json")  # type: ignore[attr-defined]

        logger.debug(f"Logged validation results: {metrics}")

    except Exception as e:
        logger.warning(f"Failed to log validation results: {e}")


__all__ = [
    # Original functions
    "setup_mlflow_experiment",
    "log_phase_metrics",
    "log_decision_tree",
    "log_checkpoint_result",
    "log_phase_artifact",
    "log_parameter",
    "get_mlflow_run_id",
    "end_mlflow_run",
    # Enhanced v2.0 functions
    "start_parent_run",
    "start_child_run",
    "end_parent_run",
    "log_tags",
    "log_lm_usage",
    "log_quality_metrics",
    "log_skill_artifacts",
    "log_validation_results",
    # Constants
    "DEFAULT_EXPERIMENT_NAME",
    "DEFAULT_TRACKING_URI",
]


============================================================
END FILE: src/skill_fleet/infrastructure/tracing/mlflow.py
============================================================

============================================================
FILE: src/skill_fleet/infrastructure/tracing/tracer.py
============================================================

"""
Capture and store LM reasoning traces throughout skill creation.

This module provides the ReasoningTracer class for capturing, displaying,
and storing LM reasoning traces during the skill creation workflow.

Tracer Modes:
- CLI: Display reasoning in real-time with --verbose flag
- MLflow: Log to MLflow with --debug flag for experiment tracking
- Training: Save to file only for training dataset runs
- None: Disable tracing

Example usage:
    # Create tracer for CLI display
    tracer = ReasoningTracer(mode="cli")

    # Start a run
    tracer.start_run("Create async Python skill", is_training=False)

    # Capture reasoning from DSPy module result
    result = my_module(input="test")
    tracer.trace(
        phase="phase1",
        step="extract_problem",
        result=result,
        inputs={"input": "test"}
    )

    # End run
    tracer.end_run(save_traces=False)
"""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any

from ...common.dspy_compat import coerce_reasoning_text

if TYPE_CHECKING:
    import dspy

logger = logging.getLogger(__name__)


@dataclass
