CHUNK_6: Offset 0-200000
Context: unknown
============================================================

Directory structure:
â””â”€â”€ stanfordnlp-dspy/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ mkdocs.yml
    â”‚   â”œâ”€â”€ Pipfile
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ vercel.json
    â”‚   â”œâ”€â”€ docs/
    â”‚   â”‚   â”œâ”€â”€ cheatsheet.md
    â”‚   â”‚   â”œâ”€â”€ faqs.md
    â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ roadmap.md
    â”‚   â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ adapters/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Adapter.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatAdapter.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ JSONAdapter.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TwoStepAdapter.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ answer_exact_match.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ answer_passage_match.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CompleteAndGrounded.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Evaluate.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EvaluationResult.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ SemanticF1.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ experimental/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Citations.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Document.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Embedder.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ LM.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ modules/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BestOfN.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChainOfThought.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CodeAct.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Module.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MultiChainComparison.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Parallel.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Predict.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ProgramOfThought.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ReAct.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Refine.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ optimizers/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BetterTogether.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BootstrapFewShot.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BootstrapFewShotWithRandomSearch.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BootstrapFinetune.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BootstrapRS.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ COPRO.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Ensemble.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InferRules.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ KNN.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ KNNFewShot.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ LabeledFewShot.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MIPROv2.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SIMBA.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ GEPA/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ GEPA_Advanced.md
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ overview.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ primitives/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Audio.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Code.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Example.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ History.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Image.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Prediction.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Tool.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ToolCalls.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ signatures/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InputField.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ OutputField.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Signature.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ tools/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ColBERTv2.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Embeddings.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ PythonInterpreter.md
    â”‚   â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚   â”‚       â”œâ”€â”€ asyncify.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ configure_cache.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ disable_litellm_logging.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ disable_logging.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ enable_litellm_logging.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ enable_logging.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ inspect_history.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ load.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ StatusMessage.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ StatusMessageProvider.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ streamify.md
    â”‚   â”‚   â”‚       â””â”€â”€ StreamListener.md
    â”‚   â”‚   â”œâ”€â”€ community/
    â”‚   â”‚   â”‚   â”œâ”€â”€ community-ports.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ community-resources.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ how-to-contribute.md
    â”‚   â”‚   â”‚   â””â”€â”€ use-cases.md
    â”‚   â”‚   â”œâ”€â”€ deep-dive/
    â”‚   â”‚   â”‚   â””â”€â”€ data-handling/
    â”‚   â”‚   â”‚       â”œâ”€â”€ built-in-datasets.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ examples.md
    â”‚   â”‚   â”‚       â””â”€â”€ loading-custom-data.md
    â”‚   â”‚   â”œâ”€â”€ js/
    â”‚   â”‚   â”‚   â”œâ”€â”€ runllm-widget.js
    â”‚   â”‚   â”‚   â””â”€â”€ tutorial-nav.js
    â”‚   â”‚   â”œâ”€â”€ learn/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ overview.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ optimization/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ optimizers.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ overview.md
    â”‚   â”‚   â”‚   â””â”€â”€ programming/
    â”‚   â”‚   â”‚       â”œâ”€â”€ 7-assertions.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ adapters.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ language_models.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ mcp.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ modules.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ overview.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ signatures.md
    â”‚   â”‚   â”‚       â””â”€â”€ tools.md
    â”‚   â”‚   â”œâ”€â”€ production/
    â”‚   â”‚   â”‚   â””â”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ static/
    â”‚   â”‚   â”‚   â””â”€â”€ .nojekyll
    â”‚   â”‚   â”œâ”€â”€ stylesheets/
    â”‚   â”‚   â”‚   â””â”€â”€ extra.css
    â”‚   â”‚   â””â”€â”€ tutorials/
    â”‚   â”‚       â”œâ”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ ai_text_game/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ async/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ build_ai_program/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ cache/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ classification/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ classification_finetuning/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ conversation_history/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ core_development/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ custom_module/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ deployment/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ email_extraction/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ games/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ gepa_ai_program/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ gepa_papillon/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ llms_txt_generation/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ math/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ mcp/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ mem0_react_agent/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ observability/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ optimize_ai_program/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ optimizer_tracking/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ output_refinement/
    â”‚   â”‚       â”‚   â””â”€â”€ best-of-n-and-refine.md
    â”‚   â”‚       â”œâ”€â”€ papillon/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ program_of_thought/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ real_world_examples/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ rl_ai_program/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ rl_multihop/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ rl_papillon/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â”œâ”€â”€ sample_code_generation/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ saving/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ streaming/
    â”‚   â”‚       â”‚   â””â”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ tool_use/
    â”‚   â”‚       â”‚   â””â”€â”€ index.ipynb
    â”‚   â”‚       â””â”€â”€ yahoo_finance_react/
    â”‚   â”‚           â””â”€â”€ index.md
    â”‚   â”œâ”€â”€ overrides/
    â”‚   â”‚   â”œâ”€â”€ home.html
    â”‚   â”‚   â”œâ”€â”€ main.html
    â”‚   â”‚   â””â”€â”€ partials/
    â”‚   â”‚       â””â”€â”€ tabs.html
    â”‚   â””â”€â”€ scripts/
    â”‚       â”œâ”€â”€ generate_api_docs.py
    â”‚       â””â”€â”€ generate_api_summary.py
    â”œâ”€â”€ dspy/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __metadata__.py
    â”‚   â”œâ”€â”€ adapters/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ baml_adapter.py
    â”‚   â”‚   â”œâ”€â”€ base.py
    â”‚   â”‚   â”œâ”€â”€ chat_adapter.py
    â”‚   â”‚   â”œâ”€â”€ json_adapter.py
    â”‚   â”‚   â”œâ”€â”€ two_step_adapter.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ xml_adapter.py
    â”‚   â”‚   â””â”€â”€ types/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ audio.py
    â”‚   â”‚       â”œâ”€â”€ base_type.py
    â”‚   â”‚       â”œâ”€â”€ citation.py
    â”‚   â”‚       â”œâ”€â”€ code.py
    â”‚   â”‚       â”œâ”€â”€ document.py
    â”‚   â”‚       â”œâ”€â”€ file.py
    â”‚   â”‚       â”œâ”€â”€ history.py
    â”‚   â”‚       â”œâ”€â”€ image.py
    â”‚   â”‚       â”œâ”€â”€ reasoning.py
    â”‚   â”‚       â””â”€â”€ tool.py
    â”‚   â”œâ”€â”€ clients/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ base_lm.py
    â”‚   â”‚   â”œâ”€â”€ cache.py
    â”‚   â”‚   â”œâ”€â”€ databricks.py
    â”‚   â”‚   â”œâ”€â”€ embedding.py
    â”‚   â”‚   â”œâ”€â”€ lm.py
    â”‚   â”‚   â”œâ”€â”€ lm_local.py
    â”‚   â”‚   â”œâ”€â”€ openai.py
    â”‚   â”‚   â”œâ”€â”€ provider.py
    â”‚   â”‚   â””â”€â”€ utils_finetune.py
    â”‚   â”œâ”€â”€ datasets/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ colors.py
    â”‚   â”‚   â”œâ”€â”€ dataloader.py
    â”‚   â”‚   â”œâ”€â”€ dataset.py
    â”‚   â”‚   â”œâ”€â”€ gsm8k.py
    â”‚   â”‚   â”œâ”€â”€ hotpotqa.py
    â”‚   â”‚   â”œâ”€â”€ math.py
    â”‚   â”‚   â””â”€â”€ alfworld/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ alfworld.py
    â”‚   â”‚       â””â”€â”€ base_config.yml
    â”‚   â”œâ”€â”€ dsp/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ colbertv2.py
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ dpr.py
    â”‚   â”‚       â”œâ”€â”€ settings.py
    â”‚   â”‚       â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ evaluate/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ auto_evaluation.py
    â”‚   â”‚   â”œâ”€â”€ evaluate.py
    â”‚   â”‚   â””â”€â”€ metrics.py
    â”‚   â”œâ”€â”€ experimental/
    â”‚   â”‚   â””â”€â”€ __init__.py
    â”‚   â”œâ”€â”€ predict/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ aggregation.py
    â”‚   â”‚   â”œâ”€â”€ best_of_n.py
    â”‚   â”‚   â”œâ”€â”€ chain_of_thought.py
    â”‚   â”‚   â”œâ”€â”€ code_act.py
    â”‚   â”‚   â”œâ”€â”€ knn.py
    â”‚   â”‚   â”œâ”€â”€ multi_chain_comparison.py
    â”‚   â”‚   â”œâ”€â”€ parallel.py
    â”‚   â”‚   â”œâ”€â”€ parameter.py
    â”‚   â”‚   â”œâ”€â”€ predict.py
    â”‚   â”‚   â”œâ”€â”€ program_of_thought.py
    â”‚   â”‚   â”œâ”€â”€ react.py
    â”‚   â”‚   â”œâ”€â”€ refine.py
    â”‚   â”‚   â”œâ”€â”€ retry.py
    â”‚   â”‚   â”œâ”€â”€ rlm.py
    â”‚   â”‚   â””â”€â”€ avatar/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ avatar.py
    â”‚   â”‚       â”œâ”€â”€ models.py
    â”‚   â”‚       â””â”€â”€ signatures.py
    â”‚   â”œâ”€â”€ primitives/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ base_module.py
    â”‚   â”‚   â”œâ”€â”€ code_interpreter.py
    â”‚   â”‚   â”œâ”€â”€ example.py
    â”‚   â”‚   â”œâ”€â”€ module.py
    â”‚   â”‚   â”œâ”€â”€ prediction.py
    â”‚   â”‚   â”œâ”€â”€ python_interpreter.py
    â”‚   â”‚   â”œâ”€â”€ repl_types.py
    â”‚   â”‚   â””â”€â”€ runner.js
    â”‚   â”œâ”€â”€ propose/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ dataset_summary_generator.py
    â”‚   â”‚   â”œâ”€â”€ grounded_proposer.py
    â”‚   â”‚   â”œâ”€â”€ propose_base.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ retrievers/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ databricks_rm.py
    â”‚   â”‚   â”œâ”€â”€ embeddings.py
    â”‚   â”‚   â”œâ”€â”€ retrieve.py
    â”‚   â”‚   â””â”€â”€ weaviate_rm.py
    â”‚   â”œâ”€â”€ signatures/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ field.py
    â”‚   â”‚   â”œâ”€â”€ signature.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ streaming/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ messages.py
    â”‚   â”‚   â”œâ”€â”€ streamify.py
    â”‚   â”‚   â””â”€â”€ streaming_listener.py
    â”‚   â”œâ”€â”€ teleprompt/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ avatar_optimizer.py
    â”‚   â”‚   â”œâ”€â”€ bettertogether.py
    â”‚   â”‚   â”œâ”€â”€ bootstrap.py
    â”‚   â”‚   â”œâ”€â”€ bootstrap_finetune.py
    â”‚   â”‚   â”œâ”€â”€ bootstrap_trace.py
    â”‚   â”‚   â”œâ”€â”€ copro_optimizer.py
    â”‚   â”‚   â”œâ”€â”€ ensemble.py
    â”‚   â”‚   â”œâ”€â”€ grpo.py
    â”‚   â”‚   â”œâ”€â”€ infer_rules.py
    â”‚   â”‚   â”œâ”€â”€ knn_fewshot.py
    â”‚   â”‚   â”œâ”€â”€ mipro_optimizer_v2.py
    â”‚   â”‚   â”œâ”€â”€ random_search.py
    â”‚   â”‚   â”œâ”€â”€ signature_opt.py
    â”‚   â”‚   â”œâ”€â”€ simba.py
    â”‚   â”‚   â”œâ”€â”€ simba_utils.py
    â”‚   â”‚   â”œâ”€â”€ teleprompt.py
    â”‚   â”‚   â”œâ”€â”€ teleprompt_optuna.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ vanilla.py
    â”‚   â”‚   â””â”€â”€ gepa/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ gepa.py
    â”‚   â”‚       â”œâ”€â”€ gepa_utils.py
    â”‚   â”‚       â””â”€â”€ instruction_proposal.py
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ annotation.py
    â”‚       â”œâ”€â”€ asyncify.py
    â”‚       â”œâ”€â”€ caching.py
    â”‚       â”œâ”€â”€ callback.py
    â”‚       â”œâ”€â”€ dummies.py
    â”‚       â”œâ”€â”€ exceptions.py
    â”‚       â”œâ”€â”€ hasher.py
    â”‚       â”œâ”€â”€ inspect_history.py
    â”‚       â”œâ”€â”€ langchain_tool.py
    â”‚       â”œâ”€â”€ logging_utils.py
    â”‚       â”œâ”€â”€ magicattr.py
    â”‚       â”œâ”€â”€ mcp.py
    â”‚       â”œâ”€â”€ parallelizer.py
    â”‚       â”œâ”€â”€ saving.py
    â”‚       â”œâ”€â”€ syncify.py
    â”‚       â”œâ”€â”€ unbatchify.py
    â”‚       â””â”€â”€ usage_tracker.py
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ conftest.py
    â”‚   â”œâ”€â”€ mock_interpreter.py
    â”‚   â”œâ”€â”€ adapters/
    â”‚   â”‚   â”œâ”€â”€ test_adapter_utils.py
    â”‚   â”‚   â”œâ”€â”€ test_audio.py
    â”‚   â”‚   â”œâ”€â”€ test_baml_adapter.py
    â”‚   â”‚   â”œâ”€â”€ test_base_type.py
    â”‚   â”‚   â”œâ”€â”€ test_chat_adapter.py
    â”‚   â”‚   â”œâ”€â”€ test_citation.py
    â”‚   â”‚   â”œâ”€â”€ test_code.py
    â”‚   â”‚   â”œâ”€â”€ test_document.py
    â”‚   â”‚   â”œâ”€â”€ test_json_adapter.py
    â”‚   â”‚   â”œâ”€â”€ test_reasoning.py
    â”‚   â”‚   â”œâ”€â”€ test_tool.py
    â”‚   â”‚   â”œâ”€â”€ test_two_step_adapter.py
    â”‚   â”‚   â””â”€â”€ test_xml_adapter.py
    â”‚   â”œâ”€â”€ callback/
    â”‚   â”‚   â””â”€â”€ test_callback.py
    â”‚   â”œâ”€â”€ clients/
    â”‚   â”‚   â”œâ”€â”€ test_cache.py
    â”‚   â”‚   â”œâ”€â”€ test_databricks.py
    â”‚   â”‚   â”œâ”€â”€ test_embedding.py
    â”‚   â”‚   â”œâ”€â”€ test_inspect_global_history.py
    â”‚   â”‚   â”œâ”€â”€ test_lm.py
    â”‚   â”‚   â””â”€â”€ test_lm_local.py
    â”‚   â”œâ”€â”€ datasets/
    â”‚   â”‚   â””â”€â”€ test_dataset.py
    â”‚   â”œâ”€â”€ docs/
    â”‚   â”‚   â””â”€â”€ test_mkdocs_links.py
    â”‚   â”œâ”€â”€ evaluate/
    â”‚   â”‚   â”œâ”€â”€ test_evaluate.py
    â”‚   â”‚   â””â”€â”€ test_metrics.py
    â”‚   â”œâ”€â”€ examples/
    â”‚   â”‚   â””â”€â”€ test_baleen.py
    â”‚   â”œâ”€â”€ metadata/
    â”‚   â”‚   â””â”€â”€ test_metadata.py
    â”‚   â”œâ”€â”€ predict/
    â”‚   â”‚   â”œâ”€â”€ test_aggregation.py
    â”‚   â”‚   â”œâ”€â”€ test_best_of_n.py
    â”‚   â”‚   â”œâ”€â”€ test_chain_of_thought.py
    â”‚   â”‚   â”œâ”€â”€ test_code_act.py
    â”‚   â”‚   â”œâ”€â”€ test_knn.py
    â”‚   â”‚   â”œâ”€â”€ test_multi_chain_comparison.py
    â”‚   â”‚   â”œâ”€â”€ test_parallel.py
    â”‚   â”‚   â”œâ”€â”€ test_predict.py
    â”‚   â”‚   â”œâ”€â”€ test_program_of_thought.py
    â”‚   â”‚   â”œâ”€â”€ test_react.py
    â”‚   â”‚   â”œâ”€â”€ test_refine.py
    â”‚   â”‚   â”œâ”€â”€ test_retry.py
    â”‚   â”‚   â””â”€â”€ test_rlm.py
    â”‚   â”œâ”€â”€ primitives/
    â”‚   â”‚   â”œâ”€â”€ test_base_module.py
    â”‚   â”‚   â”œâ”€â”€ test_example.py
    â”‚   â”‚   â”œâ”€â”€ test_module.py
    â”‚   â”‚   â”œâ”€â”€ test_python_interpreter.py
    â”‚   â”‚   â””â”€â”€ resources/
    â”‚   â”‚       â””â”€â”€ saved_program.json
    â”‚   â”œâ”€â”€ propose/
    â”‚   â”‚   â””â”€â”€ test_grounded_proposer.py
    â”‚   â”œâ”€â”€ reliability/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ conftest.py
    â”‚   â”‚   â”œâ”€â”€ reliability_conf.yaml
    â”‚   â”‚   â”œâ”€â”€ test_generated.py
    â”‚   â”‚   â”œâ”€â”€ test_pydantic_models.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ complex_types/
    â”‚   â”‚   â”‚   â””â”€â”€ generated/
    â”‚   â”‚   â”‚       â”œâ”€â”€ test_many_types_1/
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ program.py
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ schema.json
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ inputs/
    â”‚   â”‚   â”‚       â”‚       â”œâ”€â”€ input1.json
    â”‚   â”‚   â”‚       â”‚       â””â”€â”€ input2.json
    â”‚   â”‚   â”‚       â”œâ”€â”€ test_nesting_1/
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ program.py
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ schema.json
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ inputs/
    â”‚   â”‚   â”‚       â”‚       â”œâ”€â”€ input1.json
    â”‚   â”‚   â”‚       â”‚       â””â”€â”€ input2.json
    â”‚   â”‚   â”‚       â””â”€â”€ test_nesting_2/
    â”‚   â”‚   â”‚           â”œâ”€â”€ program.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ schema.json
    â”‚   â”‚   â”‚           â””â”€â”€ inputs/
    â”‚   â”‚   â”‚               â””â”€â”€ input1.json
    â”‚   â”‚   â”œâ”€â”€ generate/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ __main__.py
    â”‚   â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”‚   â””â”€â”€ input_formats/
    â”‚   â”‚       â””â”€â”€ generated/
    â”‚   â”‚           â””â”€â”€ test_markdown_1/
    â”‚   â”‚               â”œâ”€â”€ program.py
    â”‚   â”‚               â”œâ”€â”€ schema.json
    â”‚   â”‚               â””â”€â”€ inputs/
    â”‚   â”‚                   â”œâ”€â”€ input1.json
    â”‚   â”‚                   â””â”€â”€ input2.json
    â”‚   â”œâ”€â”€ retrievers/
    â”‚   â”‚   â”œâ”€â”€ test_colbertv2.py
    â”‚   â”‚   â””â”€â”€ test_embeddings.py
    â”‚   â”œâ”€â”€ signatures/
    â”‚   â”‚   â”œâ”€â”€ test_adapter_file.py
    â”‚   â”‚   â”œâ”€â”€ test_adapter_image.py
    â”‚   â”‚   â”œâ”€â”€ test_custom_types.py
    â”‚   â”‚   â””â”€â”€ test_signature.py
    â”‚   â”œâ”€â”€ teleprompt/
    â”‚   â”‚   â”œâ”€â”€ gepa_dummy_lm.json
    â”‚   â”‚   â”œâ”€â”€ test_bootstrap.py
    â”‚   â”‚   â”œâ”€â”€ test_bootstrap_finetune.py
    â”‚   â”‚   â”œâ”€â”€ test_bootstrap_trace.py
    â”‚   â”‚   â”œâ”€â”€ test_copro_optimizer.py
    â”‚   â”‚   â”œâ”€â”€ test_ensemble.py
    â”‚   â”‚   â”œâ”€â”€ test_finetune.py
    â”‚   â”‚   â”œâ”€â”€ test_gepa.py
    â”‚   â”‚   â”œâ”€â”€ test_gepa_instruction_proposer.py
    â”‚   â”‚   â”œâ”€â”€ test_gepa_tool_optimization.py
    â”‚   â”‚   â”œâ”€â”€ test_grpo.py
    â”‚   â”‚   â”œâ”€â”€ test_knn_fewshot.py
    â”‚   â”‚   â”œâ”€â”€ test_random_search.py
    â”‚   â”‚   â”œâ”€â”€ test_teleprompt.py
    â”‚   â”‚   â””â”€â”€ test_utils.py
    â”‚   â”œâ”€â”€ test_utils/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ server/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ litellm_server.py
    â”‚   â”‚       â””â”€â”€ litellm_server_config.yaml
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ test_annotation.py
    â”‚       â”œâ”€â”€ test_asyncify.py
    â”‚       â”œâ”€â”€ test_exceptions.py
    â”‚       â”œâ”€â”€ test_langchain_tool.py
    â”‚       â”œâ”€â”€ test_magicattr.py
    â”‚       â”œâ”€â”€ test_mcp.py
    â”‚       â”œâ”€â”€ test_parallelizer.py
    â”‚       â”œâ”€â”€ test_saving.py
    â”‚       â”œâ”€â”€ test_settings.py
    â”‚       â”œâ”€â”€ test_syncify.py
    â”‚       â”œâ”€â”€ test_unbatchify.py
    â”‚       â”œâ”€â”€ test_usage_tracker.py
    â”‚       â””â”€â”€ resources/
    â”‚           â””â”€â”€ mcp_server.py
    â””â”€â”€ .github/
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ bug_report.yml
        â”‚   â””â”€â”€ feature_request.yml
        â”œâ”€â”€ PULL_REQUEST_TEMPLATE/
        â”‚   â””â”€â”€ pull_request_template.md
        â”œâ”€â”€ workflow_scripts/
        â”‚   â””â”€â”€ install_testpypi_pkg.sh
        â”œâ”€â”€ workflows/
        â”‚   â”œâ”€â”€ build_and_release.yml
        â”‚   â”œâ”€â”€ docs-push.yml
        â”‚   â”œâ”€â”€ precommits_check.yml
        â”‚   â”œâ”€â”€ run_tests.yml
        â”‚   â””â”€â”€ build_utils/
        â”‚       â””â”€â”€ test_version.py
        â””â”€â”€ .internal_dspyai/
            â”œâ”€â”€ pyproject.toml
            â””â”€â”€ internals/
                â”œâ”€â”€ build-and-release.md
                â””â”€â”€ release-checklist.md

================================================
FILE: README.md
================================================
<p align="center">
  <img align="center" src="docs/docs/static/img/dspy_logo.png" width="460px" />
</p>
<p align="left">


## DSPy: _Programming_â€”not promptingâ€”Foundation Models

**Documentation:** [DSPy Docs](https://dspy.ai/)

[![PyPI Downloads](https://static.pepy.tech/personalized-badge/dspy?period=monthly)](https://pepy.tech/projects/dspy)


----

DSPy is the framework for _programmingâ€”rather than promptingâ€”language models_. It allows you to iterate fast on **building modular AI systems** and offers algorithms for **optimizing their prompts and weights**, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.

DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional _Python code_ and use DSPy to **teach your LM to deliver high-quality outputs**. Learn more via our [official documentation site](https://dspy.ai/) or meet the community, seek help, or start contributing via this GitHub repo and our [Discord server](https://discord.gg/XCGy2WDCQB).


## Documentation: [dspy.ai](https://dspy.ai)


**Please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**


## Installation


```bash
pip install dspy
```

To install the very latest from `main`:

```bash
pip install git+https://github.com/stanfordnlp/dspy.git
````




## ðŸ“œ Citation & Reading More

If you're looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai).

If you're looking to understand the underlying research, this is a set of our papers:

**[Jul'25] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)**       
**[Jun'24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct'23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul'24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun'24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb'24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan'24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec'23] [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)   
[Dec'22] [Demonstrate-Search-Predict: Composing Retrieval & Language Models for Knowledge-Intensive NLP](https://arxiv.org/abs/2212.14024.pdf)

To stay up to date or learn more, follow [@DSPyOSS](https://twitter.com/DSPyOSS) on Twitter or the DSPy page on LinkedIn.

The **DSPy** logo is designed by **Chuyi Zhang**.

If you use DSPy or DSP in a research paper, please cite our work as follows:

```
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
```

<!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) -->




================================================
FILE: CONTRIBUTING.md
================================================
# Contribution Guide

DSPy is an actively growing project and community! We welcome your contributions and involvement. Below are instructions for how to contribute to DSPy.

## Finding an Issue

The fastest way to contribute is to find open issues that need an assignee. We maintain two lists of GitHub tags for contributors:

- [good first issue](https://github.com/stanfordnlp/dspy/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22):
  a list of small, well-defined issues for newcomers to the project.
- [help wanted](https://github.com/stanfordnlp/dspy/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22help%20wanted%22):
  a list of issues that welcome community contributions. These issues have a wide range of complexity.

We also welcome new ideas! If you would like to propose a new feature, please open a feature request to
discuss. If you already have a design in mind, please include a notebook/code example to demonstrate
your idea. Keep in mind that designing a new feature or use case may take longer than contributing to
an open issue.

## Contributing Code

Follow these steps to submit your code contribution.

### Step 1. Open an Issue

Before making any changes, we recommend opening an issue (if one doesn't already exist) and discussing your
proposed changes. This way, we can give you feedback and validate the proposed changes.

If your code change involves fixing a bug, please include a code snippet or notebook
to show how to reproduce the broken behavior.

For minor changes (simple bug fixes or documentation fixes), feel free to open a PR without discussion.

### Step 2. Make Code Changes

To make code changes, fork the repository and set up your local development environment following the
instructions in the "Environment Setup" section below.

### Step 3 Commit Your Code and Run Autoformatting

We follow the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html) and use `ruff` for both linting and formatting. To ensure consistent code quality, we use pre-commit hooks that automatically check and fix common issues.


First you need to set up the pre-commit hooks (do this once after cloning the repository):

```shell
pre-commit install
```

Then stage and commit your changes. When you run `git commit`, the pre-commit hook will be
automatically run.

```shell
git add .
git commit -m "your commit message"
```

If the hooks make any changes, you'll need to stage and commit those changes as well.

You can also run the hooks manually:

- Check staged files only:

  ```shell
  pre-commit run
  ```

- Check specific files:

  ```shell
  pre-commit run --files path/to/file1.py path/to/file2.py
  ```

Please ensure all pre-commit checks pass before creating your pull request. If you're unsure about any
formatting issues, feel free to commit your changes and let the pre-commit hooks fix them automatically.

### Step 4. Create a Pull Request

Once your changes are ready, open a pull request from your branch in your fork to the main branch in the
[DSPy repo](https://github.com/stanfordnlp/dspy).

### Step 5. Code Review

Once your PR is up and passes all CI tests, we will assign reviewers to review the code. There may be
several rounds of comments and code changes before the pull request gets approved by the reviewer.

### Step 6. Merging

Once the pull request is approved, a team member will take care of merging.

## Environment Setup

Python 3.10 or later is required.

Setting up your DSPy development environment requires you to fork the DSPy repository and clone it locally.
If you are not familiar with the GitHub fork process, please refer to [Fork a repository](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo). After creating the fork, clone
it to your local development device:

```shell
git clone {url-to-your-fork}
cd dspy
```

Next, we must set up a Python environment with the correct dependencies. There are two recommended ways to set up the
dev environment.

### [Recommended] Set Up Environment Using uv

[uv](https://github.com/astral-sh/uv) is a rust-based Python package and project manager that provides a fast
way to set up the development environment. First, install uv by following the
[installation guide](https://docs.astral.sh/uv/getting-started/installation/).

After uv is installed, in your working directory (`dspy/`), create a virtual environment using Python 3.10:

```shell
uv venv --python 3.10
```
This creates a `.venv` directory. Now, sync the environment with the development dependencies:

```shell
uv sync --extra dev
```

Then you are all set!

To verify that your environment is set up successfully, run some unit tests:

```shell
uv run pytest tests/predict
```

Note: You need to use the `uv run` prefix for every Python command, as uv creates a Python virtual
environment and `uv run` points the command to that environment. For example, to execute a Python script you will need
`uv run python script.py`.

### Set Up Environment Using conda + pip

You can also set up the virtual environment via conda + pip, which takes a few extra steps but offers more flexibility. Before starting,
make sure you have conda installed. If not, please follow the instructions
[here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).

To set up the environment, run:

```shell
conda create -n dspy-dev python=3.10
conda activate dspy-dev
pip install -e ".[dev]"
```

Then verify the installation by running some unit tests:

```shell
pytest tests/predict
```




================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2023 Stanford Future Data Systems

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=77.0.1"]
build-backend = "setuptools.build_meta"

[project]
# Do not add spaces around the '=' sign for any of the fields
# preceded by a marker comment as it affects the publish workflow.
#replace_package_name_marker
name="dspy"
#replace_package_version_marker
version="3.1.2"
description = "DSPy"
readme = "README.md"
authors = [{ name = "Omar Khattab", email = "okhattab@stanford.edu" }]
license = {file = "LICENSE"}
requires-python = ">=3.10, <3.15"
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Science/Research",
    "Operating System :: POSIX :: Linux",
    "Programming Language :: Python :: 3"
]
dependencies = [
    "openai>=0.28.1",
    "regex>=2023.10.3",
    "orjson>=3.9.0",
    "tqdm>=4.66.1",
    "requests>=2.31.0",
    "optuna>=3.4.0",
    "pydantic>=2.0",
    "litellm>=1.64.0",
    "diskcache>=5.6.0",
    "json-repair>=0.54.2",
    "tenacity>=8.2.3",
    "anyio",
    "asyncer==0.0.8",
    "cachetools>=5.5.0",
    "cloudpickle>=3.0.0",
    "numpy>=1.26.0",
    "xxhash>=3.5.0",
    "gepa[dspy]==0.0.26",
]

[project.optional-dependencies]
anthropic = ["anthropic>=0.18.0,<1.0.0"]
weaviate = ["weaviate-client~=4.5.4"]
mcp = ["mcp; python_version >= '3.10'"]
langchain = ["langchain_core"]
dev = [
    "pytest>=6.2.5",
    "pytest-mock>=3.12.0",
    "pytest-asyncio>=0.26.0",
    "ruff>=0.3.0",
    "pre-commit>=3.7.0",
    "pillow>=10.1.0",
    "datamodel_code_generator>=0.26.3",
    "build>=1.0.3",
    "litellm>=1.64.0; sys_platform == 'win32' or python_version == '3.14'",
    "litellm[proxy]>=1.64.0; sys_platform != 'win32' and python_version < '3.14'",  # Remove 3.14 condition once uvloop supports
]
test_extras = [
    "mcp; python_version >= '3.10'",
    "datasets>=2.14.6",
    "pandas>=2.1.1",
    "optuna>=3.4.0",
    "langchain_core",
]

[tool.setuptools.packages.find]
where = ["."]
include = ["dspy", "dspy.*"]
exclude = ["tests", "tests.*"]

[tool.setuptools.package-data]
dspy = ["primitives/*.js"]

[project.urls]
homepage = "https://github.com/stanfordnlp/dspy"

[tool.coverage.run]
branch = true
omit = [
    "*/__init__.py",
    "*/test_*.py",
    "*/tests/*.py",
    "*/conftest.py",
    "*/venv/*",
    "*/virtualenv/*",
    "*/.venv/*",
    "*/.virtualenv/*",
    "*/env/*",
    "*/.env/*",
    "*/setup.py",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == '__main__':",
    "logger",
    "try",
    "except",
    "^\\s*self\\.\\w+(:\\s*[^=]+)?\\s*=.*$",
    "continue",
]

[tool.pytest.ini_options]
markers = [
    "integration: tests requiring external dependencies (Deno, LM)",
]
filterwarnings = [
    # litellm uses deprecated pydantic config classes sometimes.
    # The issue has been fixed repeatedly, but still keeps showing up.
    # For examples, see litellm PRs #6903, #7300, #8096, #9372, and #12528.
    "ignore:.+class-based `config` is deprecated, use ConfigDict:DeprecationWarning",
]

[tool.ruff]
include = ["dspy/**/*.py", "tests/**/*.py"]
exclude = [
  "dspy/__metadata__.py",
  "tests/reliability/*.py",
]


line-length = 120
indent-width = 4
target-version = "py310"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C",   # flake8-comprehensions
    "B",   # flake8-bugbear
    "UP",  # pyupgrade
    "N",   # pep8-naming
    "RUF", # ruff-specific rules
    "Q",   # flake8-quotes
]

ignore = [
    "B027",  # Allow non-abstract empty methods in abstract base classes
    "FBT003",# Allow boolean positional values in function calls
    "C901",  # Ignore complexity checking
    "E501",  # Ignore line length errors (handled by formatter)
    "UP035", # Allow python typing modules
    "RUF005", # Allow using + operator to concatenate collections
    "B904", # Allow raise custom exceptions in except blocks
    "F403", # Allow wildcard imports
    "E721", # Allow using == to compare with type
    "UP031", # Allow percent format
    "RUF022", # Allow unsorted __all__ value
]

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

[tool.ruff.format]
docstring-code-format = false
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.ruff.lint.isort]
known-first-party = ["dspy"]

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all"

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = [
    "S101",    # Allow assert statements in tests
    "TID252",  # Allow relative imports in tests
    "ARG001",  # Allow unused arguments in tests (like pytest fixtures)
]
"__init__.py" = ["F401"]  # Init files can be empty
"dspy/__init__.py" = [
    "I001",  # Allow unsorted or unformatted imports (isort)
    "E402",  # Allow imports not at the top of the file (needed for certain __init__ patterns)
    "F405",  # Allow undefined names from wildcard imports (common in __init__ files)
]



================================================
FILE: .pre-commit-config.yaml
================================================
default_language_version:
  python: python3.10

default_stages: [pre-commit]
default_install_hook_types: [pre-commit]

repos:
  - repo: local
    hooks:
      - id: ruff-check
        name: ruff (lint)
        entry: ruff
        language: system
        types_or: [python, pyi]
        files: ^(dspy|tests)/.*\.py$
        exclude: ^(dspy/__metadata__\.py|tests/reliability/.*\.py)$
        args: [check, --fix-only]

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: check-yaml
        args: ["--allow-multiple-documents", "--unsafe"]
      - id: check-toml
      - id: check-added-large-files
        args: ["--maxkb=1024"]
      - id: check-merge-conflict
      - id: debug-statements



================================================
FILE: docs/README.md
================================================
**If you're looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**

&nbsp;

--------

&nbsp;

The content below is focused on how to modify the documentation site.

&nbsp;

# Modifying the DSPy Documentation


This website is built using [Material for MKDocs](https://squidfunk.github.io/mkdocs-material/), a Material UI inspired theme for MKDocs.

## Building docs locally

To build and test the documentation locally:

1. Navigate to the `docs` directory:
   ```bash
   cd docs
   ```

2. Install the necessary dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. In docs/ directory, run the command below to generate the API docs and index them:
   ```bash
   python scripts/generate_api_docs.py
   python scripts/generate_api_summary.py
   ```

4. (Optional) On MacOS you may also need to install libraries for building the site
   ```bash
   brew install cairo freetype libffi libjpeg libpng zlib
   export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib
   ```

5. Run the build command:
   ```bash
   mkdocs build
   ```

This will generate a static build of the documentation site in the `site` directory. You can then serve this directory to view the site locally using:

```bash
mkdocs serve
```

If you see the build failing make sure to fix it before pushing.

## Continuous Integration (CI) Build Checks

We have automated build checks set up in our CI pipeline to ensure the documentation builds successfully before merging changes. These checks:

1. Run the `mkdocs build` command
2. Verify that the build completes without errors
3. Help catch potential issues early in the development process

If the CI build check fails, please review your changes and ensure the documentation builds correctly locally before pushing updates.

## Contributing to the `docs` Folder

This guide is for contributors looking to make changes to the documentation in the `dspy/docs` folder. 

1. **Pull the up-to-date version of the website**: Please pull the latest version of the live documentation site via cloning the dspy repo.  The current docs are in the `dspy/docs` folder.

2. **Push your new changes on a new branch**: Feel free to add or edit existing documentation and open a PR for your changes. Once your PR is reviewed and approved, the changes will be ready to merge into main. 

3. **Updating the website**: Once your changes are merged to main, the changes would be reflected on live websites usually in 5-15 mins.

## LLMs.txt

The build process generates an `/llms.txt` file for LLM consumption using [mkdocs-llmstxt](https://github.com/pawamoy/mkdocs-llmstxt). Configure sections in `mkdocs.yml` under the `llmstxt` plugin.




================================================
FILE: docs/mkdocs.yml
================================================
site_name: DSPy
site_description: The framework for programmingâ€”rather than promptingâ€”language models.
site_url: https://dspy.ai/

repo_url: https://github.com/stanfordnlp/dspy
repo_name: stanfordnlp/dspy

edit_uri: blob/main/docs/docs/
docs_dir: "docs/"

nav:
    - Get Started: index.md
    - Learn DSPy:
        - Learning DSPy: learn/index.md
        - DSPy Programming:
            - Programming Overview: learn/programming/overview.md
            - Language Models: learn/programming/language_models.md
            - Signatures: learn/programming/signatures.md
            - Modules: learn/programming/modules.md
            - Adapters: learn/programming/adapters.md
            - Tools: learn/programming/tools.md
            - MCP: learn/programming/mcp.md
        - DSPy Evaluation:
            - Evaluation Overview: learn/evaluation/overview.md
            - Data Handling: learn/evaluation/data.md
            - Metrics: learn/evaluation/metrics.md
        - DSPy Optimization:
            - Optimization Overview: learn/optimization/overview.md
            - Optimizers: learn/optimization/optimizers.md
    - Tutorials:
        - Tutorials Overview: tutorials/index.md
        - Build AI Programs with DSPy:
            - Overview: tutorials/build_ai_program/index.md
            - Managing Conversation History: tutorials/conversation_history/index.md
            - Building AI Agents with DSPy: tutorials/customer_service_agent/index.ipynb
            - Building AI Applications by Customizing DSPy Modules: tutorials/custom_module/index.ipynb
            - Retrieval-Augmented Generation (RAG): tutorials/rag/index.ipynb
            - Building RAG as Agent: tutorials/agents/index.ipynb
            - Entity Extraction: tutorials/entity_extraction/index.ipynb
            - Classification: tutorials/classification/index.md
            - Multi-Hop RAG: tutorials/multihop_search/index.ipynb
            - Privacy-Conscious Delegation: tutorials/papillon/index.md
            - Program Of Thought: tutorials/program_of_thought/index.ipynb
            - Image Generation Prompt iteration: tutorials/image_generation_prompting/index.ipynb
            - Audio: tutorials/audio/index.ipynb
        - Optimize AI Programs with DSPy:
            - Overview: tutorials/optimize_ai_program/index.md
            - Math Reasoning: tutorials/math/index.ipynb
            - Classification Finetuning: tutorials/classification_finetuning/index.ipynb
            - Advanced Tool Use: tutorials/tool_use/index.ipynb
            - Finetuning Agents: tutorials/games/index.ipynb
        - Reflective Prompt Evolution with dspy.GEPA:
            - Overview: tutorials/gepa_ai_program/index.md
            - GEPA for AIME (Math): tutorials/gepa_aime/index.ipynb
            - GEPA for Structured Information Extraction for Enterprise Tasks: tutorials/gepa_facilitysupportanalyzer/index.ipynb
            - GEPA for Privacy-Conscious Delegation: tutorials/gepa_papillon/index.ipynb
            - GEPA for Code Backdoor Classification (AI control): tutorials/gepa_trusted_monitor/index.ipynb
        - Experimental RL Optimization for DSPy:
            - Overview: tutorials/rl_ai_program/index.md
            - RL for Privacy-Conscious Delegation: tutorials/rl_papillon/index.ipynb
            - RL for Multi-Hop Research: tutorials/rl_multihop/index.ipynb
        - Tools, Development, and Deployment:
            - Overview: tutorials/core_development/index.md
            - Use MCP in DSPy: tutorials/mcp/index.md
            - Output Refinement: tutorials/output_refinement/best-of-n-and-refine.md
            - Saving and Loading: tutorials/saving/index.md
            - Cache: tutorials/cache/index.md
            - Deployment: tutorials/deployment/index.md
            - Debugging & Observability: tutorials/observability/index.md
            - Tracking DSPy Optimizers: tutorials/optimizer_tracking/index.md
            - Streaming: tutorials/streaming/index.md
            - Async: tutorials/async/index.md
        - Real-World Examples:
            - Overview: tutorials/real_world_examples/index.md
            - Generating llms.txt: tutorials/llms_txt_generation/index.md
            - Memory-Enabled ReAct Agents: tutorials/mem0_react_agent/index.md
            - Financial Analysis with Yahoo Finance: tutorials/yahoo_finance_react/index.md
            - Email Information Extraction: tutorials/email_extraction/index.md
            - Code Generation for Unfamiliar Libraries: tutorials/sample_code_generation/index.md
            - Building a Creative Text-Based AI Game: tutorials/ai_text_game/index.md
    - DSPy in Production: production/index.md
    - Community:
        - Community Resources: community/community-resources.md
        - Use Cases: community/use-cases.md
        - Community Ports: community/community-ports.md
        - Contributing: community/how-to-contribute.md
    - FAQ:
        - FAQ: faqs.md
        - Cheatsheet: cheatsheet.md

    - API Reference:
        - API Reference: api/index.md
        - Adapters:
            - Adapter: api/adapters/Adapter.md
            - ChatAdapter: api/adapters/ChatAdapter.md
            - JSONAdapter: api/adapters/JSONAdapter.md
            - TwoStepAdapter: api/adapters/TwoStepAdapter.md
        - Evaluation:
            - CompleteAndGrounded: api/evaluation/CompleteAndGrounded.md
            - Evaluate: api/evaluation/Evaluate.md
            - EvaluationResult: api/evaluation/EvaluationResult.md
            - SemanticF1: api/evaluation/SemanticF1.md
            - answer_exact_match: api/evaluation/answer_exact_match.md
            - answer_passage_match: api/evaluation/answer_passage_match.md
        - Experimental:
            - Citations: api/experimental/Citations.md
            - Document: api/experimental/Document.md
        - Models:
            - Embedder: api/models/Embedder.md
            - LM: api/models/LM.md
        - Modules:
            - BestOfN: api/modules/BestOfN.md
            - ChainOfThought: api/modules/ChainOfThought.md
            - CodeAct: api/modules/CodeAct.md
            - Module: api/modules/Module.md
            - MultiChainComparison: api/modules/MultiChainComparison.md
            - Parallel: api/modules/Parallel.md
            - Predict: api/modules/Predict.md
            - ProgramOfThought: api/modules/ProgramOfThought.md
            - ReAct: api/modules/ReAct.md
            - Refine: api/modules/Refine.md
        - Optimizers:
            - GEPA:
                - 1. GEPA Overview: api/optimizers/GEPA/overview.md
                - 2. GEPA Advanced: api/optimizers/GEPA/GEPA_Advanced.md
            - BetterTogether: api/optimizers/BetterTogether.md
            - BootstrapFewShot: api/optimizers/BootstrapFewShot.md
            - BootstrapFewShotWithRandomSearch: api/optimizers/BootstrapFewShotWithRandomSearch.md
            - BootstrapFinetune: api/optimizers/BootstrapFinetune.md
            - BootstrapRS: api/optimizers/BootstrapRS.md
            - COPRO: api/optimizers/COPRO.md
            - Ensemble: api/optimizers/Ensemble.md
            - InferRules: api/optimizers/InferRules.md
            - KNN: api/optimizers/KNN.md
            - KNNFewShot: api/optimizers/KNNFewShot.md
            - LabeledFewShot: api/optimizers/LabeledFewShot.md
            - MIPROv2: api/optimizers/MIPROv2.md
            - SIMBA: api/optimizers/SIMBA.md
        - Primitives:
            - Audio: api/primitives/Audio.md
            - Code: api/primitives/Code.md
            - Example: api/primitives/Example.md
            - History: api/primitives/History.md
            - Image: api/primitives/Image.md
            - Prediction: api/primitives/Prediction.md
            - Tool: api/primitives/Tool.md
            - ToolCalls: api/primitives/ToolCalls.md
        - Signatures:
            - InputField: api/signatures/InputField.md
            - OutputField: api/signatures/OutputField.md
            - Signature: api/signatures/Signature.md
        - Tools:
            - ColBERTv2: api/tools/ColBERTv2.md
            - Embeddings: api/tools/Embeddings.md
            - PythonInterpreter: api/tools/PythonInterpreter.md
        - Utils:
            - StatusMessage: api/utils/StatusMessage.md
            - StatusMessageProvider: api/utils/StatusMessageProvider.md
            - StreamListener: api/utils/StreamListener.md
            - asyncify: api/utils/asyncify.md
            - configure_cache: api/utils/configure_cache.md
            - disable_litellm_logging: api/utils/disable_litellm_logging.md
            - disable_logging: api/utils/disable_logging.md
            - enable_litellm_logging: api/utils/enable_litellm_logging.md
            - enable_logging: api/utils/enable_logging.md
            - inspect_history: api/utils/inspect_history.md
            - load: api/utils/load.md
            - streamify: api/utils/streamify.md

theme:
    name: material
    custom_dir: overrides
    features:
        - navigation.tabs
        - navigation.path
        - navigation.indexes
        - navigation.expand
        - toc.follow
        - toc.integrate
        - navigation.top
        - search.suggest
        - search.highlight
        - content.tabs.link
        - content.code.annotation
        - content.code.copy
        - navigation.footer
        - content.action.edit
    language: en
    palette:
        - scheme: default
          toggle:
            icon: material/weather-night
            name: Switch to dark mode
          primary: white
          accent: black
        - scheme: slate
          toggle:
            icon: material/weather-sunny
            name: Switch to light mode
          primary: black
          accent: lime
    icon:
        repo: fontawesome/brands/git-alt
        edit: material/pencil
        view: material/eye
    logo: static/img/dspy_logo.png
    favicon: static/img/logo.png

extra_css:
    - stylesheets/extra.css

plugins:
    - social
    - search:
        lang: en
        separator: '[\s\-\.]+'
    - mkdocstrings:
        handlers:
            python:
                options:
                    docstring_style: google
                    show_source: true
                    show_root_heading: true
                    heading_level: 3
                    members_order: source
                    separate_signature: false
                    show_category_heading: true
                    show_symbol_type_heading: true
                    show_docstring_parameters: true
                    show_if_no_docstring: true
                    show_signature_annotations: true
                    unwrap_annotated: true
                    annotations_path: brief
                    docstring_section_style: table
                    merge_init_into_class: true
                    rendering:
                        show_if_no_docstring: true
                        show_warnings: false
                        html_meta: false
    - mkdocs-jupyter:
        ignore_h1_titles: true
    - redirects:
        redirect_maps:
            # Redirect /intro/ to the main page
            "intro/index.md": "index.md"
            "intro.md": "index.md"
            
            "deep-dive/optimizers/bootstrap-fewshot.md": "api/optimizers/BootstrapFewShot.md"
            "deep-dive/optimizers/bfrs.md": "api/optimizers/BootstrapFewShotWithRandomSearch.md"
            "deep-dive/optimizers/BootstrapFinetune.md": "api/optimizers/BootstrapFinetune.md"
            "deep-dive/optimizers/copro.md": "api/optimizers/COPRO.md"
            "deep-dive/optimizers/Ensemble.md": "api/optimizers/Ensemble.md"
            "deep-dive/optimizers/LabeledFewShot.md": "api/optimizers/LabeledFewShot.md"
            "deep-dive/optimizers/miprov2.md": "api/optimizers/MIPROv2.md"
            "api/optimizers/GEPA/index.md": "api/optimizers/GEPA/overview.md"

            "docs/quick-start/getting-started-01.md": "tutorials/rag/index.ipynb"
            "docs/quick-start/getting-started-02.md": "tutorials/rag/index.ipynb"
            "quick-start/getting-started-01.md": "tutorials/rag/index.ipynb"
            "quick-start/getting-started-02.md": "tutorials/rag/index.ipynb"
    - llmstxt:
        markdown_description: >
          DSPy is the framework for programmingâ€”rather than promptingâ€”language models.
          DSPy unifies techniques for prompting, fine-tuning, reasoning, tool use, and evaluation of LMs.
          It provides a systematic approach to building AI applications through composable modules, 
          optimization techniques, and evaluation frameworks.
        sections:
          Getting Started:
            - index.md: DSPy overview and quick start guide
            - cheatsheet.md: DSPy cheatsheet for quick reference
          Core Concepts:
            - learn/programming/overview.md: Programming paradigm and philosophy
            - learn/programming/signatures.md: Signatures - declarative input/output specifications
            - learn/programming/modules.md: Modules - composable AI components
            - learn/programming/language_models.md: Language model interfaces and configuration
          Essential Tutorials:
            - tutorials/rag/index.ipynb: Retrieval-Augmented Generation (RAG) tutorial
            - tutorials/classification/index.md: Classification with DSPy
            - tutorials/agents/index.ipynb: Building AI agents with DSPy
          Optimization:
            - learn/optimization/overview.md: Optimization techniques overview
            - tutorials/optimize_ai_program/index.md: Guide to optimizing AI programs
            - api/optimizers/BootstrapFewShot.md: Bootstrap few-shot optimizer
          Key Modules API:
            - api/modules/Predict.md: Basic prediction module
            - api/modules/ChainOfThought.md: Chain of thought reasoning
            - api/modules/ReAct.md: ReAct agent module
          Core API Reference:
            - api/signatures/Signature.md: Signature system documentation
            - api/primitives/Example.md: Example primitive for training data
          Production:
            - tutorials/deployment/index.md: Production deployment guide
            - tutorials/observability/index.md: Debugging and observability

extra:
    social:
        - icon: fontawesome/brands/github
          link: https://github.com/stanfordnlp/dspy
        - icon: fontawesome/brands/discord
          link: https://discord.gg/XCGy2WDCQB

extra_javascript:
    - "js/runllm-widget.js"
    - "js/tutorial-nav.js"

markdown_extensions:
    - toc:
        permalink: true
        toc_depth: 3
    - pymdownx.tabbed:
        alternate_style: true
    - pymdownx.highlight:
        anchor_linenums: true
    - pymdownx.inlinehilite
    - pymdownx.snippets
    - admonition
    - pymdownx.arithmatex:
        generic: true
    - footnotes
    - pymdownx.details
    - pymdownx.superfences
    - pymdownx.mark
    - attr_list
    - md_in_html
    - pymdownx.emoji:
        emoji_index: !!python/name:material.extensions.emoji.twemoji
        emoji_generator: !!python/name:material.extensions.emoji.to_svg

copyright: |
    &copy; 2025 <a href="https://github.com/stanfordnlp"  target="_blank" rel="noopener">DSPy</a>


================================================
FILE: docs/Pipfile
================================================
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
dspy = {git = "https://github.com/stanfordnlp/dspy.git"}
mkdocs-material = "*"
mkdocs-jupyter = "*"
"mkdocs-material[imaging]" = "*"
mkdocs-redirects = "*"
mkdocstrings = "*"
mkdocstrings-python = "*"
urllib3 = "==1.26.6"
mistune = "==3.0.2"

[requires]
python_version = "3.12"



================================================
FILE: docs/requirements.txt
================================================
git+https://github.com/stanfordnlp/dspy.git
mkdocs-material
mkdocs-jupyter
mkdocs-material[imaging]
mkdocs-redirects
mkdocstrings
mkdocstrings-python
mkdocs-llmstxt>=0.3.0
urllib3==1.26.6
mistune==3.0.2



================================================
FILE: docs/vercel.json
================================================
{
  "trailingSlash": true,
  "headers": [
    {
      "source": "/(.*).md",
      "headers": [
        {
          "key": "Content-Type",
          "value": "text/markdown; charset=utf-8"
        }
      ]
    }
  ]
}



================================================
FILE: docs/docs/cheatsheet.md
================================================
---
sidebar_position: 999
---

# DSPy Cheatsheet

This page will contain snippets for frequent usage patterns.

## DSPy Programs

### Forcing fresh LM outputs

DSPy caches LM calls. Provide a unique ``rollout_id`` and set a non-zero
``temperature`` (e.g., 1.0) to bypass an existing cache entry while still caching
the new result:

```python
predict = dspy.Predict("question -> answer")
predict(question="1+1", config={"rollout_id": 1, "temperature": 1.0})
```

### dspy.Signature

```python
class BasicQA(dspy.Signature):
    """Answer questions with short factoid answers."""

    question: str = dspy.InputField()
    answer: str = dspy.OutputField(desc="often between 1 and 5 words")
```

### dspy.ChainOfThought

```python
generate_answer = dspy.ChainOfThought(BasicQA)

# Call the predictor on a particular input alongside a hint.
question='What is the color of the sky?'
pred = generate_answer(question=question)
```

### dspy.ProgramOfThought

```python
pot = dspy.ProgramOfThought(BasicQA)

question = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'
result = pot(question=question)

print(f"Question: {question}")
print(f"Final Predicted Answer (after ProgramOfThought process): {result.answer}")
```

### dspy.ReAct

```python
react_module = dspy.ReAct(BasicQA)

question = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'
result = react_module(question=question)

print(f"Question: {question}")
print(f"Final Predicted Answer (after ReAct process): {result.answer}")
```

### dspy.Retrieve

```python
colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')
dspy.configure(rm=colbertv2_wiki17_abstracts)

#Define Retrieve Module
retriever = dspy.Retrieve(k=3)

query='When was the first FIFA World Cup held?'

# Call the retriever on a particular query.
topK_passages = retriever(query).passages

for idx, passage in enumerate(topK_passages):
    print(f'{idx+1}]', passage, '\n')
```

### dspy.CodeAct

```python
from dspy import CodeAct

def factorial(n):
    """Calculate factorial of n"""
    if n == 1:
        return 1
    return n * factorial(n-1)

act = CodeAct("n->factorial", tools=[factorial])
result = act(n=5)
result # Returns 120
```

### dspy.Parallel

```python
import dspy

parallel = dspy.Parallel(num_threads=2)
predict = dspy.Predict("question -> answer")
result = parallel(
    [
        (predict, dspy.Example(question="1+1").with_inputs("question")),
        (predict, dspy.Example(question="2+2").with_inputs("question"))
    ]
)
result
```

## DSPy Metrics

### Function as Metric

To create a custom metric you can create a function that returns either a number or a boolean value:

```python
def parse_integer_answer(answer, only_first_line=True):
    try:
        if only_first_line:
            answer = answer.strip().split('\n')[0]

        # find the last token that has a number in it
        answer = [token for token in answer.split() if any(c.isdigit() for c in token)][-1]
        answer = answer.split('.')[0]
        answer = ''.join([c for c in answer if c.isdigit()])
        answer = int(answer)

    except (ValueError, IndexError):
        # print(answer)
        answer = 0

    return answer

# Metric Function
def gsm8k_metric(gold, pred, trace=None) -> int:
    return int(parse_integer_answer(str(gold.answer))) == int(parse_integer_answer(str(pred.answer)))
```

### LLM as Judge

```python
class FactJudge(dspy.Signature):
    """Judge if the answer is factually correct based on the context."""

    context = dspy.InputField(desc="Context for the prediction")
    question = dspy.InputField(desc="Question to be answered")
    answer = dspy.InputField(desc="Answer for the question")
    factually_correct: bool = dspy.OutputField(desc="Is the answer factually correct based on the context?")

judge = dspy.ChainOfThought(FactJudge)

def factuality_metric(example, pred):
    factual = judge(context=example.context, question=example.question, answer=pred.answer)
    return factual.factually_correct
```

## DSPy Evaluation

```python
from dspy.evaluate import Evaluate

evaluate_program = Evaluate(devset=devset, metric=your_defined_metric, num_threads=NUM_THREADS, display_progress=True, display_table=num_rows_to_display)

evaluate_program(your_dspy_program)
```

## DSPy Optimizers

### LabeledFewShot

```python
from dspy.teleprompt import LabeledFewShot

labeled_fewshot_optimizer = LabeledFewShot(k=8)
your_dspy_program_compiled = labeled_fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)
```

### BootstrapFewShot

```python
from dspy.teleprompt import BootstrapFewShot

fewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=10)

your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)
```

#### Using another LM for compilation, specifying in teacher_settings

```python
from dspy.teleprompt import BootstrapFewShot

fewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=10, teacher_settings=dict(lm=gpt4))

your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)
```

#### Compiling a compiled program - bootstrapping a bootstrapped program

```python
your_dspy_program_compiledx2 = teleprompter.compile(
    your_dspy_program,
    teacher=your_dspy_program_compiled,
    trainset=trainset,
)
```

#### Saving/loading a compiled program

```python
save_path = './v1.json'
your_dspy_program_compiledx2.save(save_path)
```

```python
loaded_program = YourProgramClass()
loaded_program.load(path=save_path)
```

### BootstrapFewShotWithRandomSearch

Detailed documentation on BootstrapFewShotWithRandomSearch can be found [here](api/optimizers/BootstrapFewShot.md).

```python
from dspy.teleprompt import BootstrapFewShotWithRandomSearch

fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)

your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset, valset=devset)

```

Other custom configurations are similar to customizing the `BootstrapFewShot` optimizer.

### Ensemble

```python
from dspy.teleprompt import BootstrapFewShotWithRandomSearch
from dspy.teleprompt.ensemble import Ensemble

fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)
your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset, valset=devset)

ensemble_optimizer = Ensemble(reduce_fn=dspy.majority)
programs = [x[-1] for x in your_dspy_program_compiled.candidate_programs]
your_dspy_program_compiled_ensemble = ensemble_optimizer.compile(programs[:3])
```

### BootstrapFinetune

```python
from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune

#Compile program on current dspy.settings.lm
fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_threads=NUM_THREADS)
your_dspy_program_compiled = tp.compile(your_dspy_program, trainset=trainset[:some_num], valset=trainset[some_num:])

#Configure model to finetune
config = dict(target=model_to_finetune, epochs=2, bf16=True, bsize=6, accumsteps=2, lr=5e-5)

#Compile program on BootstrapFinetune
finetune_optimizer = BootstrapFinetune(metric=your_defined_metric)
finetune_program = finetune_optimizer.compile(your_dspy_program, trainset=some_new_dataset_for_finetuning_model, **config)

finetune_program = your_dspy_program

#Load program and activate model's parameters in program before evaluation
ckpt_path = "saved_checkpoint_path_from_finetuning"
LM = dspy.HFModel(checkpoint=ckpt_path, model=model_to_finetune)

for p in finetune_program.predictors():
    p.lm = LM
    p.activated = False
```

### COPRO

Detailed documentation on COPRO can be found [here](api/optimizers/COPRO.md).

```python
from dspy.teleprompt import COPRO

eval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)

copro_teleprompter = COPRO(prompt_model=model_to_generate_prompts, metric=your_defined_metric, breadth=num_new_prompts_generated, depth=times_to_generate_prompts, init_temperature=prompt_generation_temperature, verbose=False)

compiled_program_optimized_signature = copro_teleprompter.compile(your_dspy_program, trainset=trainset, eval_kwargs=eval_kwargs)
```

### MIPROv2

Note: detailed documentation can be found [here](api/optimizers/MIPROv2.md). `MIPROv2` is the latest extension of `MIPRO` which includes updates such as (1) improvements to instruction proposal and (2) more efficient search with minibatching.

#### Optimizing with MIPROv2

This shows how to perform an easy out-of-the box run with `auto=light`, which configures many hyperparameters for you and performs a light optimization run. You can alternatively set `auto=medium` or `auto=heavy` to perform longer optimization runs. The more detailed `MIPROv2` documentation [here](api/optimizers/MIPROv2.md) also provides more information about how to set hyperparameters by hand.

```python
# Import the optimizer
from dspy.teleprompt import MIPROv2

# Initialize optimizer
teleprompter = MIPROv2(
    metric=gsm8k_metric,
    auto="light", # Can choose between light, medium, and heavy optimization runs
)

# Optimize program
print(f"Optimizing program with MIPRO...")
optimized_program = teleprompter.compile(
    program.deepcopy(),
    trainset=trainset,
    max_bootstrapped_demos=3,
    max_labeled_demos=4,
)

# Save optimize program for future use
optimized_program.save(f"mipro_optimized")

# Evaluate optimized program
print(f"Evaluate optimized program...")
evaluate(optimized_program, devset=devset[:])
```

#### Optimizing instructions only with MIPROv2 (0-Shot)

```python
# Import the optimizer
from dspy.teleprompt import MIPROv2

# Initialize optimizer
teleprompter = MIPROv2(
    metric=gsm8k_metric,
    auto="light", # Can choose between light, medium, and heavy optimization runs
)

# Optimize program
print(f"Optimizing program with MIPRO...")
optimized_program = teleprompter.compile(
    program.deepcopy(),
    trainset=trainset,
    max_bootstrapped_demos=0,
    max_labeled_demos=0,
)

# Save optimize program for future use
optimized_program.save(f"mipro_optimized")

# Evaluate optimized program
print(f"Evaluate optimized program...")
evaluate(optimized_program, devset=devset[:])
```

### KNNFewShot

```python
from sentence_transformers import SentenceTransformer
from dspy import Embedder
from dspy.teleprompt import KNNFewShot
from dspy import ChainOfThought

knn_optimizer = KNNFewShot(k=3, trainset=trainset, vectorizer=Embedder(SentenceTransformer("all-MiniLM-L6-v2").encode))

qa_compiled = knn_optimizer.compile(student=ChainOfThought("question -> answer"))
```

### BootstrapFewShotWithOptuna

```python
from dspy.teleprompt import BootstrapFewShotWithOptuna

fewshot_optuna_optimizer = BootstrapFewShotWithOptuna(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)

your_dspy_program_compiled = fewshot_optuna_optimizer.compile(student=your_dspy_program, trainset=trainset, valset=devset)
```

Other custom configurations are similar to customizing the `dspy.BootstrapFewShot` optimizer.


### SIMBA

SIMBA, which stands for Stochastic Introspective Mini-Batch Ascent, is a prompt optimizer that accepts arbitrary DSPy programs and proceeds in a sequence of mini-batches seeking to make incremental improvements to the prompt instructions or few-shot examples.

```python
from dspy.teleprompt import SIMBA

simba = SIMBA(metric=your_defined_metric, max_steps=12, max_demos=10)

optimized_program = simba.compile(student=your_dspy_program, trainset=trainset)
```


## DSPy Tools and Utilities

### dspy.Tool

```python
import dspy

def search_web(query: str) -> str:
    """Search the web for information"""
    return f"Search results for: {query}"

tool = dspy.Tool(search_web)
result = tool(query="Python programming")
```

### dspy.streamify

```python
import dspy
import asyncio

predict = dspy.Predict("question->answer")

stream_predict = dspy.streamify(
    predict,
    stream_listeners=[dspy.streaming.StreamListener(signature_field_name="answer")],
)

async def read_output_stream():
    output_stream = stream_predict(question="Why did a chicken cross the kitchen?")

    async for chunk in output_stream:
        print(chunk)

asyncio.run(read_output_stream())
```


### dspy.asyncify

```python
import dspy

dspy_program = dspy.ChainOfThought("question -> answer")
dspy_program = dspy.asyncify(dspy_program)

asyncio.run(dspy_program(question="What is DSPy"))
```


### Track Usage

```python
import dspy
dspy.configure(track_usage=True)

result = dspy.ChainOfThought(BasicQA)(question="What is 2+2?")
print(f"Token usage: {result.get_lm_usage()}")
```

### dspy.configure_cache

```python
import dspy

# Configure cache settings
dspy.configure_cache(
    enable_disk_cache=False,
    enable_memory_cache=False,
)
```

## DSPy `Refine` and `BestofN`

>`dspy.Suggest` and `dspy.Assert` are replaced by `dspy.Refine` and `dspy.BestofN` in DSPy 2.6.

### BestofN

Runs a module up to `N` times with different rollout IDs (bypassing cache) and returns the best prediction, as defined by the `reward_fn`, or the first prediction that passes the `threshold`.

```python
import dspy

qa = dspy.ChainOfThought("question -> answer")
def one_word_answer(args, pred):
    return 1.0 if len(pred.answer) == 1 else 0.0
best_of_3 = dspy.BestOfN(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0)
best_of_3(question="What is the capital of Belgium?").answer
# Brussels
```

### Refine

Refines a module by running it up to `N` times with different rollout IDs (bypassing cache) and returns the best prediction, as defined by the `reward_fn`, or the first prediction that passes the `threshold`. After each attempt (except the final one), `Refine` automatically generates detailed feedback about the module's performance and uses this feedback as hints for subsequent runs, creating an iterative refinement process.

```python
import dspy

qa = dspy.ChainOfThought("question -> answer")
def one_word_answer(args, pred):
    return 1.0 if len(pred.answer) == 1 else 0.0
best_of_3 = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0)
best_of_3(question="What is the capital of Belgium?").answer
# Brussels
```

#### Error Handling

By default, `Refine` will try to run the module up to N times until the threshold is met. If the module encounters an error, it will keep going up to N failed attempts. You can change this behavior by setting `fail_count` to a smaller number than `N`.

```python
refine = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0, fail_count=1)
...
refine(question="What is the capital of Belgium?")
# If we encounter just one failed attempt, the module will raise an error.
```

If you want to run the module up to N times without any error handling, you can set `fail_count` to `N`. This is the default behavior.

```python
refine = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0, fail_count=3)
...
refine(question="What is the capital of Belgium?")
```



================================================
FILE: docs/docs/faqs.md
================================================
---
sidebar_position: 998
---

!!! warning "This page is outdated and may not be fully accurate in DSPy 2.5 and 2.6"


# FAQs

## Is DSPy right for me? DSPy vs. other frameworks

The **DSPy** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.

**DSPy vs. thin wrappers for prompts (OpenAI API, MiniChain, basic templating)** In other words: _Why can't I just write my prompts directly as string templates?_ Well, for extremely simple settings, this _might_ work just fine. (If you're familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.) However, when you need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components. **DSPy** cleanly abstracts away (_and_ powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the _same program_ expressed in 10 or 20 lines of **DSPy** can easily be compiled into multi-stage instructions for `GPT-4`, detailed prompts for `Llama2-13b`, or finetunes for `T5-base`. Oh, and you wouldn't need to maintain long, brittle, model-specific strings at the core of your project anymore.

**DSPy vs. application development libraries like LangChain, LlamaIndex** LangChain and LlamaIndex target high-level application development; they offer _batteries-included_, pre-built application modules that plug in with your data or configuration. If you'd be happy to use a generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL, you will find a rich ecosystem in these libraries. **DSPy** doesn't internally contain hand-crafted prompts that target specific applications. Instead, **DSPy** introduces a small set of much more powerful and general-purpose modules _that can learn to prompt (or finetune) your LM within your pipeline on your data_. when you change your data, make tweaks to your program's control flow, or change your target LM, the **DSPy compiler** can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that **DSPy** obtains the highest quality for your task, with the least effort, provided you're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model â€” not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).

**DSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines** These are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression. This is very useful in many settings, but it's generally focused on low-level, structured control of a single LM call. It doesn't help ensure the JSON (or structured output) you get is going to be correct or useful for your task. In contrast, **DSPy** automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured outputs. That said, we are considering allowing **Signatures** in **DSPy** to express regex-like constraints that are implemented by these libraries.

## Basic Usage

**How should I use DSPy for my task?** We wrote a [eight-step guide](learn/index.md) on this. In short, using DSPy is an iterative process. You first define your task and the metrics you want to maximize, and prepare a few example inputs â€” typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers (`modules`) to use, giving each layer a `signature` (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy `optimizer` to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.

**How do I convert my complex prompt into a DSPy pipeline?** See the same answer above.

**What do DSPy optimizers tune?** Or, _what does compiling actually do?_ Each optimizer is different, but they all seek to maximize a metric on your program by updating prompts or LM weights. Current DSPy `optimizers` can inspect your data, simulate traces through your program to generate good/bad examples of each step, propose or refine instructions for each step based on past results, finetune the weights of your LM on self-generated examples, or combine several of these to improve quality or cut cost. We'd love to merge new optimizers that explore a richer space: most manual steps you currently go through for prompt engineering, "synthetic data" generation, or self-improvement can probably generalized into a DSPy optimizer that acts on arbitrary LM programs.

Other FAQs. We welcome PRs to add formal answers to each of these here. You will find the answer in existing issues, tutorials, or the papers for all or most of these.

- **How do I get multiple outputs?**

You can specify multiple output fields. For the short-form signature, you can list multiple outputs as comma separated values, following the "->" indicator (e.g. "inputs -> output1, output2"). For the long-form signature, you can include multiple `dspy.OutputField`s.


- **How do I define my own metrics? Can metrics return a float?**

You can define metrics as simply Python functions that process model generations and evaluate them based on user-defined requirements. Metrics can compare existent data (e.g. gold labels) to model predictions or they can be used to assess various components of an output using validation feedback from LMs (e.g. LLMs-as-Judges). Metrics can return `bool`, `int`, and `float` types scores. Check out the official [Metrics docs](learn/evaluation/metrics.md) to learn more about defining custom metrics and advanced evaluations using AI feedback and/or DSPy programs.

- **How expensive or slow is compiling??**

To reflect compiling metrics, we highlight an experiment for reference, compiling a program using the [BootstrapFewShotWithRandomSearch](api/optimizers/BootstrapFewShotWithRandomSearch.md) optimizer on the `gpt-3.5-turbo-1106` model over 7 candidate programs and 10 threads. We report that compiling this program takes around 6 minutes with 3200 API calls, 2.7 million input tokens and 156,000 output tokens, reporting a total cost of $3 USD (at the current pricing of the OpenAI model).

Compiling DSPy `optimizers` naturally will incur additional LM calls, but we substantiate this overhead with minimalistic executions with the goal of maximizing performance. This invites avenues to enhance performance of smaller models by compiling DSPy programs with larger models to learn enhanced behavior during compile-time and propagate such behavior to the tested smaller model during inference-time.  


## Deployment or Reproducibility Concerns

- **How do I save a checkpoint of my compiled program?**

Here is an example of saving/loading a compiled module:

```python
cot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)

#Saving
cot_compiled.save('compiled_cot_gsm8k.json')

#Loading:
cot = CoT()
cot.load('compiled_cot_gsm8k.json')
```

- **How do I export for deployment?**

Exporting DSPy programs is simply saving them as highlighted above!

- **How do I search my own data?**

Open source libraries such as [RAGautouille](https://github.com/bclavie/ragatouille) enable you to search for your own data through advanced retrieval models like ColBERT with tools to embed and index documents. Feel free to integrate such libraries to create searchable datasets while developing your DSPy programs!

- **How do I turn off the cache? How do I export the cache?**

From v2.5, you can turn off the cache by setting `cache` parameter in `dspy.LM` to `False`:

```python
dspy.LM('openai/gpt-4o-mini',  cache=False)
```

Your local cache will be saved to the global env directory `os.environ["DSP_CACHEDIR"]` or for notebooks `os.environ["DSP_NOTEBOOK_CACHEDIR"]`. You can usually set the cachedir to `os.path.join(repo_path, 'cache')` and export this cache from here:
```python
os.environ["DSP_NOTEBOOK_CACHEDIR"] = os.path.join(os.getcwd(), 'cache')
```

!!! warning "Important"
    `DSP_CACHEDIR` is responsible for old clients (including dspy.OpenAI, dspy.ColBERTv2, etc.) and `DSPY_CACHEDIR` is responsible for the new dspy.LM client.

    In the AWS lambda deployment, you should disable both DSP_\* and DSPY_\*.


## Advanced Usage

- **How do I parallelize?**
You can parallelize DSPy programs during both compilation and evaluation by specifying multiple thread settings in the respective DSPy `optimizers` or within the `dspy.Evaluate` utility function.

- **How do freeze a module?**

Modules can be frozen by setting their `._compiled` attribute to be True, indicating the module has gone through optimizer compilation and should not have its parameters adjusted. This is handled internally in optimizers such as `dspy.BootstrapFewShot` where the student program is ensured to be frozen before the teacher propagates the gathered few-shot demonstrations in the bootstrapping process. 

- **How do I use DSPy assertions?**

    a) **How to Add Assertions to Your Program**:
    - **Define Constraints**: Use `dspy.Assert` and/or `dspy.Suggest` to define constraints within your DSPy program. These are based on boolean validation checks for the outcomes you want to enforce, which can simply be Python functions to validate the model outputs.
    - **Integrating Assertions**: Keep your Assertion statements following a model generations (hint: following a module layer)

    b) **How to Activate the Assertions**:
    1. **Using `assert_transform_module`**:
        - Wrap your DSPy module with assertions using the `assert_transform_module` function, along with a `backtrack_handler`. This function transforms your program to include internal assertions backtracking and retry logic, which can be customized as well:
        `program_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)`
    2. **Activate Assertions**:
        - Directly call `activate_assertions` on your DSPy program with assertions: `program_with_assertions = ProgramWithAssertions().activate_assertions()`

    **Note**: To use Assertions properly, you must **activate** a DSPy program that includes `dspy.Assert` or `dspy.Suggest` statements from either of the methods above. 

## Errors

- **How do I deal with "context too long" errors?**

If you're dealing with "context too long" errors in DSPy, you're likely using DSPy optimizers to include demonstrations within your prompt, and this is exceeding your current context window. Try reducing these parameters (e.g. `max_bootstrapped_demos` and `max_labeled_demos`). Additionally, you can also reduce the number of retrieved passages/docs/embeddings to ensure your prompt is fitting within your model context length.

A more general fix is simply increasing the number of `max_tokens` specified to the LM request (e.g. `lm = dspy.OpenAI(model = ..., max_tokens = ...`).

## Set Verbose Level
DSPy utilizes the [logging library](https://docs.python.org/3/library/logging.html) to print logs. If you want to debug your DSPy code, set the logging level to DEBUG with the example code below.

```python
import logging
logging.getLogger("dspy").setLevel(logging.DEBUG)
```

Alternatively, if you want to reduce the amount of logs, set the logging level to WARNING or ERROR.

```python
import logging
logging.getLogger("dspy").setLevel(logging.WARNING)
```


================================================
FILE: docs/docs/index.md
================================================
---
sidebar_position: 1
hide:
  - navigation
  - toc

---

![DSPy](static/img/dspy_logo.png){ width="200", align=left }

# _Programming_â€”not promptingâ€”_LMs_

[![PyPI Downloads](https://static.pepy.tech/personalized-badge/dspy?period=monthly)](https://pepy.tech/projects/dspy)

DSPy is a declarative framework for building modular AI software. It allows you to **iterate fast on structured code**, rather than brittle strings, and offers algorithms that **compile AI programs into effective prompts and weights** for your language models, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.

Instead of wrangling prompts or training jobs, DSPy (Declarative Self-improving Python) enables you to **build AI software from natural-language modules** and to _generically compose them_ with different models, inference strategies, or learning algorithms. This makes AI software **more reliable, maintainable, and portable** across models and strategies.

*tl;dr* Think of DSPy as a higher-level language for AI programming ([lecture](https://www.youtube.com/watch?v=JEMYuzrKLUw)), like the shift from assembly to C or pointer arithmetic to SQL. Meet the community, seek help, or start contributing via [GitHub](https://github.com/stanfordnlp/dspy) and [Discord](https://discord.gg/XCGy2WDCQB).

<!-- Its abstractions make your AI software more reliable and maintainable, and allow it to become more portable as new models and learning techniques emerge. It's also just rather elegant! -->

!!! info "Getting Started I: Install DSPy and set up your LM"

    ```bash
    > pip install -U dspy
    ```

    === "OpenAI"
        You can authenticate by setting the `OPENAI_API_KEY` env variable or passing `api_key` below.

        ```python linenums="1"
        import dspy
        lm = dspy.LM("openai/gpt-5-mini", api_key="YOUR_OPENAI_API_KEY")
        dspy.configure(lm=lm)
        ```

    === "Anthropic"
        You can authenticate by setting the `ANTHROPIC_API_KEY` env variable or passing `api_key` below.

        ```python linenums="1"
        import dspy
        lm = dspy.LM("anthropic/claude-sonnet-4-5-20250929", api_key="YOUR_ANTHROPIC_API_KEY")
        dspy.configure(lm=lm)
        ```

    === "Databricks"
        If you're on the Databricks platform, authentication is automatic via their SDK. If not, you can set the env variables `DATABRICKS_API_KEY` and `DATABRICKS_API_BASE`, or pass `api_key` and `api_base` below.

        ```python linenums="1"
        import dspy
        lm = dspy.LM(
            "databricks/databricks-llama-4-maverick",
            api_key="YOUR_DATABRICKS_ACCESS_TOKEN",
            api_base="YOUR_DATABRICKS_WORKSPACE_URL",  # e.g.: https://dbc-64bf4923-e39e.cloud.databricks.com/serving-endpoints
        )
        dspy.configure(lm=lm)
        ```

    === "Gemini"
        You can authenticate by setting the `GEMINI_API_KEY` env variable or passing `api_key` below.

        ```python linenums="1"
        import dspy
        lm = dspy.LM("gemini/gemini-2.5-flash", api_key="YOUR_GEMINI_API_KEY")
        dspy.configure(lm=lm)
        ```

    === "Local LMs on your laptop"
          First, install [Ollama](https://github.com/ollama/ollama) and launch its server with your LM.

          ```bash
          > curl -fsSL https://ollama.ai/install.sh | sh
          > ollama run llama3.2:1b
          ```

          Then, connect to it from your DSPy code.

        ```python linenums="1"
        import dspy
        lm = dspy.LM("ollama_chat/llama3.2:1b", api_base="http://localhost:11434", api_key="")
        dspy.configure(lm=lm)
        ```

    === "Local LMs on a GPU server"
          First, install [SGLang](https://docs.sglang.ai/get_started/install.html) and launch its server with your LM.

          ```bash
          > pip install "sglang[all]"
          > pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ 

          > CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct
          ```
        
        If you don't have access from Meta to download `meta-llama/Llama-3.1-8B-Instruct`, use `Qwen/Qwen2.5-7B-Instruct` for example.

        Next, connect to your local LM from your DSPy code as an `OpenAI`-compatible endpoint.

          ```python linenums="1"
          lm = dspy.LM("openai/meta-llama/Llama-3.1-8B-Instruct",
                       api_base="http://localhost:7501/v1",  # ensure this points to your port
                       api_key="local", model_type="chat")
          dspy.configure(lm=lm)
          ```

    === "Other providers"
        In DSPy, you can use any of the dozens of [LLM providers supported by LiteLLM](https://docs.litellm.ai/docs/providers). Simply follow their instructions for which `{PROVIDER}_API_KEY` to set and how to write pass the `{provider_name}/{model_name}` to the constructor.

        Some examples:

        - `anyscale/mistralai/Mistral-7B-Instruct-v0.1`, with `ANYSCALE_API_KEY`
        - `together_ai/togethercomputer/llama-2-70b-chat`, with `TOGETHERAI_API_KEY`
        - `sagemaker/<your-endpoint-name>`, with `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION_NAME`
        - `azure/<your_deployment_name>`, with `AZURE_API_KEY`, `AZURE_API_BASE`, `AZURE_API_VERSION`, and the optional `AZURE_AD_TOKEN` and `AZURE_API_TYPE`

        
        If your provider offers an OpenAI-compatible endpoint, just add an `openai/` prefix to your full model name.

        ```python linenums="1"
        import dspy
        lm = dspy.LM("openai/your-model-name", api_key="PROVIDER_API_KEY", api_base="YOUR_PROVIDER_URL")
        dspy.configure(lm=lm)
        ```

??? "Calling the LM directly."

     Idiomatic DSPy involves using _modules_, which we define in the rest of this page. However, it's still easy to call the `lm` you configured above directly. This gives you a unified API and lets you benefit from utilities like automatic caching.

     ```python linenums="1"       
     lm("Say this is a test!", temperature=0.7)  # => ['This is a test!']
     lm(messages=[{"role": "user", "content": "Say this is a test!"}])  # => ['This is a test!']
     ``` 


## 1) **Modules** help you describe AI behavior as _code_, not strings.

To build reliable AI systems, you must iterate fast. But maintaining prompts makes that hard: it forces you to tinker with strings or data _every time you change your LM, metrics, or pipeline_. Having built over a dozen best-in-class compound LM systems since 2020, we learned this the hard wayâ€”and so built DSPy to decouple AI system design from messy incidental choices about specific LMs or prompting strategies.

DSPy shifts your focus from tinkering with prompt strings to **programming with structured and declarative natural-language modules**. For every AI component in your system, you specify input/output behavior as a _signature_ and select a _module_ to assign a strategy for invoking your LM. DSPy expands your signatures into prompts and parses your typed outputs, so you can compose different modules together into ergonomic, portable, and optimizable AI systems.


!!! info "Getting Started II: Build DSPy modules for various tasks"
    Try the examples below after configuring your `lm` above. Adjust the fields to explore what tasks your LM can do well out of the box. Each tab below sets up a DSPy module, like `dspy.Predict`, `dspy.ChainOfThought`, or `dspy.ReAct`, with a task-specific _signature_. For example, `question -> answer: float` tells the module to take a question and to produce a `float` answer.

    === "Math"

        ```python linenums="1"
        math = dspy.ChainOfThought("question -> answer: float")
        math(question="Two dice are tossed. What is the probability that the sum equals two?")
        ```
        
        **Possible Output:**
        ```text
        Prediction(
            reasoning='When two dice are tossed, each die has 6 faces, resulting in a total of 6 x 6 = 36 possible outcomes. The sum of the numbers on the two dice equals two only when both dice show a 1. This is just one specific outcome: (1, 1). Therefore, there is only 1 favorable outcome. The probability of the sum being two is the number of favorable outcomes divided by the total number of possible outcomes, which is 1/36.',
            answer=0.0277776
        )
        ```

    === "RAG"

        ```python linenums="1"       
        def search_wikipedia(query: str) -> list[str]:
            results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
            return [x["text"] for x in results]
        
        rag = dspy.ChainOfThought("context, question -> response")

        question = "What's the name of the castle that David Gregory inherited?"
        rag(context=search_wikipedia(question), question=question)
        ```
        
        **Possible Output:**
        ```text
        Prediction(
            reasoning='The context provides information about David Gregory, a Scottish physician and inventor. It specifically mentions that he inherited Kinnairdy Castle in 1664. This detail directly answers the question about the name of the castle that David Gregory inherited.',
            response='Kinnairdy Castle'
        )
        ```

    === "Classification"

        ```python linenums="1"
        from typing import Literal

        class Classify(dspy.Signature):
            """Classify sentiment of a given sentence."""
            
            sentence: str = dspy.InputField()
            sentiment: Literal["positive", "negative", "neutral"] = dspy.OutputField()
            confidence: float = dspy.OutputField()

        classify = dspy.Predict(Classify)
        classify(sentence="This book was super fun to read, though not the last chapter.")
        ```
        
        **Possible Output:**

        ```text
        Prediction(
            sentiment='positive',
            confidence=0.75
        )
        ```

    === "Information Extraction"

        ```python linenums="1"        
        class ExtractInfo(dspy.Signature):
            """Extract structured information from text."""
            
            text: str = dspy.InputField()
            title: str = dspy.OutputField()
            headings: list[str] = dspy.OutputField()
            entities: list[dict[str, str]] = dspy.OutputField(desc="a list of entities and their metadata")
        
        module = dspy.Predict(ExtractInfo)

        text = "Apple Inc. announced its latest iPhone 14 today." \
            "The CEO, Tim Cook, highlighted its new features in a press release."
        response = module(text=text)

        print(response.title)
        print(response.headings)
        print(response.entities)
        ```
        
        **Possible Output:**
        ```text
        Apple Inc. Announces iPhone 14
        ['Introduction', "CEO's Statement", 'New Features']
        [{'name': 'Apple Inc.', 'type': 'Organization'}, {'name': 'iPhone 14', 'type': 'Product'}, {'name': 'Tim Cook', 'type': 'Person'}]
        ```

    === "Agents"

        ```python linenums="1"       
        def evaluate_math(expression: str):
            return dspy.PythonInterpreter({}).execute(expression)

        def search_wikipedia(query: str):
            results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
            return [x["text"] for x in results]

        react = dspy.ReAct("question -> answer: float", tools=[evaluate_math, search_wikipedia])

        pred = react(question="What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?")
        print(pred.answer)
        ```
        
        **Possible Output:**

        ```text
        5761.328
        ```
    
    === "Multi-Stage Pipelines"

        ```python linenums="1"       
        class Outline(dspy.Signature):
            """Outline a thorough overview of a topic."""
            
            topic: str = dspy.InputField()
            title: str = dspy.OutputField()
            sections: list[str] = dspy.OutputField()
            section_subheadings: dict[str, list[str]] = dspy.OutputField(desc="mapping from section headings to subheadings")

        class DraftSection(dspy.Signature):
            """Draft a top-level section of an article."""
            
            topic: str = dspy.InputField()
            section_heading: str = dspy.InputField()
            section_subheadings: list[str] = dspy.InputField()
            content: str = dspy.OutputField(desc="markdown-formatted section")

        class DraftArticle(dspy.Module):
            def __init__(self):
                self.build_outline = dspy.ChainOfThought(Outline)
                self.draft_section = dspy.ChainOfThought(DraftSection)

            def forward(self, topic):
                outline = self.build_outline(topic=topic)
                sections = []
                for heading, subheadings in outline.section_subheadings.items():
                    section, subheadings = f"## {heading}", [f"### {subheading}" for subheading in subheadings]
                    section = self.draft_section(topic=outline.title, section_heading=section, section_subheadings=subheadings)
                    sections.append(section.content)
                return dspy.Prediction(title=outline.title, sections=sections)

        draft_article = DraftArticle()
        article = draft_article(topic="World Cup 2002")
        ```
        
        **Possible Output:**

        A 1500-word article on the topic, e.g.

        ```text
        ## Qualification Process

        The qualification process for the 2002 FIFA World Cup involved a series of..... [shortened here for presentation].

        ### UEFA Qualifiers

        The UEFA qualifiers involved 50 teams competing for 13..... [shortened here for presentation].

        .... [rest of the article]
        ```

        Note that DSPy makes it straightforward to optimize multi-stage modules like this. As long as you can evaluate the _final_ output of the system, every DSPy optimizer can tune all of the intermediate modules.

??? "Using DSPy in practice: from quick scripting to building sophisticated systems."

    Standard prompts conflate interface ("what should the LM do?") with implementation ("how do we tell it to do that?"). DSPy isolates the former as _signatures_ so we can infer the latter or learn it from data â€” in the context of a bigger program.
    
    Even before you start using optimizers, DSPy's modules allow you to script effective LM systems as ergonomic, portable _code_. Across many tasks and LMs, we maintain _signature test suites_ that assess the reliability of the built-in DSPy adapters. Adapters are the components that map signatures to prompts prior to optimization. If you find a task where a simple prompt consistently outperforms idiomatic DSPy for your LM, consider that a bug and [file an issue](https://github.com/stanfordnlp/dspy/issues). We'll use this to improve the built-in adapters.


## 2) **Optimizers** tune the prompts and weights of your AI modules.

DSPy provides you with the tools to compile high-level code with natural language annotations into the low-level computations, prompts, or weight updates that align your LM with your program's structure and metrics. If you change your code or your metrics, you can simply re-compile accordingly.

Given a few tens or hundreds of representative _inputs_ of your task and a _metric_ that can measure the quality of your system's outputs, you can use a DSPy optimizer. Different optimizers in DSPy work by **synthesizing good few-shot examples** for every module, like `dspy.BootstrapRS`,<sup>[1](https://arxiv.org/abs/2310.03714)</sup> **proposing and intelligently exploring better natural-language instructions** for every prompt, like [`dspy.GEPA`](https://dspy.ai/tutorials/gepa_ai_program/)<sup>[2](https://arxiv.org/abs/2507.19457)</sup>, `dspy.MIPROv2`,<sup>[3](https://arxiv.org/abs/2406.11695)</sup> and **building datasets for your modules and using them to finetune the LM weights** in your system, like `dspy.BootstrapFinetune`.<sup>[4](https://arxiv.org/abs/2407.10930)</sup> For detailed tutorials on running `dspy.GEPA`, please take a look at [dspy.GEPA tutorials](https://dspy.ai/tutorials/gepa_ai_program/).


!!! info "Getting Started III: Optimizing the LM prompts or weights in DSPy programs"
    A typical simple optimization run costs on the order of $2 USD and takes around 20 minutes, but be careful when running optimizers with very large LMs or very large datasets.
    Optimization can cost as little as a few cents or up to tens of dollars, depending on your LM, dataset, and configuration.

    Examples below rely on HuggingFace/datasets, you can install it by the command below.

    ```bash
    > pip install -U datasets
    ```

    === "Optimizing prompts for a ReAct agent"
        This is a minimal but fully runnable example of setting up a `dspy.ReAct` agent that answers questions via
        search from Wikipedia and then optimizing it using `dspy.MIPROv2` in the cheap `light` mode on 500
        question-answer pairs sampled from the `HotPotQA` dataset.

        ```python linenums="1"
        import dspy
        from dspy.datasets import HotPotQA

        dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

        def search_wikipedia(query: str) -> list[str]:
            results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
            return [x["text"] for x in results]

        trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train]
        react = dspy.ReAct("question -> answer", tools=[search_wikipedia])

        tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto="light", num_threads=24)
        optimized_react = tp.compile(react, trainset=trainset)
        ```

        An informal run like this raises ReAct's score from 24% to 51%, by teaching `gpt-4o-mini` more about the specifics of the task.

    === "Optimizing prompts for RAG"
        Given a retrieval index to `search`, your favorite `dspy.LM`, and a small `trainset` of questions and ground-truth responses, the following code snippet can optimize your RAG system with long outputs against the built-in `SemanticF1` metric, which is implemented as a DSPy module.

        ```python linenums="1"
        class RAG(dspy.Module):
            def __init__(self, num_docs=5):
                self.num_docs = num_docs
                self.respond = dspy.ChainOfThought("context, question -> response")

            def forward(self, question):
                context = search(question, k=self.num_docs)   # defined in tutorial linked below
                return self.respond(context=context, question=question)

        tp = dspy.MIPROv2(metric=dspy.evaluate.SemanticF1(decompositional=True), auto="medium", num_threads=24)
        optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2)
        ```

        For a complete RAG example that you can run, start this [tutorial](tutorials/rag/index.ipynb). It improves the quality of a RAG system over a subset of StackExchange communities by 10% relative gain.

    === "Optimizing weights for Classification"
        <details><summary>Click to show dataset setup code.</summary>

        ```python linenums="1"
        import random
        from typing import Literal

        from datasets import load_dataset

        import dspy
        from dspy.datasets import DataLoader

        # Load the Banking77 dataset.
        CLASSES = load_dataset("PolyAI/banking77", split="train", trust_remote_code=True).features["label"].names
        kwargs = {"fields": ("text", "label"), "input_keys": ("text",), "split": "train", "trust_remote_code": True}

        # Load the first 2000 examples from the dataset, and assign a hint to each *training* example.
        trainset = [
            dspy.Example(x, hint=CLASSES[x.label], label=CLASSES[x.label]).with_inputs("text", "hint")
            for x in DataLoader().from_huggingface(dataset_name="PolyAI/banking77", **kwargs)[:2000]
        ]
        random.Random(0).shuffle(trainset)
        ```
        </details>

        ```python linenums="1"
        import dspy
        lm=dspy.LM('openai/gpt-4o-mini-2024-07-18')

        # Define the DSPy module for classification. It will use the hint at training time, if available.
        signature = dspy.Signature("text, hint -> label").with_updated_fields("label", type_=Literal[tuple(CLASSES)])
        classify = dspy.ChainOfThought(signature)
        classify.set_lm(lm)

        # Optimize via BootstrapFinetune.
        optimizer = dspy.BootstrapFinetune(metric=(lambda x, y, trace=None: x.label == y.label), num_threads=24)
        optimized = optimizer.compile(classify, trainset=trainset)

        optimized(text="What does a pending cash withdrawal mean?")
        
        # For a complete fine-tuning tutorial, see: https://dspy.ai/tutorials/classification_finetuning/
        ```

        **Possible Output (from the last line):**
        ```text
        Prediction(
            reasoning='A pending cash withdrawal indicates that a request to withdraw cash has been initiated but has not yet been completed or processed. This status means that the transaction is still in progress and the funds have not yet been deducted from the account or made available to the user.',
            label='pending_cash_withdrawal'
        )
        ```

        An informal run similar to this on DSPy 2.5.29 raises GPT-4o-mini's score 66% to 87%.


??? "What's an example of a DSPy optimizer? How do different optimizers work?"

    Take the `dspy.MIPROv2` optimizer as an example. First, MIPRO starts with the **bootstrapping stage**. It takes your program, which may be unoptimized at this point, and runs it many times across different inputs to collect traces of input/output behavior for each one of your modules. It filters these traces to keep only those that appear in trajectories scored highly by your metric. Second, MIPRO enters its **grounded proposal stage**. It previews your DSPy program's code, your data, and traces from running your program, and uses them to draft many potential instructions for every prompt in your program. Third, MIPRO launches the **discrete search stage**. It samples mini-batches from your training set, proposes a combination of instructions and traces to use for constructing every prompt in the pipeline, and evaluates the candidate program on the mini-batch. Using the resulting score, MIPRO updates a surrogate model that helps the proposals get better over time.

    One thing that makes DSPy optimizers so powerful is that they can be composed. You can run `dspy.MIPROv2` and use the produced program as an input to `dspy.MIPROv2` again or, say, to `dspy.BootstrapFinetune` to get better results. This is partly the essence of `dspy.BetterTogether`. Alternatively, you can run the optimizer and then extract the top-5 candidate programs and build a `dspy.Ensemble` of them. This allows you to scale _inference-time compute_ (e.g., ensembles) as well as DSPy's unique _pre-inference time compute_ (i.e., optimization budget) in highly systematic ways.



<!-- Future:
BootstrapRS or MIPRO on ??? with a local SGLang LM
BootstrapFS on MATH with a tiny LM like Llama-3.2 with Ollama (maybe with a big teacher) -->



## 3) **DSPy's Ecosystem** advances open-source AI research.

Compared to monolithic LMs, DSPy's modular paradigm enables a large community to improve the compositional architectures, inference-time strategies, and optimizers for LM programs in an open, distributed way. This gives DSPy users more control, helps them iterate much faster, and allows their programs to get better over time by applying the latest optimizers or modules.

The DSPy research effort started at Stanford NLP in Feb 2022, building on what we had learned from developing early [compound LM systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/) like [ColBERT-QA](https://arxiv.org/abs/2007.00814), [Baleen](https://arxiv.org/abs/2101.00436), and [Hindsight](https://arxiv.org/abs/2110.07752). The first version was released as [DSP](https://arxiv.org/abs/2212.14024) in Dec 2022 and evolved by Oct 2023 into [DSPy](https://arxiv.org/abs/2310.03714). Thanks to [250 contributors](https://github.com/stanfordnlp/dspy/graphs/contributors), DSPy has introduced tens of thousands of people to building and optimizing modular LM programs.

Since then, DSPy's community has produced a large body of work on optimizers, like [MIPROv2](https://arxiv.org/abs/2406.11695), [BetterTogether](https://arxiv.org/abs/2407.10930), and [LeReT](https://arxiv.org/abs/2410.23214), on program architectures, like [STORM](https://arxiv.org/abs/2402.14207), [IReRa](https://arxiv.org/abs/2401.12178), and [DSPy Assertions](https://arxiv.org/abs/2312.13382), and on successful applications to new problems, like [PAPILLON](https://arxiv.org/abs/2410.17127), [PATH](https://arxiv.org/abs/2406.11706), [WangLab@MEDIQA](https://arxiv.org/abs/2404.14544), [UMD's Prompting Case Study](https://arxiv.org/abs/2406.06608), and [Haize's Red-Teaming Program](https://blog.haizelabs.com/posts/dspy/), in addition to many open-source projects, production applications, and other [use cases](community/use-cases.md).



================================================
FILE: docs/docs/roadmap.md
================================================
---
draft: true
---

!!! warning "This document is from Aug 2024. Since then, DSPy 2.5 and 2.6 were released, DSPy has grown considerably, and 3.0 is approaching! Content below is highly outdated."



# Roadmap Sketch: DSPy 2.5+

Itâ€™s been a year since DSPy evolved out of Demonstrateâ€“Searchâ€“Predict (DSP), whose research started at Stanford NLP all the way back in February 2022. Thanks to 200 wonderful contributors, DSPy has introduced tens of thousands of people to building modular LM programs and optimizing their prompts and weights automatically. In this time, DSPy has grown to 160,000 monthly downloads and 16,000 stars on GitHub, becoming synonymous with prompt optimization in many circles and inspiring at least a half-dozen cool new libraries.

This document is an initial sketch of DSPyâ€™s public roadmap for the next few weeks and months, as we work on DSPy 2.5 and plan for DSPy 3.0. Suggestions and open-source contributors are more than welcome: just open an issue or submit a pull request regarding the roadmap.



## Technical Objectives

The thesis of DSPy is that for LMs to be useful, we have to shift from ad-hoc prompting to new notions of programming LMs. Instead of relying on LMs gaining much more general or more compositional capabilities, we need to enable developers to iteratively explore their problems and build modular software that invokes LMs for well-scoped tasks. We need to enable that through modules and optimizers that isolate how they decompose their problems and describe their system's objectives from how their LMs are invoked or fine-tuned to maximize their objectives. DSPy's goal has been to develop (and to build the community and shared infrastructure for the collective development of) the abstractions, programming patterns, and optimizers toward this thesis.

To a first approximation, DSPyâ€™s current user-facing language has the minimum number of appropriate abstractions that address the goals above: declarative signatures, define-by-run modules, and optimizers that can be composed quite powerfully. But there are several things we need to do better to realize our goals. The upcoming DSPy releases will have the following objectives.

1. Polishing the core functionality.
2. Developing more accurate, lower-cost optimizers.
3. Building end-to-end tutorials from DSPyâ€™s ML workflow to deployment.
4. Shifting towards more interactive optimization & tracking.



## Team & Organization

DSPy is fairly unusual in its technical objectives, contributors, and audience. Though DSPy takes inspiration from PyTorch, a library for building and optimizing DNNs, there is one major difference: PyTorch was introduced well after DNNs were mature ML concepts, but DSPy seeks to establish and advance core LM Programs research: the framework is propelled by constant academic research from programming abstractions (like the original **Demonstrateâ€“Searchâ€“Predict** concepts, DSPy **Signatures**, or **LM Assertions**) to NLP systems (like **STORM**, **PATH**, and **IReRa**) to prompt optimizers (like **MIPRO**) and RL (like **BetterTogether**), among many other related directions.

This research all composes into a concrete, practical library, thanks to dozens of industry contributors, many of whom are deploying apps in production using DSPy. Because of this, DSPy reaches not only of grad students and ML engineers, but also many non-ML engineers, from early adopter SWEs to hobbyists exploring new ways of using LMs. The following team, with help from many folks in the OSS community, is working towards the objectives in this Roadmap.

**Project Lead:** Omar Khattab (Stanford & Databricks)

**Project Mentors:** Chris Potts (Stanford), Matei Zaharia (UC Berkeley & Databricks), Heather Miller (CMU & Two Sigma)

**Core Library:** Arnav Singhvi (Databricks & Stanford), Herumb Shandilya (Stanford), Hanna Moazam (Databricks), Sri Vardhamanan (Dashworks), Cyrus Nouroozi (Zenbase), Amir Mehr (Zenbase), Kyle Caverly (Modular), with special thanks to Keshav Santhanam (Stanford), Thomas Ahle (Normal Computing), Connor Shorten (Weaviate)

**Prompt Optimization:** Krista Opsahl-Ong (Stanford), Michael Ryan (Stanford), Josh Purtell (Basis), with special thanks to Eric Zhang (Stanford)

**Finetuning & RL:** Dilara Soylu (Stanford), Isaac Miller (Anyscale), Karel D'Oosterlinck (Ghent), with special thanks to Paridhi Masehswari (Stanford)

**PL Abstractions:** Shangyin Tan (UC Berkeley), Manish Shetty (UC Berkeley), Peter Zhong (CMU)

**Applications:** Jasper Xian (Waterloo), Saron Samuel (Stanford), Alberto Mancarella (Stanford), Faraz Khoubsirat (Waterloo), Saiful Haq (IIT-B), Ashutosh Sharma (UIUC)



## 1) Polishing the core functionality.

Over the next month, polishing is the main objective and likely the one to have the highest ROI on the experience of the average user. Conceptually, DSPy has an extremely small core. Itâ€™s nothing but (1) LMs, (2) Signatures & Modules, (3) Optimizers, and (4) Assertions. These concepts and their implementations evolved organically over the past couple of years. We are working now to consolidate what weâ€™ve learned and refactor internally so that things â€œjust workâ€ out of the box for new users, who may not know all the tips-and-tricks just yet.

More concretely:

1. We want to increase the quality of zero-shot, off-the-shelf DSPy programs, i.e. those not yet compiled on custom data.
2. Wherever possible, DSPy should delegate lower-level internal complexity (like managing LMs and structured generation) to emerging lower-level libraries. When required, we may fork smaller libraries out of DSPy to support infrastructure pieces as their own projects.
3. DSPy should internally be more modular and we need higher compatibility between internal components. Specifically, we need more deeper and more native investment in (i) typed multi-field constraints, (ii) assertions, (iii) observability and experimental tracking, (iv) deployment of artifacts and related concerns like streaming and async, and (v) fine-tuning and serving open models.


### On LMs

As of DSPy 2.4, the library has approximately 20,000 lines of code and roughly another 10,000 lines of code for tests, examples, and documentation. Some of these are clearly necessary (e.g., DSPy optimizers) but others exist only because the LM space lacks the building blocks we need under the hood. Luckily, for LM interfaces, a very strong library now exists: LiteLLM, a library that unifies interfaces to various LM and embedding providers. We expect to reduce around 6000 LoCs of support for custom LMs and retrieval models by shifting a lot of that to LiteLLM.

Objectives in this space include improved caching, saving/loading of LMs, support for streaming and async LM requests. Work here is currently led by Hanna Moazam and Sri Vardhamanan, building on a foundation by Cyrus Nouroozi, Amir Mehr, Kyle Caverly, and others.


### On Signatures & Modules

Traditionally, LMs offer text-in-text-out interfaces. Toward modular programming, DSPy introduced signatures for the first time (as DSP Templates in Jan 2023) as a way to structure the inputs and outputs of LM interactions. Standard prompts conflate interface (â€œwhat should the LM do?â€) with implementation (â€œhow do we tell it to do that?â€). DSPy signatures isolate the former so we can infer and learn the latter from data â€” in the context of a bigger program. Today in the LM landscape, notions of "structured outputs" have evolved dramatically, thanks to constrained decoding and other improvements, and have become mainstream. What may be called "structured inputs" remains is yet to become mainstream outside of DSPy, but is as crucial.

Objectives in this space include refining the abstractions and implementations first-class notion of LM Adapters in DSPy, as translators that sits between signatures and LM interfaces. While Optimizers adjust prompts through interactions with a user-supplied metric and data, Adapters are more concerned with building up interactions with LMs to account for, e.g. (i) non-plaintext LM interfaces like chat APIs, structured outputs, function calling, and multi-modal APIs, (ii) languages beyond English or other forms of higher-level specialization. This has been explored in DSPy on and off in various forms, but we have started working on more fundamental approaches to this problem that will offer tangible improvements to most use-cases. Work here is currently led by Omar Khattab.


### On Finetuning & Serving

In February 2023, DSPy introduced the notion of compiling to optimize the weights of an LM program. (To understand just how long ago that was in AI terms, this was before the Alpaca training project at Stanford had even started and a month before the first GPT-4 was released.) Since then, we have shown in October 2023 and, much more expansively, in July 2024, that the fine-tuning flavor of DSPy can deliver large gains for small LMs, especially when composed with prompt optimization.

Overall, though, most DSPy users in practice explore prompt optimization and not weight optimization and most of our examples do the same. The primary reason for a lot of this is infrastructure. Fine-tuning in the DSPy flavor is more than just training a model: ultimately, we need to bootstrap training data for several different modules in a program, train multiple models and handle model selection, and then load and plug in those models into the program's modules. Doing this robustly at the level of abstraction DSPy offers requires a level of resource management that is not generally supported by external existing tools. Major efforts in this regard are currently led by Dilara Soylu and Isaac Miller.


### On Optimizers & Assertions

This is a naturally major direction in the course of polishing. We will share more thoughts here after making more progress on the three angles above.



## 2) Developing more accurate, lower-cost optimizers.

A very large fraction of the research in DSPy focuses on optimizing the prompts and the weights of LM programs. In December 2022, we introduced the algorithm and abstractions behind BootstrapFewShot (as Demonstrate in DSP) and several of its variants. In February 2023, we introduced the core version of what later became BootstrapFinetune. In August 2023, we introduced new variations of both of these. In December 2023, we introduced the first couple of instruction optimizers into DSPy, CA-OPRO and early versions of MIPRO. These were again upgraded in March 2024. Fast forward to June and July 2024, we released MIPROv2 for prompt optimization and BetterTogether for fine-tuning the weights of LM programs.

We have been working towards a number of stronger optimizers. While we cannot share the internal details of research on new optimizers yet, we can outline the goals. A DSPy optimizer can be characterized via three angles:

1. Quality: How much quality can it deliver from various LMs? How effective does it need the zero-shot program to be in order to work well?
2. Cost: How many labeled (and unlabeled) inputs does it need? How many invocations of the program does it need? How expensive is the resulting optimized program at inference time?
3. Robustness: How well can it generalize to different unseen data points or distributions? How sensitive is it to mistakes of the metric or labels?

Over the next six months, our goal is to dramatically improve each angle of these _when the other two are held constant_.  Concretely, there are three directions here.

- Benchmarking: A key prerequisite here is work on benchmarking. On the team, Michael Ryan and Shangyin Tan are leading these efforts. More soon.

- Quality: The goal here is optimizers that extract, on average, 20% more on representative tasks than MIPROv2 and BetterTogether, under the usual conditions â€” like a few hundred inputs with labels and a good metric starting from a decent zero-shot program. Various efforts here are led by Dilara Soylu, Michael Ryan, Josh Purtell, Krista Opsahl-Ong, and Isaac Miller.

- Efficiency: The goal here is optimizers that match the current best scores from MIPROv2 and BetterTogether but under 1-2 challenges like: (i) starting from only 10-20 inputs with labels, (ii) starting with a weak zero-shot program that scores 0%, (iii) where significant misalignment exists between train/validation and test, or (iii) where the user supplies no metric but provides a very small number of output judgments.



## 3) Building end-to-end tutorials from DSPyâ€™s ML workflow to deployment.

Using DSPy well for solving a new task is just doing good machine learning with LMs, but teaching this is hard. On the one hand, it's an iterative process: you make some initial choices, which will be sub-optimal, and then you refine them incrementally. It's highly exploratory: it's often the case that no one knows yet how to best solve a problem in a DSPy-esque way. One the other hand, DSPy offers many emerging lessons from several years of building LM systems, in which the design space, the data regime, and many other factors are new both to ML experts and to the very large fraction of users that have no ML experience.

Though current docs do address [a bunch of this](learn/index.md) in isolated ways, one thing we've learned is that we should separate teaching the core DSPy language (which is ultimately pretty small) from teaching the emerging ML workflow that works well in a DSPy-esque setting. As a natural extension of this, we need to place more emphasis on steps prior and after to the explicit coding in DSPy, from data collection to deployment that serves and monitors the optimized DSPy program in practice. This is just starting but efforts will be ramping up led by Omar Khattab, Isaac Miller, and Herumb Shandilya.


## 4) Shifting towards more interactive optimization & tracking.

Right now, a DSPy user has a few ways to observe and tweak the process of optimization. They can study the prompts before, during, and after optimization methods like `inspect_history`, built-in logging, and/or the metadata returned by optimizers. Similarly, they can rely on `program.save` and `program.load` to potentially adjust the optimized prompts by hand. Alternatively, they can use one of the many powerful observability integrations â€” like from Phoenix Arize, LangWatch, or Weights & Biases Weave â€” to observe _in real time_ the process of optimization (e.g., scores, stack traces, successful & failed traces, and candidate prompts). DSPy encourages iterative engineering by adjusting the program, data, or metrics across optimization runs. For example, some optimizers allow â€œcheckpointingâ€ â€” e.g., if you optimize with BootstrapFewShotWithRandomSearch for 10 iterations then increase to 15 iterations, the first 10 will be loaded from cache.

While these can accomplish a lot of goals, there are two limitations that future versions of DSPy will seek to address.

1. In general, DSPyâ€™s (i) observability, (ii) experimental tracking, (iii) cost management, and (iii) deployment of programs should become first-class concerns via integration with tools like MLFlow. We will share more plans addressing this for DSPy 2.6 in the next 1-2 months.

2. DSPy 3.0 will introduce new optimizers that prioritize ad-hoc, human-in-the-loop feedback. This is perhaps the only substantial paradigm shift we see as necessary in the foreseeable future in DSPy. It involves various research questions at the level of the abstractions, UI/HCI, and ML, so it is a longer-term goal that we will share more about in the next 3-4 month.





================================================
FILE: docs/docs/api/index.md
================================================
# API Reference

Welcome to the DSPy API reference documentation. This section provides detailed information about DSPy's classes, modules, and functions.


================================================
FILE: docs/docs/api/adapters/Adapter.md
================================================
# dspy.Adapter

<!-- START_API_REF -->
::: dspy.Adapter
    handler: python
    options:
        members:
            - __call__
            - acall
            - format
            - format_assistant_message_content
            - format_conversation_history
            - format_demos
            - format_field_description
            - format_field_structure
            - format_system_message
            - format_task_description
            - format_user_message_content
            - parse
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/adapters/ChatAdapter.md
================================================
# dspy.ChatAdapter

<!-- START_API_REF -->
::: dspy.ChatAdapter
    handler: python
    options:
        members:
            - __call__
            - acall
            - format
            - format_assistant_message_content
            - format_conversation_history
            - format_demos
            - format_field_description
            - format_field_structure
            - format_field_with_value
            - format_finetune_data
            - format_system_message
            - format_task_description
            - format_user_message_content
            - parse
            - user_message_output_requirements
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/adapters/JSONAdapter.md
================================================
# dspy.JSONAdapter

<!-- START_API_REF -->
::: dspy.JSONAdapter
    handler: python
    options:
        members:
            - __call__
            - acall
            - format
            - format_assistant_message_content
            - format_conversation_history
            - format_demos
            - format_field_description
            - format_field_structure
            - format_field_with_value
            - format_finetune_data
            - format_system_message
            - format_task_description
            - format_user_message_content
            - parse
            - user_message_output_requirements
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/adapters/TwoStepAdapter.md
================================================
# dspy.TwoStepAdapter

<!-- START_API_REF -->
::: dspy.TwoStepAdapter
    handler: python
    options:
        members:
            - __call__
            - acall
            - format
            - format_assistant_message_content
            - format_conversation_history
            - format_demos
            - format_field_description
            - format_field_structure
            - format_system_message
            - format_task_description
            - format_user_message_content
            - parse
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/evaluation/answer_exact_match.md
================================================
# dspy.evaluate.answer_exact_match

<!-- START_API_REF -->
::: dspy.evaluate.answer_exact_match
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/evaluation/answer_passage_match.md
================================================
# dspy.evaluate.answer_passage_match

<!-- START_API_REF -->
::: dspy.evaluate.answer_passage_match
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/evaluation/CompleteAndGrounded.md
================================================
# dspy.evaluate.CompleteAndGrounded

<!-- START_API_REF -->
::: dspy.evaluate.CompleteAndGrounded
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/evaluation/Evaluate.md
================================================
# dspy.Evaluate

<!-- START_API_REF -->
::: dspy.Evaluate
    handler: python
    options:
        members:
            - __call__
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/evaluation/EvaluationResult.md
================================================
# dspy.evaluate.EvaluationResult

<!-- START_API_REF -->
::: dspy.evaluate.EvaluationResult
    handler: python
    options:
        members:
            - copy
            - from_completions
            - get
            - get_lm_usage
            - inputs
            - items
            - keys
            - labels
            - set_lm_usage
            - toDict
            - values
            - with_inputs
            - without
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/evaluation/SemanticF1.md
================================================
# dspy.evaluate.SemanticF1

<!-- START_API_REF -->
::: dspy.evaluate.SemanticF1
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/experimental/Citations.md
================================================
# dspy.experimental.Citations

<!-- START_API_REF -->
::: dspy.experimental.Citations
    handler: python
    options:
        members:
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - from_dict_list
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
            - validate_input
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/experimental/Document.md
================================================
# dspy.experimental.Document

<!-- START_API_REF -->
::: dspy.experimental.Document
    handler: python
    options:
        members:
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
            - validate_input
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/models/Embedder.md
================================================
# dspy.Embedder

<!-- START_API_REF -->
::: dspy.Embedder
    handler: python
    options:
        members:
            - __call__
            - acall
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/models/LM.md
================================================
# dspy.LM

<!-- START_API_REF -->
::: dspy.LM
    handler: python
    options:
        members:
            - __call__
            - acall
            - aforward
            - copy
            - dump_state
            - finetune
            - forward
            - infer_provider
            - inspect_history
            - kill
            - launch
            - reinforce
            - update_history
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/BestOfN.md
================================================
# dspy.BestOfN

<!-- START_API_REF -->
::: dspy.BestOfN
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/ChainOfThought.md
================================================
# dspy.ChainOfThought

<!-- START_API_REF -->
::: dspy.ChainOfThought
    handler: python
    options:
        members:
            - __call__
            - acall
            - aforward
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/CodeAct.md
================================================
# dspy.CodeAct

<!-- START_API_REF -->
::: dspy.CodeAct
    handler: python
    options:
        members:
            - __call__
            - batch
            - deepcopy
            - dump_state
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
<!-- END_API_REF -->

# CodeAct

CodeAct is a DSPy module that combines code generation with tool execution to solve problems. It generates Python code snippets that use provided tools and the Python standard library to accomplish tasks.

## Basic Usage

Here's a simple example of using CodeAct:

```python
import dspy
from dspy.predict import CodeAct

# Define a simple tool function
def factorial(n: int) -> int:
    """Calculate the factorial of a number."""
    if n == 1:
        return 1
    return n * factorial(n-1)

# Create a CodeAct instance
act = CodeAct("n->factorial_result", tools=[factorial])

# Use the CodeAct instance
result = act(n=5)
print(result) # Will calculate factorial(5) = 120
```

## How It Works

CodeAct operates in an iterative manner:

1. Takes input parameters and available tools
2. Generates Python code snippets that use these tools
3. Executes the code using a Python sandbox
4. Collects the output and determines if the task is complete
5. Answer the original question based on the collected information

## âš ï¸ Limitations

### Only accepts pure functions as tools (no callable objects)

The following example does not work due to the usage of a callable object.

```python
# âŒ NG
class Add():
    def __call__(self, a: int, b: int):
        return a + b

dspy.CodeAct("question -> answer", tools=[Add()])
```

### External libraries cannot be used

The following example does not work due to the usage of the external library `numpy`.

```python
# âŒ NG
import numpy as np

def exp(i: int):
    return np.exp(i)

dspy.CodeAct("question -> answer", tools=[exp])
```

### All dependent functions need to be passed to `CodeAct`

Functions that depend on other functions or classes not passed to `CodeAct` cannot be used. The following example does not work because the tool functions depend on other functions or classes that are not passed to `CodeAct`, such as `Profile` or `secret_function`.

```python
# âŒ NG
from pydantic import BaseModel

class Profile(BaseModel):
    name: str
    age: int
    
def age(profile: Profile):
    return 

def parent_function():
    print("Hi!")

def child_function():
    parent_function()

dspy.CodeAct("question -> answer", tools=[age, child_function])
```

Instead, the following example works since all necessary tool functions are passed to `CodeAct`:

```python
# âœ… OK

def parent_function():
    print("Hi!")

def child_function():
    parent_function()

dspy.CodeAct("question -> answer", tools=[parent_function, child_function])
```



================================================
FILE: docs/docs/api/modules/Module.md
================================================
# dspy.Module

<!-- START_API_REF -->
::: dspy.Module
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/MultiChainComparison.md
================================================
# dspy.MultiChainComparison

<!-- START_API_REF -->
::: dspy.MultiChainComparison
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/Parallel.md
================================================
# dspy.Parallel

<!-- START_API_REF -->
::: dspy.Parallel
    handler: python
    options:
        members:
            - __call__
            - forward
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/Predict.md
================================================
# dspy.Predict

<!-- START_API_REF -->
::: dspy.Predict
    handler: python
    options:
        members:
            - __call__
            - acall
            - aforward
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_config
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset
            - reset_copy
            - save
            - set_lm
            - update_config
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/ProgramOfThought.md
================================================
# dspy.ProgramOfThought

<!-- START_API_REF -->
::: dspy.ProgramOfThought
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/ReAct.md
================================================
# dspy.ReAct

<!-- START_API_REF -->
::: dspy.ReAct
    handler: python
    options:
        members:
            - __call__
            - acall
            - aforward
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
            - truncate_trajectory
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/modules/Refine.md
================================================
# dspy.Refine

<!-- START_API_REF -->
::: dspy.Refine
    handler: python
    options:
        members:
            - __call__
            - acall
            - batch
            - deepcopy
            - dump_state
            - forward
            - get_lm
            - inspect_history
            - load
            - load_state
            - map_named_predictors
            - named_parameters
            - named_predictors
            - named_sub_modules
            - parameters
            - predictors
            - reset_copy
            - save
            - set_lm
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/BetterTogether.md
================================================
# dspy.BetterTogether

<!-- START_API_REF -->
::: dspy.BetterTogether
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/BootstrapFewShot.md
================================================
# dspy.BootstrapFewShot

<!-- START_API_REF -->
::: dspy.BootstrapFewShot
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/BootstrapFewShotWithRandomSearch.md
================================================
# dspy.BootstrapFewShotWithRandomSearch

<!-- START_API_REF -->
::: dspy.BootstrapFewShotWithRandomSearch
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/BootstrapFinetune.md
================================================
# dspy.BootstrapFinetune

<!-- START_API_REF -->
::: dspy.BootstrapFinetune
    handler: python
    options:
        members:
            - compile
            - convert_to_lm_dict
            - finetune_lms
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/BootstrapRS.md
================================================
# dspy.BootstrapRS

<!-- START_API_REF -->
::: dspy.BootstrapRS
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/COPRO.md
================================================
# dspy.COPRO

<!-- START_API_REF -->
::: dspy.COPRO
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/Ensemble.md
================================================
# dspy.Ensemble

<!-- START_API_REF -->
::: dspy.Ensemble
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/InferRules.md
================================================
# dspy.InferRules

<!-- START_API_REF -->
::: dspy.InferRules
    handler: python
    options:
        members:
            - compile
            - evaluate_program
            - format_examples
            - get_params
            - get_predictor_demos
            - induce_natural_language_rules
            - update_program_instructions
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/KNN.md
================================================
# dspy.KNN

<!-- START_API_REF -->
::: dspy.KNN
    handler: python
    options:
        members:
            - __call__
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/KNNFewShot.md
================================================
# dspy.KNNFewShot

<!-- START_API_REF -->
::: dspy.KNNFewShot
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/LabeledFewShot.md
================================================
# dspy.LabeledFewShot

<!-- START_API_REF -->
::: dspy.LabeledFewShot
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/optimizers/MIPROv2.md
================================================
# dspy.MIPROv2

`MIPROv2` (<u>M</u>ultiprompt <u>I</u>nstruction <u>PR</u>oposal <u>O</u>ptimizer Version 2) is an prompt optimizer capable of optimizing both instructions and few-shot examples jointly. It does this by bootstrapping few-shot example candidates, proposing instructions grounded in different dynamics of the task, and finding an optimized combination of these options using Bayesian Optimization. It can be used for optimizing few-shot examples & instructions jointly, or just instructions for 0-shot optimization.

<!-- START_API_REF -->
::: dspy.MIPROv2
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->

## Example Usage

The program below shows optimizing a math program with MIPROv2

```python
import dspy
from dspy.datasets.gsm8k import GSM8K, gsm8k_metric

# Import the optimizer
from dspy.teleprompt import MIPROv2

# Initialize the LM
lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')
dspy.configure(lm=lm)

# Initialize optimizer
teleprompter = MIPROv2(
    metric=gsm8k_metric,
    auto="medium", # Can choose between light, medium, and heavy optimization runs
)

# Optimize program
print(f"Optimizing program with MIPROv2...")
gsm8k = GSM8K()
optimized_program = teleprompter.compile(
    dspy.ChainOfThought("question -> answer"),
    trainset=gsm8k.train,
)

# Save optimize program for future use
optimized_program.save(f"optimized.json")
```

## How `MIPROv2` works

At a high level, `MIPROv2` works by creating both few-shot examples and new instructions for each predictor in your LM program, and then searching over these using Bayesian Optimization to find the best combination of these variables for your program.  If you want a visual explanation check out this [twitter thread](https://x.com/michaelryan207/status/1804189184988713065).

These steps are broken down in more detail below:

1) **Bootstrap Few-Shot Examples**: Randomly samples examples from your training set, and run them through your LM program. If the output from the program is correct for this example, it is kept as a valid few-shot example candidate. Otherwise, we try another example until we've curated the specified amount of few-shot example candidates. This step creates `num_candidates` sets of `max_bootstrapped_demos` bootstrapped examples and `max_labeled_demos` basic examples sampled from the training set.

2) **Propose Instruction Candidates**. The instruction proposer includes (1) a generated summary of properties of the training dataset, (2) a generated summary of your LM program's code and the specific predictor that an instruction is being generated for, (3) the previously bootstrapped few-shot examples to show reference inputs / outputs for a given predictor and (4) a randomly sampled tip for generation (i.e. "be creative", "be concise", etc.) to help explore the feature space of potential instructions.  This context is provided to a `prompt_model` which writes high quality instruction candidates.

3) **Find an Optimized Combination of Few-Shot Examples & Instructions**. Finally, we use Bayesian Optimization to choose which combinations of instructions and demonstrations work best for each predictor in our program. This works by running a series of `num_trials` trials, where a new set of prompts are evaluated over our validation set at each trial. The new set of prompts are only evaluated on a minibatch of size `minibatch_size` at each trial (when `minibatch`=`True`). The best averaging set of prompts is then evaluated on the full validation set every `minibatch_full_eval_steps`. At the end of the optimization process, the LM program with the set of prompts that performed best on the full validation set is returned.

For those interested in more details, more information on `MIPROv2` along with a study on `MIPROv2` compared with other DSPy optimizers can be found in [this paper](https://arxiv.org/abs/2406.11695).



================================================
FILE: docs/docs/api/optimizers/SIMBA.md
================================================
# dspy.SIMBA

<!-- START_API_REF -->
::: dspy.SIMBA
    handler: python
    options:
        members:
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
<!-- END_API_REF -->

## Example Usage

```python
optimizer = dspy.SIMBA(metric=your_metric)
optimized_program = optimizer.compile(your_program, trainset=your_trainset)

# Save optimize program for future use
optimized_program.save(f"optimized.json")
```

## How `SIMBA` works
SIMBA (Stochastic Introspective Mini-Batch Ascent) is a DSPy optimizer that uses the LLM to analyze its own performance and generate improvement rules. It samples mini-batches, identifies challenging examples with high output variability, then either creates self-reflective rules or adds successful examples as demonstrations. See [this great blog post](https://blog.mariusvach.com/posts/dspy-simba) from [Marius](https://x.com/rasmus1610) for more details.


================================================
FILE: docs/docs/api/optimizers/GEPA/GEPA_Advanced.md
================================================
# dspy.GEPA - Advanced Features

## Custom Instruction Proposers

### What is instruction_proposer?

The `instruction_proposer` is the component responsible for invoking the `reflection_lm` and proposing new prompts during GEPA optimization. When GEPA identifies underperforming components in your DSPy program, the instruction proposer analyzes execution traces, feedback, and failures to generate improved instructions tailored to the observed issues.

### Default Implementation

By default, GEPA uses the built-in instruction proposer from the [GEPA library](https://github.com/gepa-ai/gepa), which implements the [`ProposalFn`](https://github.com/gepa-ai/gepa/blob/main/src/gepa/core/adapter.py). The [default proposer](https://github.com/gepa-ai/gepa/blob/main/src/gepa/proposer/reflective_mutation/reflective_mutation.py#L53-L75) uses this prompt template:

````
I provided an assistant with the following instructions to perform a task for me:
```
<curr_instructions>
```

The following are examples of different task inputs provided to the assistant along with the assistant's response for each of them, and some feedback on how the assistant's response could be better:
```
<inputs_outputs_feedback>
```

Your task is to write a new instruction for the assistant.

Read the inputs carefully and identify the input format and infer detailed task description about the task I wish to solve with the assistant.

Read all the assistant responses and the corresponding feedback. Identify all niche and domain specific factual information about the task and include it in the instruction, as a lot of it may not be available to the assistant in the future. The assistant may have utilized a generalizable strategy to solve the task, if so, include that in the instruction as well.

Provide the new instructions within ``` blocks.
````

This template is automatically filled with:

- `<curr_instructions>`: The current instruction being optimized
- `<inputs_outputs_feedback>`: Structured markdown containing predictor inputs, generated outputs, and evaluation feedback

Example of default behavior:

```python
# Default instruction proposer is used automatically
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
    auto="medium"
)
optimized_program = gepa.compile(student, trainset=examples)
```

### When to Use Custom instruction_proposer

**Note:** Custom instruction proposers are an advanced feature. Most users should start with the default proposer, which works well for most text-based optimization tasks.

Consider implementing a custom instruction proposer when you need:

- **Multi-modal handling**: Process images (dspy.Image) alongside textual information in your inputs
- **Nuanced control on limits and length constraints**: Have more fine-grained control over instruction length, format, and structural requirements
- **Domain-specific information**: Inject specialized knowledge, terminology, or context that the default proposer lacks and cannot be provided via feedback_func. This is an advanced feature, and most users should not need to use this.
- **Provider-specific prompting guides**: Optimize instructions for specific LLM providers (OpenAI, Anthropic, etc.) with their unique formatting preferences
- **Coupled component updates**: Handle situations where 2 or more components need to be updated together in a coordinated manner, rather than optimizing each component independently (refer to component_selector parameter, in [Custom Component Selection](#custom-component-selection) section, for related functionality)
- **External knowledge integration**: Connect to databases, APIs, or knowledge bases during instruction generation

### Available Options

**Built-in Options:**

- **Default Proposer**: The standard GEPA instruction proposer (used when `instruction_proposer=None`). The default instruction proposer IS an instruction proposer as well! It is the most general one, that was used for the diverse experiments reported in the GEPA paper and tutorials.
- **MultiModalInstructionProposer**: Handles `dspy.Image` inputs and structured multimodal content.

```python
from dspy.teleprompt.gepa.instruction_proposal import MultiModalInstructionProposer

# For tasks involving images or multimodal inputs
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
    instruction_proposer=MultiModalInstructionProposer(),
    auto="medium"
)
```

We invite community contributions of new instruction proposers for specialized domains as the [GEPA library](https://github.com/gepa-ai/gepa) continues to grow.

### How to Implement Custom Instruction Proposers

Custom instruction proposers must implement the `ProposalFn` protocol by defining a callable class or function. GEPA will call your proposer during optimization:

```python
from dspy.teleprompt.gepa.gepa_utils import ReflectiveExample

class CustomInstructionProposer:
    def __call__(
        self,
        candidate: dict[str, str],                          # Candidate component name -> instruction mapping to be updated in this round
        reflective_dataset: dict[str, list[ReflectiveExample]],  # Component -> examples with structure: {"Inputs": ..., "Generated Outputs": ..., "Feedback": ...}
        components_to_update: list[str]                     # Which components to improve
    ) -> dict[str, str]:                                    # Return new instruction mapping only for components being updated
        # Your custom instruction generation logic here
        return updated_instructions

# Or as a function:
def custom_instruction_proposer(candidate, reflective_dataset, components_to_update):
    # Your custom instruction generation logic here
    return updated_instructions
```

**Reflective Dataset Structure:**

- `dict[str, list[ReflectiveExample]]` - Maps component names to lists of examples
- `ReflectiveExample` TypedDict contains:
  - `Inputs: dict[str, Any]` - Predictor inputs (may include dspy.Image objects)
  - `Generated_Outputs: dict[str, Any] | str` - Success: output fields dict, Failure: error message
  - `Feedback: str` - Always a string from metric function or auto-generated by GEPA

#### Basic Example: Word Limit Proposer

```python
import dspy
from gepa.core.adapter import ProposalFn
from dspy.teleprompt.gepa.gepa_utils import ReflectiveExample

class GenerateWordLimitedInstruction(dspy.Signature):
    """Given a current instruction and feedback examples, generate an improved instruction with word limit constraints."""

    current_instruction = dspy.InputField(desc="The current instruction that needs improvement")
    feedback_summary = dspy.InputField(desc="Feedback from examples that might include both positive and negative cases")
    max_words = dspy.InputField(desc="Maximum number of words allowed in the new instruction")

    improved_instruction = dspy.OutputField(desc="A new instruction that fixes the issues while staying under the max_words limit")

class WordLimitProposer(ProposalFn):
    def __init__(self, max_words: int = 1000):
        self.max_words = max_words
        self.instruction_improver = dspy.ChainOfThought(GenerateWordLimitedInstruction)

    def __call__(self, candidate: dict[str, str], reflective_dataset: dict[str, list[ReflectiveExample]], components_to_update: list[str]) -> dict[str, str]:
        updated_components = {}

        for component_name in components_to_update:
            if component_name not in candidate or component_name not in reflective_dataset:
                continue

            current_instruction = candidate[component_name]
            component_examples = reflective_dataset[component_name]

            # Create feedback summary
            feedback_text = "\n".join([
                f"Example {i+1}: {ex.get('Feedback', 'No feedback')}"
                for i, ex in enumerate(component_examples)  # Limit examples to prevent context overflow
            ])

            # Use the module to improve the instruction
            result = self.instruction_improver(
                current_instruction=current_instruction,
                feedback_summary=feedback_text,
                max_words=self.max_words
            )

            updated_components[component_name] = result.improved_instruction

        return updated_components

# Usage
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
    instruction_proposer=WordLimitProposer(max_words=700),
    auto="medium"
)
```

#### Advanced Example: RAG-Enhanced Instruction Proposer

```python
import dspy
from gepa.core.adapter import ProposalFn
from dspy.teleprompt.gepa.gepa_utils import ReflectiveExample

class GenerateDocumentationQuery(dspy.Signature):
    """Analyze examples with feedback to identify common issue patterns and generate targeted database queries for retrieving relevant documentation.

    Your goal is to search a document database for guidelines that address the problematic patterns found in the examples. Look for recurring issues, error types, or failure modes in the feedback, then craft specific search queries that will find documentation to help resolve these patterns."""

    current_instruction = dspy.InputField(desc="The current instruction that needs improvement")
    examples_with_feedback = dspy.InputField(desc="Examples with their feedback showing what issues occurred and any recurring patterns")

    failure_patterns: str = dspy.OutputField(desc="Summarize the common failure patterns identified in the examples")

    retrieval_queries: list[str] = dspy.OutputField(desc="Specific search queries to find relevant documentation in the database that addresses the common issue patterns identified in the problematic examples")

class GenerateRAGEnhancedInstruction(dspy.Signature):
    """Generate improved instructions using retrieved documentation and examples analysis."""

    current_instruction = dspy.InputField(desc="The current instruction that needs improvement")
    relevant_documentation = dspy.InputField(desc="Retrieved guidelines and best practices from specialized documentation")
    examples_with_feedback = dspy.InputField(desc="Examples showing what issues occurred with the current instruction")

    improved_instruction: str = dspy.OutputField(desc="Enhanced instruction that incorporates retrieved guidelines and addresses the issues shown in the examples")

class RAGInstructionImprover(dspy.Module):
    """Module that uses RAG to improve instructions with specialized documentation."""

    def __init__(self, retrieval_model):
        super().__init__()
        self.retrieve = retrieval_model  # Could be dspy.Retrieve or custom retriever
        self.query_generator = dspy.ChainOfThought(GenerateDocumentationQuery)
        self.generate_answer = dspy.ChainOfThought(GenerateRAGEnhancedInstruction)

    def forward(self, current_instruction: str, component_examples: list):
        """Improve instruction using retrieved documentation."""

        # Let LM analyze examples and generate targeted retrieval queries
        query_result = self.query_generator(
            current_instruction=current_instruction,
            examples_with_feedback=component_examples
        )

        results = self.retrieve.query(
            query_texts=query_result.retrieval_queries,
            n_results=3
        )

        relevant_docs_parts = []
        for i, (query, query_docs) in enumerate(zip(query_result.retrieval_queries, results['documents'])):
            if query_docs:
                docs_formatted = "\n".join([f"  - {doc}" for doc in query_docs])
                relevant_docs_parts.append(
                    f"**Search Query #{i+1}**: {query}\n"
                    f"**Retrieved Guidelines**:\n{docs_formatted}"
                )

        relevant_docs = "\n\n" + "="*60 + "\n\n".join(relevant_docs_parts) + "\n" + "="*60

        # Generate improved instruction with retrieved context
        result = self.generate_answer(
            current_instruction=current_instruction,
            relevant_documentation=relevant_docs,
            examples_with_feedback=component_examples
        )

        return result

class DocumentationEnhancedProposer(ProposalFn):
    """Instruction proposer that accesses specialized documentation via RAG."""

    def __init__(self, documentation_retriever):
        """
        Args:
            documentation_retriever: A retrieval model that can search your specialized docs
                                   Could be dspy.Retrieve, ChromadbRM, or custom retriever
        """
        self.instruction_improver = RAGInstructionImprover(documentation_retriever)

    def __call__(self, candidate: dict[str, str], reflective_dataset: dict[str, list[ReflectiveExample]], components_to_update: list[str]) -> dict[str, str]:
        updated_components = {}

        for component_name in components_to_update:
            if component_name not in candidate or component_name not in reflective_dataset:
                continue

            current_instruction = candidate[component_name]
            component_examples = reflective_dataset[component_name]

            result = self.instruction_improver(
                current_instruction=current_instruction,
                component_examples=component_examples
            )

            updated_components[component_name] = result.improved_instruction

        return updated_components

import chromadb

client = chromadb.Client()
collection = client.get_collection("instruction_guidelines")

gepa = dspy.GEPA(
    metric=task_specific_metric,
    reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
    instruction_proposer=DocumentationEnhancedProposer(collection),
    auto="medium"
)
```

#### Integration Patterns

**Using Custom Proposer with External LM:**

```python
class ExternalLMProposer(ProposalFn):
    def __init__(self):
        # Manage your own LM instance
        self.external_lm = dspy.LM('gemini/gemini-2.5-pro')

    def __call__(self, candidate, reflective_dataset, components_to_update):
        updated_components = {}

        with dspy.context(lm=self.external_lm):
            # Your custom logic here using self.external_lm
            for component_name in components_to_update:
                # ... implementation
                pass

        return updated_components

gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=None,  # Optional when using custom proposer
    instruction_proposer=ExternalLMProposer(),
    auto="medium"
)
```

**Best Practices:**

- **Use the full power of DSPy**: Leverage DSPy components like `dspy.Module`, `dspy.Signature`, and `dspy.Predict` to create your instruction proposer rather than direct LM calls. Consider `dspy.Refine` for constraint satisfaction, `dspy.ChainOfThought` for complex reasoning tasks, and compose multiple modules for sophisticated instruction improvement workflows
- **Enable holistic feedback analysis**: While dspy.GEPA's `GEPAFeedbackMetric` processes one (gold, prediction) pair at a time, instruction proposers receive all examples for a component in batch, enabling cross-example pattern detection and systematic issue identification.
- **Mind data serialization**: Serializing everything to strings might not be ideal - handle complex input types (like `dspy.Image`) by maintaining their structure for better LM processing
- **Test thoroughly**: Test your custom proposer with representative failure cases

## Custom Component Selection

### What is component_selector?

The `component_selector` parameter controls which components (predictors) in your DSPy program are selected for optimization at each GEPA iteration. Instead of the default round-robin approach that updates one component at a time, you can implement custom selection strategies that choose single or multiple components based on optimization state, performance trajectories, and other contextual information.

### Default Behavior

By default, GEPA uses a **round-robin strategy** (`RoundRobinReflectionComponentSelector`) that cycles through components sequentially, optimizing one component per iteration:

```python
# Default round-robin component selection
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
    # component_selector="round_robin"  # This is the default
    auto="medium"
)
```

### Built-in Selection Strategies

**String-based selectors:**

- `"round_robin"` (default): Cycles through components one at a time
- `"all"`: Selects all components for simultaneous optimization

```python
# Optimize all components simultaneously
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=reflection_lm,
    component_selector="all",  # Update all components together
    auto="medium"
)

# Explicit round-robin selection
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=reflection_lm,
    component_selector="round_robin",  # One component per iteration
    auto="medium"
)
```

### When to Use Custom Component Selection

Consider implementing custom component selection when you need:

- **Dependency-aware optimization**: Update related components together (e.g., a classifier and its input formatter)
- **LLM-driven selection**: Let an LLM analyze trajectories and decide which components need attention
- **Resource-conscious optimization**: Balance optimization thoroughness with computational budget

### Custom Component Selector Protocol

Custom component selectors must implement the [`ReflectionComponentSelector`](https://github.com/gepa-ai/gepa/blob/main/src/gepa/proposer/reflective_mutation/base.py) protocol by defining a callable class or function. GEPA will call your selector during optimization:

```python
from dspy.teleprompt.gepa.gepa_utils import GEPAState, Trajectory

class CustomComponentSelector:
    def __call__(
        self,
        state: GEPAState,                    # Complete optimization state with history
        trajectories: list[Trajectory],      # Execution traces from the current minibatch
        subsample_scores: list[float],       # Scores for each example in the current minibatch
        candidate_idx: int,                  # Index of the current program candidate being optimized
        candidate: dict[str, str],           # Component name -> instruction mapping
    ) -> list[str]:                          # Return list of component names to optimize
        # Your custom component selection logic here
        return selected_components

# Or as a function:
def custom_component_selector(state, trajectories, subsample_scores, candidate_idx, candidate):
    # Your custom component selection logic here
    return selected_components
```

### Custom Implementation Example

Here's a simple function that alternates between optimizing different halves of your components:

```python
def alternating_half_selector(state, trajectories, subsample_scores, candidate_idx, candidate):
    """Optimize half the components on even iterations, half on odd iterations."""
    components = list(candidate.keys())

    # If there's only one component, always optimize it
    if len(components) <= 1:
        return components

    mid_point = len(components) // 2

    # Use state.i (iteration counter) to alternate between halves
    if state.i % 2 == 0:
        # Even iteration: optimize first half
        return components[:mid_point]
    else:
        # Odd iteration: optimize second half
        return components[mid_point:]

# Usage
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=reflection_lm,
    component_selector=alternating_half_selector,
    auto="medium"
)
```

### Integration with Custom Instruction Proposers

Component selectors work seamlessly with custom instruction proposers. The selector determines which components to update, then the instruction proposer generates new instructions for those components:

```python
# Combined custom selector + custom proposer
gepa = dspy.GEPA(
    metric=my_metric,
    reflection_lm=reflection_lm,
    component_selector=alternating_half_selector,
    instruction_proposer=WordLimitProposer(max_words=500),
    auto="medium"
)
```



================================================
FILE: docs/docs/api/optimizers/GEPA/overview.md
================================================
# dspy.GEPA: Reflective Prompt Optimizer

**GEPA** (Genetic-Pareto) is a reflective optimizer proposed in "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning" (Agrawal et al., 2025, [arxiv:2507.19457](https://arxiv.org/abs/2507.19457)), that adaptively evolves _textual components_ (such as prompts) of arbitrary systems. In addition to scalar scores returned by metrics, users can also provide GEPA with a text feedback to guide the optimization process. Such textual feedback provides GEPA more visibility into why the system got the score that it did, and then GEPA can introspect to identify how to improve the score. This allows GEPA to propose high performing prompts in very few rollouts.

<!-- START_API_REF -->
::: dspy.GEPA
    handler: python
    options:
        members:
            - auto_budget
            - compile
            - get_params
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->

One of the key insights behind GEPA is its ability to leverage domain-specific textual feedback. Users should provide a feedback function as the GEPA metric, which has the following call signature:
<!-- START_API_REF -->
::: dspy.teleprompt.gepa.gepa.GEPAFeedbackMetric
    handler: python
    options:
        members:
            - __call__
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->

When `track_stats=True`, GEPA returns detailed results about all of the proposed candidates, and metadata about the optimization run. The results are available in the `detailed_results` attribute of the optimized program returned by GEPA, and has the following type:
<!-- START_API_REF -->
::: dspy.teleprompt.gepa.gepa.DspyGEPAResult
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->

## Usage Examples

See GEPA usage tutorials in [GEPA Tutorials](../../../tutorials/gepa_ai_program/index.md).

### Inference-Time Search

GEPA can act as a test-time/inference search mechanism. By setting your `valset` to your _evaluation batch_ and using `track_best_outputs=True`, GEPA produces for each batch element the highest-scoring outputs found during the evolutionary search.

```python
gepa = dspy.GEPA(metric=metric, track_stats=True, ...)
new_prog = gepa.compile(student, trainset=my_tasks, valset=my_tasks)
highest_score_achieved_per_task = new_prog.detailed_results.highest_score_achieved_per_val_task
best_outputs = new_prog.detailed_results.best_outputs_valset
```

## How Does GEPA Work?

### 1. **Reflective Prompt Mutation**

GEPA uses LLMs to _reflect_ on structured execution traces (inputs, outputs, failures, feedback), targeting a chosen module and proposing a new instruction/program text tailored to real observed failures and rich textual/environmental feedback.

### 2. **Rich Textual Feedback as Optimization Signal**

GEPA can leverage _any_ textual feedback availableâ€”not just scalar rewards. This includes evaluation logs, code traces, failed parses, constraint violations, error message strings, or even isolated submodule-specific feedback. This allows actionable, domain-aware optimization. 

### 3. **Pareto-based Candidate Selection**

Rather than evolving just the _best_ global candidate (which leads to local optima or stagnation), GEPA maintains a Pareto frontier: the set of candidates which achieve the highest score on at least one evaluation instance. In each iteration, the next candidate to mutate is sampled (with probability proportional to coverage) from this frontier, guaranteeing both exploration and robust retention of complementary strategies.

### Algorithm Summary

1. **Initialize** the candidate pool with the the unoptimized program.
2. **Iterate**:
   - **Sample a candidate** (from Pareto frontier).
   - **Sample a minibatch** from the train set.
   - **Collect execution traces + feedbacks** for module rollout on minibatch.
   - **Select a module** of the candidate for targeted improvement.
   - **LLM Reflection:** Propose a new instruction/prompt for the targeted module using reflective meta-prompting and the gathered feedback.
   - **Roll out the new candidate** on the minibatch; **if improved, evaluate on Pareto validation set**.
   - **Update the candidate pool/Pareto frontier.**
   - **[Optionally] System-aware merge/crossover**: Combine best-performing modules from distinct lineages.
3. **Continue** until rollout or metric budget is exhausted. 
4. **Return** candidate with best aggregate performance on validation.

## Implementing Feedback Metrics

A well-designed metric is central to GEPA's sample efficiency and learning signal richness. GEPA expects the metric to returns a `dspy.Prediction(score=..., feedback=...)`. GEPA leverages natural language traces from LLM-based workflows for optimization, preserving intermediate trajectories and errors in plain text rather than reducing them to numerical rewards. This mirrors human diagnostic processes, enabling clearer identification of system behaviors and bottlenecks.

Practical Recipe for GEPA-Friendly Feedback:

- **Leverage Existing Artifacts**: Use logs, unit tests, evaluation scripts, and profiler outputs; surfacing these often suffices.
- **Decompose Outcomes**: Break scores into per-objective components (e.g., correctness, latency, cost, safety) and attribute errors to steps.
- **Expose Trajectories**: Label pipeline stages, reporting pass/fail with salient errors (e.g., in code generation pipelines).
- **Ground in Checks**: Employ automatic validators (unit tests, schemas, simulators) or LLM-as-a-judge for non-verifiable tasks (as in PUPA).
- **Prioritize Clarity**: Focus on error coverage and decision points over technical complexity.

### Examples

- **Document Retrieval** (e.g., HotpotQA): List correctly retrieved, incorrect, or missed documents, beyond mere Recall/F1 scores.
- **Multi-Objective Tasks** (e.g., PUPA): Decompose aggregate scores to reveal contributions from each objective, highlighting tradeoffs (e.g., quality vs. privacy).
- **Stacked Pipelines** (e.g., code generation: parse â†’ compile â†’ run â†’ profile â†’ evaluate): Expose stage-specific failures; natural-language traces often suffice for LLM self-correction.

## Tool Optimization with GEPA

When `enable_tool_optimization=True`, GEPA jointly optimizes `dspy.ReAct` modules with the tools - GEPA updates predictor instructions and tool descriptions/argument descriptions together, based on execution traces and feedback, instead of keeping tool behavior fixed.

For details, examples, and the underlying design (tool discovery, naming requirements, and interaction with custom instruction proposers), see [Tool Optimization](GEPA_Advanced.md#tool-optimization).

## Custom Instruction Proposal

For advanced customization of GEPA's instruction proposal mechanism, including custom instruction proposers and component selectors, see [Advanced Features](GEPA_Advanced.md).

## Further Reading

- [GEPA Paper: arxiv:2507.19457](https://arxiv.org/abs/2507.19457)
- [GEPA Github](https://github.com/gepa-ai/gepa) - This repository provides the core GEPA evolution pipeline used by `dspy.GEPA` optimizer.
- [DSPy Tutorials](../../../tutorials/gepa_ai_program/index.md)



================================================
FILE: docs/docs/api/primitives/Audio.md
================================================
# dspy.Audio

<!-- START_API_REF -->
::: dspy.Audio
    handler: python
    options:
        members:
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - from_array
            - from_file
            - from_url
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
            - validate_input
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/Code.md
================================================
# dspy.Code

<!-- START_API_REF -->
::: dspy.Code
    handler: python
    options:
        members:
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
            - validate_input
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/Example.md
================================================
# dspy.Example

<!-- START_API_REF -->
::: dspy.Example
    handler: python
    options:
        members:
            - copy
            - get
            - inputs
            - items
            - keys
            - labels
            - toDict
            - values
            - with_inputs
            - without
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/History.md
================================================
# dspy.History

<!-- START_API_REF -->
::: dspy.History
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/Image.md
================================================
# dspy.Image

<!-- START_API_REF -->
::: dspy.Image
    handler: python
    options:
        members:
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - from_PIL
            - from_file
            - from_url
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/Prediction.md
================================================
# dspy.Prediction

<!-- START_API_REF -->
::: dspy.Prediction
    handler: python
    options:
        members:
            - copy
            - from_completions
            - get
            - get_lm_usage
            - inputs
            - items
            - keys
            - labels
            - set_lm_usage
            - toDict
            - values
            - with_inputs
            - without
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/Tool.md
================================================
# dspy.Tool

<!-- START_API_REF -->
::: dspy.Tool
    handler: python
    options:
        members:
            - __call__
            - acall
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - format_as_litellm_function_call
            - from_langchain
            - from_mcp_tool
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/primitives/ToolCalls.md
================================================
# dspy.ToolCalls

<!-- START_API_REF -->
::: dspy.ToolCalls
    handler: python
    options:
        members:
            - adapt_to_native_lm_feature
            - description
            - extract_custom_type_from_annotation
            - format
            - from_dict_list
            - is_streamable
            - parse_lm_response
            - parse_stream_chunk
            - serialize_model
            - validate_input
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/signatures/InputField.md
================================================
# dspy.InputField

<!-- START_API_REF -->
::: dspy.InputField
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/signatures/OutputField.md
================================================
# dspy.OutputField

<!-- START_API_REF -->
::: dspy.OutputField
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/signatures/Signature.md
================================================
# dspy.Signature

<!-- START_API_REF -->
::: dspy.Signature
    handler: python
    options:
        members:
            - append
            - delete
            - dump_state
            - equals
            - insert
            - load_state
            - prepend
            - with_instructions
            - with_updated_fields
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/tools/ColBERTv2.md
================================================
# dspy.ColBERTv2

<!-- START_API_REF -->
::: dspy.ColBERTv2
    handler: python
    options:
        members:
            - __call__
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/tools/Embeddings.md
================================================
# dspy.retrievers.Embeddings

<!-- START_API_REF -->
::: dspy.Embeddings
    handler: python
    options:
        members:
            - __call__
            - forward
            - from_saved
            - load
            - save
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/tools/PythonInterpreter.md
================================================
# dspy.PythonInterpreter

<!-- START_API_REF -->
::: dspy.PythonInterpreter
    handler: python
    options:
        members:
            - __call__
            - execute
            - shutdown
            - start
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/utils/asyncify.md
================================================
# dspy.asyncify

<!-- START_API_REF -->
::: dspy.asyncify
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_full_path: true
        show_object_full_path: false
        separate_signature: false
        inherited_members: true
:::
<!-- END_API_REF -->



================================================
FILE: docs/docs/api/utils/configure_cache.md
================================================
# dspy.configure_cache

<!-- START_API_REF -->
::: dspy.configure_cache
    handler: python
    options:
        show_source: true
        show_root_heading: true
        heading_level: 2
        docstring_style: google
        show_root_