<!-- Chunk 502: bytes 851763-853115, type=function -->
def factuality_reward(args, pred: dspy.Prediction) -> float:
    """Reward function for factual correctness."""
    # Use LLM to judge factuality
    judge = dspy.Predict(FactualityJudge)
    result = judge(
        statement=pred.answer,
        context=args.context
    )
    return 1.0 if result.is_factual else 0.0

refine = dspy.Refine(
    module=dspy.ChainOfThought("question -> answer"),
    N=3,
    reward_fn=factuality_reward,
    threshold=1.0
)
```

## Comparison

### BestOfN vs Refine

| Aspect | BestOfN | Refine |
|---------|----------|---------|
| **Strategy** | Generate N independent candidates | Iterate with feedback loop |
| **Speed** | Faster (parallel candidates) | Slower (sequential with feedback) |
| **Quality** | Good for simple improvements | Better for complex refinements |
| **Feedback** | No feedback between attempts | Automatic feedback generation |
| **Use Case** | Select best from multiple candidates | Iteratively improve with learning |

### When to Use BestOfN

- Need to generate multiple candidates quickly
- Independent attempts are sufficient
- Simple reward functions
- Parallel processing available

### When to Use Refine

- Need iterative improvement
- Want feedback between attempts
- Complex reward functions
- Sequential refinement preferred

## Reward Functions

### Exact Match Reward

```python
