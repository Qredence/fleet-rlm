<!-- Chunk 1017: bytes 3567059-3570948, type=class -->
class OptimizerEnsemble:
    async def run_all(self, program, trainset, metric, budget) -> EnsembleResult:
        """Run all 3 optimizers, return weighted ensemble."""
        
        # 1. Run optimizers in parallel
        results = await asyncio.gather(
            self.run_bootstrap(program, trainset, metric),
            self.run_mipro(program, trainset, metric),
            self.run_gepa(program, trainset, metric)
        )
        
        # 2. Score each result
        scores = [
            (results[0], self.optimizer_weights["bootstrap"]),
            (results[1], self.optimizer_weights["mipro"]),
            (results[2], self.optimizer_weights["gepa"])
        ]
        
        # 3. Ensemble via majority voting
        best = max(scores, key=lambda x: x[0].metric * x[1])
        
        return EnsembleResult(
            best_program=best[0],
            all_results=results,
            weighted_confidence=self.compute_confidence(scores)
        )
```

**Optimizer Weights** (learned from historical data):
```python
# Track which optimizer type works best across many runs
optimizer_weights = {
    "bootstrap": 0.7,   # 70% of runs bootstrap wins
    "mipro": 0.85,      # 85% of runs mipro wins
    "gepa": 0.65        # 65% of runs gepa wins
}

# Updated periodically from results
```

**New API Endpoint**:
```
POST /api/v1/optimization/ensemble
Request: {
  "skill_path": "skills/python/async",
  "trainset_size": 50,
  "budget": "$20",
  "timeout_seconds": 300
}
Response: {
  "job_id": "job_ensemble_123",
  "status": "queued",
  "estimated_cost": "$20",
  "estimated_time": "10 min (parallel)"
}

# Results when complete:
{
  "best_program": "mipro_v1",
  "ensemble_programs": [
    {"optimizer": "bootstrap", "metric": 0.82, "cost": "$0.50"},
    {"optimizer": "mipro", "metric": 0.84, "cost": "$8.00"},
    {"optimizer": "gepa", "metric": 0.81, "cost": "$1.50"}
  ],
  "weighted_confidence": 0.89,
  "recommendation": "Use mipro result (highest quality)"
}
```

**CLI Integration**:
```bash
uv run skill-fleet optimize --ensemble --budget="$20"
# Runs all 3 optimizers in parallel
# Takes ~same time as single run (parallelization)
# Returns best result + backup options
```

**Cost Trade-off**:
- Single optimizer: $5 | Time: 10 min
- Ensemble: $15-20 | Time: 10 min (parallel) | Confidence: +15%

**Files to Create**:
1. Update `src/skill_fleet/core/dspy/modules/ensemble.py` with OptimizerEnsemble
2. Update `src/skill_fleet/api/routes/optimization.py` with /ensemble endpoint
3. Update `src/skill_fleet/cli/commands/optimize.py` with --ensemble flag
4. Track optimizer performance: `config/optimizer_performance.json`

**Timeline**: 2-3 days | **Impact**: More robust optimization, hedging against failure

---

#### **4C: Skill Similarity-Based Recommendations** üîç

**Problem**: User doesn't know if similar skill exists when creating new one. Reduces reuse, increases duplication.

**Solution**: Semantic similarity index; suggest related skills during creation. **Local-only: On-demand rebuild (not nightly scheduled).**

**Implementation**:
- New module: `src/skill_fleet/core/dspy/tools/similarity.py`
  - Class: `SkillSimilarityIndex`
  - Embeds all existing skills
  - Stores embeddings in compressed format
  - On create: Find similar skills, ask user

**Similarity Workflow**:
```
1. Phase 1 (Understanding): Extract skill intent
2. Embed intent using semantic model
3. Find top 3 similar skills from index
4. Ask user: "Similar skill exists. Extend instead? Y/N"
5. If Y: Branch to "extend existing skill" flow
   If N: Continue normal creation
```

**Index Storage**:
```json
{
  "skills/python/async": {
    "embedding": [0.1, 0.2, -0.3, ...],
    "topics": ["python", "async", "concurrency"],
    "category": "python",
    "style": "comprehensive",
    "last_updated": "2026-01-20"
  },
  ...
}
```

**Similarity Search**:
```python
