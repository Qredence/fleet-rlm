<!-- Chunk 133: bytes 187643-189275, type=function -->
def worker(thread_id):
    # Each thread can have its own LM context
    with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):
        result = qa(question=f"Question from thread {thread_id}")
        print(f"Thread {thread_id}: {result.answer}")

# Create multiple threads
threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]

# Start all threads
for thread in threads:
    thread.start()

# Wait for all threads
for thread in threads:
    thread.join()
```

## Responses API

### Enable Responses API

Configure DSPy to use the Responses API for advanced models:

```python
import dspy

# Configure DSPy to use the Responses API
dspy.configure(
    lm=dspy.LM(
        "openai/gpt-5-mini",
        model_type="responses",
        temperature=1.0,
        max_tokens=16000,
    ),
)
```

### When to Use Responses API

- **Advanced reasoning models**: Models with enhanced reasoning capabilities
- **Better features**: Access to advanced model features
- **Improved quality**: Potential for higher quality outputs

### Note

Not all models or providers support the Responses API. Check LiteLLM's documentation to verify compatibility with your chosen model.

## Best Practices

### 1. Use Environment Variables for API Keys

```python
import os
import dspy

# Good: Use environment variables
api_key = os.getenv('OPENAI_API_KEY')
lm = dspy.LM('openai/gpt-4o-mini', api_key=api_key)
dspy.configure(lm=lm)

# Bad: Hardcode API keys
lm = dspy.LM('openai/gpt-4o-mini', api_key='sk-...')
dspy.configure(lm=lm)
```

### 2. Configure Default LM at Startup

```python
# Good: Configure LM once at startup
import dspy

