<!-- Chunk 833: bytes 2885123-2889945, type=function -->
def notify_hitl_response(job_id: str, response: dict[str, Any]) -> None:
    from .job_manager import get_job_manager
    
    manager = get_job_manager()
    job = manager.get_job(job_id)
    
    if job is None:
        logger.error(f"Cannot notify: job {job_id} not found")
        return
    
    job.hitl_response = response
    if job.hitl_event is None:
        job.hitl_event = asyncio.Event()
    
    job.hitl_event.set()
    manager.update_job(job_id, {"hitl_response": response})  # Persist
```

### Phase 3: Background Cleanup Task (15 mins)

**File**: `src/skill_fleet/api/lifespan.py` (NEW)

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI
import asyncio
import logging

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan events (startup/shutdown)."""
    
    # STARTUP
    from .job_manager import initialize_job_manager
    from ..db.database import get_db_context
    from ..db.repositories import JobRepository
    
    with get_db_context() as db:
        job_repo = JobRepository(db)
        initialize_job_manager(job_repo)
    
    # Start cleanup background task
    cleanup_task = asyncio.create_task(cleanup_expired_jobs())
    logger.info("JobManager and cleanup task started")
    
    yield  # App runs here
    
    # SHUTDOWN
    cleanup_task.cancel()
    logger.info("Cleanup task stopped")


async def cleanup_expired_jobs():
    """Periodically clean up expired in-memory jobs."""
    from .job_manager import get_job_manager
    
    while True:
        try:
            await asyncio.sleep(300)  # Every 5 minutes
            
            manager = get_job_manager()
            cleaned = manager.cleanup_expired()
            
            if cleaned > 0:
                logger.info(f"Cleaned up {cleaned} expired jobs from memory")
        
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"Error in cleanup task: {e}")
```

### Phase 4: Update API App (10 mins)

**File**: `src/skill_fleet/api/app.py` (MODIFY)

```python
# OLD
from fastapi import FastAPI

app = FastAPI()

# NEW
from contextlib import asynccontextmanager
from .lifespan import lifespan

@asynccontextmanager
async def app_lifespan(app: FastAPI):
    async with lifespan(app) as _:
        yield

app = FastAPI(lifespan=app_lifespan)
```

---

## Schema Integration

### Existing Tables (Already in Place!)

**`Job` table** (db/models.py:839-913)
```sql
CREATE TABLE jobs (
    job_id UUID PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    job_type VARCHAR(64),
    status VARCHAR(32) NOT NULL,  -- pending, running, pending_hitl, completed, failed
    result JSONB,
    error TEXT,
    progress_percent INTEGER,
    created_at TIMESTAMPTZ,
    updated_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    promoted BOOLEAN DEFAULT false,
    ...
);
```

**`HITLInteraction` table** (db/models.py:916-957)
```sql
CREATE TABLE hitl_interactions (
    interaction_id SERIAL PRIMARY KEY,
    job_id UUID REFERENCES jobs(job_id) ON DELETE CASCADE,
    interaction_type VARCHAR(32),  -- clarify, confirm, preview, validate, ...
    round_number INTEGER,
    prompt_data JSONB,  -- Serialized ClarifyingQuestion
    response_data JSONB,  -- Serialized QuestionAnswer
    responded_at TIMESTAMPTZ,
    status VARCHAR(32),  -- pending, answered
    created_at TIMESTAMPTZ,
    timeout_at TIMESTAMPTZ,
    metadata JSONB,
    ...
);
```

**`DeepUnderstandingState` table** (db/models.py:960-991)
```sql
CREATE TABLE deep_understanding_state (
    state_id SERIAL PRIMARY KEY,
    job_id UUID UNIQUE REFERENCES jobs(job_id) ON DELETE CASCADE,
    questions_asked JSONB[],
    answers JSONB[],
    research_performed JSONB[],
    understanding_summary TEXT,
    user_problem TEXT,
    readiness_score FLOAT,
    complete BOOLEAN,
    ...
);
```

### No Migration Needed! âœ…
The schema already supports everything we need. We're just:
- Populating existing tables consistently
- Using repositories instead of in-memory dicts
- Adding cleanup/lifecycle management

---

## Migration Strategy

### Step 1: Deploy JobManager (Non-Breaking)
- Add JobManager code
- Keep both memory + DB working in parallel
- Write to both layers
- Read from memory first, DB as fallback
- **No breaking changes**: existing code still works

### Step 2: Monitor & Validate
- Run in production with dual-layer writes
- Verify DB persistence works
- Check for race conditions
- Confirm HITL event handling

### Step 3: Cleanup Phase-Out (Future)
- Remove direct JOBS dict usage
- Remove JSON session files (.skill_fleet_sessions)
- Retire SESSION_DIR code
- Move entirely to DB-backed approach

---

## Testing Strategy

### Unit Tests (30 mins)
```python
# tests/api/test_job_manager.py

