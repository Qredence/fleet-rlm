<!-- Chunk 1841: bytes 7622026-7623894, type=function -->
def configure_dspy(
    config_path: Path | None = None,
    default_model: str | None = None,
    temperature: float | None = None,
) -> dspy.LM:
    """
    Configure DSPy with fleet settings.

    Call this once at application startup.

    Args:
        config_path: Path to config file
        default_model: Override default model
        temperature: Override temperature

    Returns:
        Configured default LM

    """
    cfg = load_fleet_config(config_path)

    # Build default LM
    lm_config = {"model": "google/gemini-3-flash", "temperature": 0.7}

    if cfg and "llm" in cfg:
        lm_cfg = cfg["llm"]
        if "default_model" in lm_cfg:
            lm_config["model"] = lm_cfg["default_model"]
        if "temperature" in lm_cfg:
            lm_config["temperature"] = lm_cfg["temperature"]

    # Apply overrides
    if default_model:
        lm_config["model"] = default_model
    if temperature is not None:
        lm_config["temperature"] = temperature

    # Environment overrides
    if model := os.getenv("DSPY_MODEL"):
        lm_config["model"] = model
    if temp := os.getenv("DSPY_TEMPERATURE"):
        lm_config["temperature"] = float(temp)

    # Extract typed values for dspy.LM
    model_str: str = str(lm_config["model"])
    temperature: float = float(lm_config["temperature"])
    model_type: str = "chat"
    max_tokens_val = lm_config.get("max_tokens")
    max_tokens: int | None = None
    if max_tokens_val is not None:
        max_tokens = int(max_tokens_val)
    cache: bool = bool(lm_config.get("cache", True))
    num_retries: int = int(lm_config.get("num_retries", 3))

    lm = dspy.LM(
        model=model_str,
        model_type=model_type,
        temperature=temperature,
        max_tokens=max_tokens,
        cache=cache,
        num_retries=num_retries,
    )
    dspy.configure(lm=lm)

    return lm


