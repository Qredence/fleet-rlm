# Codebase: scripts
# Source: /Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/_WORLD/skills-fleet/.skills/rlm-long-context/scripts
# Generated by: codebase_concat.py
#
# Format: ======== FILE: <path> ========
#         <content>
#         ======== END FILE ========
#


============================================================
FILE: cache_manager.py
============================================================

#!/usr/bin/env python3
"""
Cache manager for RLM subagent results.
Avoids re-analyzing chunks for repeated queries.
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
from pathlib import Path


def get_cache_key(chunk_path: str, query: str) -> str:
    """Generate cache key from chunk path and query."""
    key_data = f"{chunk_path}:{query}"
    return hashlib.sha256(key_data.encode()).hexdigest()[:32]


def get_cache_path(cache_dir: str, cache_key: str) -> str:
    """Get full cache file path."""
    return os.path.join(cache_dir, f"{cache_key}.json")


def get_cached_result(cache_dir: str, chunk_path: str, query: str) -> dict | None:
    """Check if result exists in cache.

    Args:
        cache_dir: Directory containing cache files
        chunk_path: Path to chunk file
        query: Query string

    Returns:
        Cached result dict or None if not found
    """
    cache_key = get_cache_key(chunk_path, query)
    cache_path = get_cache_path(cache_dir, cache_key)

    if os.path.exists(cache_path):
        with open(cache_path) as f:
            return json.load(f)
    return None


def cache_result(
    cache_dir: str,
    chunk_path: str,
    query: str,
    result: dict,
) -> str:
    """Cache a subagent result.

    Args:
        cache_dir: Directory for cache files
        chunk_path: Path to chunk file
        query: Query string
        result: Result dict to cache

    Returns:
        Path to cache file
    """
    os.makedirs(cache_dir, exist_ok=True)

    cache_key = get_cache_key(chunk_path, query)
    cache_path = get_cache_path(cache_dir, cache_key)

    # Add metadata
    cache_entry = {
        "chunk_path": chunk_path,
        "query": query,
        "cache_key": cache_key,
        "result": result,
    }

    with open(cache_path, "w") as f:
        json.dump(cache_entry, f, indent=2)

    return cache_path


def invalidate_cache(cache_dir: str, pattern: str | None = None) -> int:
    """Invalidate cache entries.

    Args:
        cache_dir: Directory containing cache files
        pattern: If provided, only invalidate keys matching this pattern

    Returns:
        Number of entries invalidated
    """
    if not os.path.exists(cache_dir):
        return 0

    count = 0
    for filename in os.listdir(cache_dir):
        if not filename.endswith(".json"):
            continue

        filepath = os.path.join(cache_dir, filename)

        if pattern:
            with open(filepath) as f:
                data = json.load(f)
                if pattern not in data.get("query", ""):
                    continue

        os.remove(filepath)
        count += 1

    return count


def list_cache(cache_dir: str) -> list[dict]:
    """List all cached entries.

    Args:
        cache_dir: Directory containing cache files

    Returns:
        List of cache entry metadata
    """
    if not os.path.exists(cache_dir):
        return []

    entries = []
    for filename in os.listdir(cache_dir):
        if not filename.endswith(".json"):
            continue

        filepath = os.path.join(cache_dir, filename)
        with open(filepath) as f:
            data = json.load(f)
            entries.append({
                "cache_key": data.get("cache_key"),
                "query": data.get("query", "")[:50] + "...",
                "chunk": os.path.basename(data.get("chunk_path", "")),
            })

    return entries


def get_cache_stats(cache_dir: str) -> dict:
    """Get cache statistics.

    Args:
        cache_dir: Directory containing cache files

    Returns:
        Dict with cache stats
    """
    if not os.path.exists(cache_dir):
        return {"entries": 0, "size_bytes": 0}

    entries = 0
    size_bytes = 0

    for filename in os.listdir(cache_dir):
        if filename.endswith(".json"):
            entries += 1
            filepath = os.path.join(cache_dir, filename)
            size_bytes += os.path.getsize(filepath)

    return {
        "entries": entries,
        "size_bytes": size_bytes,
        "size_mb": round(size_bytes / (1024 * 1024), 2),
    }


def main():
    parser = argparse.ArgumentParser(
        description="Manage RLM subagent result cache"
    )
    parser.add_argument(
        "--cache-dir",
        default=".claude/rlm_state/cache",
        help="Cache directory",
    )

    subparsers = parser.add_subparsers(dest="command", help="Command")

    # get command
    get_parser = subparsers.add_parser("get", help="Get cached result")
    get_parser.add_argument("--chunk", required=True, help="Chunk file path")
    get_parser.add_argument("--query", required=True, help="Query string")

    # set command
    set_parser = subparsers.add_parser("set", help="Cache a result")
    set_parser.add_argument("--chunk", required=True, help="Chunk file path")
    set_parser.add_argument("--query", required=True, help="Query string")
    set_parser.add_argument("--result", required=True, help="Result JSON string")

    # invalidate command
    inv_parser = subparsers.add_parser("invalidate", help="Invalidate cache")
    inv_parser.add_argument("--pattern", help="Only invalidate matching queries")
    inv_parser.add_argument("--all", action="store_true", help="Clear all cache")

    # list command
    subparsers.add_parser("list", help="List cached entries")

    # stats command
    subparsers.add_parser("stats", help="Show cache statistics")

    args = parser.parse_args()

    if args.command == "get":
        result = get_cached_result(args.cache_dir, args.chunk, args.query)
        if result:
            print(json.dumps(result, indent=2))
        else:
            print("null")

    elif args.command == "set":
        result = json.loads(args.result)
        cache_path = cache_result(args.cache_dir, args.chunk, args.query, result)
        print(f"Cached result to {cache_path}")

    elif args.command == "invalidate":
        if args.all:
            count = invalidate_cache(args.cache_dir)
            print(f"Invalidated {count} cache entries")
        elif args.pattern:
            count = invalidate_cache(args.cache_dir, args.pattern)
            print(f"Invalidated {count} entries matching '{args.pattern}'")
        else:
            print("Use --all or --pattern")

    elif args.command == "list":
        entries = list_cache(args.cache_dir)
        print(f"{'Cache Key':<34} {'Chunk':<20} Query")
        print("-" * 80)
        for e in entries:
            print(f"{e['cache_key']:<34} {e['chunk']:<20} {e['query']}")

    elif args.command == "stats":
        stats = get_cache_stats(args.cache_dir)
        print(f"Cache entries: {stats['entries']}")
        print(f"Cache size: {stats['size_mb']} MB")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()


============================================================
END FILE: cache_manager.py
============================================================

============================================================
FILE: codebase_concat.py
============================================================

#!/usr/bin/env python3
"""Concatenate codebase into single processable file with path markers."""

import os
import argparse
from pathlib import Path


def should_include_file(filepath, include_patterns, exclude_patterns, exclude_dirs):
    """Check if file should be included based on patterns."""
    path_str = str(filepath)
    
    # Check exclude dirs
    for ex_dir in exclude_dirs:
        if ex_dir in path_str.split(os.sep):
            return False
    
    # Check exclude patterns
    for pattern in exclude_patterns:
        if filepath.match(pattern) or pattern in path_str:
            return False
    
    # Check include patterns
    if include_patterns:
        for pattern in include_patterns:
            if filepath.match(pattern) or path_str.endswith(pattern):
                return True
        return False
    
    return True


def concatenate_codebase(source_dir, output_file, 
                         include_patterns=None, 
                         exclude_patterns=None,
                         exclude_dirs=None):
    """
    Concatenate all code files into single file with path markers.
    
    Format:
    ======== FILE: /path/to/file.py ========
    <content>
    ======== END FILE ========
    """
    if include_patterns is None:
        include_patterns = ['*.py', '*.js', '*.ts', '*.jsx', '*.tsx', '*.java', 
                          '*.go', '*.rs', '*.c', '*.cpp', '*.h', '*.md', '*.yaml', 
                          '*.yml', '*.json', '*.toml', '*.sh', '*.Dockerfile']
    
    if exclude_patterns is None:
        exclude_patterns = ['*.min.js', '*.min.css', '*.lock', '*.sum']
    
    if exclude_dirs is None:
        exclude_dirs = ['.git', 'node_modules', '__pycache__', '.venv', 
                       'venv', 'dist', 'build', '.pytest_cache', '.mypy_cache',
                       '.tox', '.coverage', 'htmlcov', '.eggs', '*.egg-info']
    
    source_path = Path(source_dir).resolve()
    files_processed = 0
    total_lines = 0
    total_bytes = 0
    
    with open(output_file, 'w', encoding='utf-8') as outf:
        # Write header
        outf.write(f"# Codebase: {source_path.name}\n")
        outf.write(f"# Source: {source_path}\n")
        outf.write(f"# Generated by: codebase_concat.py\n")
        outf.write("#\n# Format: ======== FILE: <path> ========\n")
        outf.write("#         <content>\n")
        outf.write("#         ======== END FILE ========\n#\n\n")
        
        for filepath in sorted(source_path.rglob('*')):
            if not filepath.is_file():
                continue
            
            rel_path = filepath.relative_to(source_path)
            
            if not should_include_file(filepath, include_patterns, 
                                      exclude_patterns, exclude_dirs):
                continue
            
            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as inf:
                    content = inf.read()
                
                if not content.strip():
                    continue
                
                # Write file marker and content
                outf.write(f"\n{'='*60}\n")
                outf.write(f"FILE: {rel_path}\n")
                outf.write(f"{'='*60}\n\n")
                outf.write(content)
                outf.write(f"\n\n{'='*60}\n")
                outf.write(f"END FILE: {rel_path}\n")
                outf.write(f"{'='*60}\n")
                
                files_processed += 1
                total_lines += content.count('\n')
                total_bytes += len(content.encode('utf-8'))
                
            except Exception as e:
                print(f"Warning: Could not process {rel_path}: {e}")
    
    print(f"Codebase concatenation complete:")
    print(f"  Files: {files_processed}")
    print(f"  Lines: {total_lines:,}")
    print(f"  Size: {total_bytes / (1024*1024):.2f} MB")
    print(f"  Output: {output_file}")
    
    return {
        'files': files_processed,
        'lines': total_lines,
        'bytes': total_bytes,
        'output': output_file
    }


def extract_file_from_concat(concat_file, target_path, output_file=None):
    """Extract a specific file from concatenated codebase."""
    if output_file is None:
        output_file = target_path.replace('/', '_')
    
    with open(concat_file, 'r') as f:
        content = f.read()
    
    # Find the file section
    pattern = f"======== FILE: {target_path} ========\n(.*?)======== END FILE"
    import re
    match = re.search(pattern, content, re.DOTALL)
    
    if match:
        with open(output_file, 'w') as f:
            f.write(match.group(1))
        print(f"Extracted: {target_path} -> {output_file}")
        return True
    else:
        print(f"File not found: {target_path}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='Concatenate codebase into single processable file'
    )
    parser.add_argument('source_dir', help='Source directory to process')
    parser.add_argument('-o', '--output', default='codebase_concat.txt',
                       help='Output file (default: codebase_concat.txt)')
    parser.add_argument('-i', '--include', nargs='+',
                       help='Include patterns (e.g., *.py *.js)')
    parser.add_argument('-e', '--exclude', nargs='+',
                       help='Exclude patterns')
    parser.add_argument('--exclude-dirs', nargs='+',
                       help='Exclude directories')
    parser.add_argument('--extract', metavar='PATH',
                       help='Extract specific file from concatenated output')
    
    args = parser.parse_args()
    
    if args.extract:
        extract_file_from_concat(args.output, args.extract)
    else:
        concatenate_codebase(
            args.source_dir,
            args.output,
            include_patterns=args.include,
            exclude_patterns=args.exclude,
            exclude_dirs=args.exclude_dirs
        )


if __name__ == '__main__':
    main()


============================================================
END FILE: codebase_concat.py
============================================================

============================================================
FILE: orchestrate.py
============================================================

#!/usr/bin/env python3
"""
Main orchestrator for RLM workflow.
Coordinates query-guided selection, semantic chunking, caching, and subagent delegation.
"""

from __future__ import annotations

import argparse
import json
import os
import pickle
import subprocess
from pathlib import Path
from typing import Any


class RLMConfig:
    """Configuration for RLM workflow."""

    def __init__(
        self,
        state_path: str = ".claude/rlm_state/state.pkl",
        chunks_dir: str = ".claude/rlm_state/chunks",
        cache_dir: str = ".claude/rlm_state/cache",
        chunk_size: int = 200000,
        overlap: int = 0,
        top_k: int | None = None,
        confidence_threshold: float = 0.95,
        enable_cache: bool = True,
        enable_early_exit: bool = True,
    ):
        self.state_path = state_path
        self.chunks_dir = chunks_dir
        self.cache_dir = cache_dir
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.top_k = top_k
        self.confidence_threshold = confidence_threshold
        self.enable_cache = enable_cache
        self.enable_early_exit = enable_early_exit


def load_content(state_path: str) -> str:
    """Load content from RLM state."""
    with open(state_path, "rb") as f:
        state = pickle.load(f)
    return state.get("content", "")


def run_rank_chunks(
    config: RLMConfig,
    query: str,
) -> list[tuple[int, float]]:
    """Run chunk ranking script."""
    cmd = [
        "python3",
        "-m", "skills.rlm_long_context.scripts.rank_chunks",
        "--state", config.state_path,
        "--query", query,
        "--chunk-size", str(config.chunk_size),
    ]

    if config.top_k:
        cmd.extend(["--top-k", str(config.top_k)])

    # Parse output to get ranked chunks
    # This is a simplified version - in practice you'd parse the script output
    content = load_content(config.state_path)
    import re

    keywords = [w.lower() for w in re.findall(r"\b\w{3,}\b", query)]
    pattern = re.compile("|".join(re.escape(k) for k in keywords), re.IGNORECASE)

    scores = []
    for i in range(0, len(content), config.chunk_size):
        chunk = content[i:i + config.chunk_size]
        score = len(pattern.findall(chunk))
        scores.append((i // config.chunk_size, score))

    scores.sort(key=lambda x: x[1], reverse=True)

    if config.top_k:
        scores = scores[:config.top_k]

    return scores


def check_cache(
    cache_dir: str,
    chunk_path: str,
    query: str,
) -> dict | None:
    """Check if result is cached."""
    import hashlib

    key_data = f"{chunk_path}:{query}"
    cache_key = hashlib.sha256(key_data.encode()).hexdigest()[:32]
    cache_path = os.path.join(cache_dir, f"{cache_key}.json")

    if os.path.exists(cache_path):
        with open(cache_path) as f:
            return json.load(f)
    return None


def save_cache(
    cache_dir: str,
    chunk_path: str,
    query: str,
    result: dict,
):
    """Save result to cache."""
    import hashlib

    os.makedirs(cache_dir, exist_ok=True)

    key_data = f"{chunk_path}:{query}"
    cache_key = hashlib.sha256(key_data.encode()).hexdigest()[:32]
    cache_path = os.path.join(cache_dir, f"{cache_key}.json")

    cache_entry = {
        "chunk_path": chunk_path,
        "query": query,
        "cache_key": cache_key,
        "result": result,
    }

    with open(cache_path, "w") as f:
        json.dump(cache_entry, f, indent=2)


def estimate_confidence(results: list[dict], query: str) -> float:
    """Estimate confidence based on results so far."""
    if not results:
        return 0.0

    # Simple heuristic: more high-confidence findings = higher confidence
    total_findings = sum(len(r.get("relevant", [])) for r in results)
    high_conf = sum(
        1 for r in results
        for f in r.get("relevant", [])
        if f.get("confidence") == "high"
    )

    # Confidence based on finding density and quality
    if total_findings == 0:
        return 0.1

    quality_ratio = high_conf / total_findings if total_findings > 0 else 0
    density = min(total_findings / len(results), 5) / 5  # Cap at 5 findings per chunk

    return min((quality_ratio * 0.7 + density * 0.3), 1.0)


def print_progress(current: int, total: int, confidence: float):
    """Print progress bar."""
    pct = (current / total) * 100
    bar_len = 30
    filled = int(bar_len * current / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
    print(f"\r[{bar}] {pct:.1f}% ({current}/{total}) Confidence: {confidence:.2f}", end="")


def orchestrate(
    query: str,
    config: RLMConfig,
) -> list[dict]:
    """Main orchestration loop.

    Args:
        query: User query
        config: RLM configuration

    Returns:
        List of subagent results
    """
    print(f"ðŸ” Query: {query}")
    print()

    # Step 1: Rank chunks by relevance
    print("ðŸ“Š Ranking chunks by relevance...")
    ranked_chunks = run_rank_chunks(config, query)
    print(f"   Found {len(ranked_chunks)} chunks to process\n")

    # Step 2: Process chunks with caching and early exit
    results = []
    chunks_to_process = [
        (idx, os.path.join(config.chunks_dir, f"chunk_{idx:04d}.txt"))
        for idx, _ in ranked_chunks
    ]

    print("ðŸ¤– Processing chunks...")
    for i, (chunk_idx, chunk_path) in enumerate(chunks_to_process, 1):
        # Check cache first
        if config.enable_cache:
            cached = check_cache(config.cache_dir, chunk_path, query)
            if cached:
                results.append(cached["result"])
                print_progress(i, len(chunks_to_process), estimate_confidence(results, query))
                print(f"  [cached] chunk_{chunk_idx:04d}")
                continue

        # In a real implementation, this would delegate to the subagent
        # For now, we'll simulate with a placeholder
        result = {
            "chunk_id": f"chunk_{chunk_idx:04d}",
            "relevant": [],  # Would be filled by subagent
            "confidence": 0.0,
        }

        # Cache the result
        if config.enable_cache:
            save_cache(config.cache_dir, chunk_path, query, result)

        results.append(result)

        # Update progress
        confidence = estimate_confidence(results, query)
        print_progress(i, len(chunks_to_process), confidence)

        # Early exit check
        if config.enable_early_exit and i >= 3:
            if confidence >= config.confidence_threshold:
                print(f"\nâœ“ Early exit: confidence {confidence:.2f} >= {config.confidence_threshold}")
                break

    print()  # End progress line
    print(f"\nâœ… Processed {len(results)} chunks")

    # Show cache stats
    if config.enable_cache:
        cache_count = len([
            f for f in os.listdir(config.cache_dir)
            if f.endswith(".json")
        ]) if os.path.exists(config.cache_dir) else 0
        print(f"ðŸ’¾ Cache entries: {cache_count}")

    return results


def main():
    parser = argparse.ArgumentParser(
        description="Orchestrate RLM workflow with optimizations"
    )
    parser.add_argument(
        "--query",
        "-q",
        required=True,
        help="Query to process",
    )
    parser.add_argument(
        "--state",
        default=".claude/rlm_state/state.pkl",
        help="Path to RLM state file",
    )
    parser.add_argument(
        "--chunks-dir",
        default=".claude/rlm_state/chunks",
        help="Directory containing chunks",
    )
    parser.add_argument(
        "--cache-dir",
        default=".claude/rlm_state/cache",
        help="Cache directory",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=200000,
        help="Chunk size",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        help="Process only top K chunks",
    )
    parser.add_argument(
        "--confidence",
        type=float,
        default=0.95,
        help="Confidence threshold for early exit",
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Disable caching",
    )
    parser.add_argument(
        "--no-early-exit",
        action="store_true",
        help="Disable early exit",
    )
    parser.add_argument(
        "--output",
        "-o",
        help="Output file for results (JSON)",
    )

    args = parser.parse_args()

    config = RLMConfig(
        state_path=args.state,
        chunks_dir=args.chunks_dir,
        cache_dir=args.cache_dir,
        chunk_size=args.chunk_size,
        top_k=args.top_k,
        confidence_threshold=args.confidence,
        enable_cache=not args.no_cache,
        enable_early_exit=not args.no_early_exit,
    )

    results = orchestrate(args.query, config)

    # Output results
    if args.output:
        with open(args.output, "w") as f:
            json.dump(results, f, indent=2)
        print(f"\nðŸ“ Results written to {args.output}")
    else:
        print("\nðŸ“‹ Results:")
        print(json.dumps(results, indent=2))


if __name__ == "__main__":
    main()


============================================================
END FILE: orchestrate.py
============================================================

============================================================
FILE: rank_chunks.py
============================================================

#!/usr/bin/env python3
"""
Query-guided chunk selection for RLM workflow.
Ranks chunks by relevance to a query before processing.
"""

from __future__ import annotations

import argparse
import pickle
import re
from pathlib import Path


def load_context(state_path: str) -> str:
    """Load context from RLM state file."""
    with open(state_path, "rb") as f:
        state = pickle.load(f)
    return state.get("content", "")


def rank_chunks_by_query(
    content: str,
    query: str,
    chunk_size: int = 200000,
    top_k: int | None = None,
) -> list[tuple[int, int, float]]:
    """Rank chunks by relevance to query.

    Args:
        content: Full text content
        query: User query string
        chunk_size: Size of each chunk in characters
        top_k: Return only top K chunks (None = all)

    Returns:
        List of (start_pos, end_pos, score) tuples, sorted by score descending
    """
    # Extract keywords from query (simple approach)
    keywords = [w.lower() for w in re.findall(r"\b\w{3,}\b", query) if w.lower() not in {
        "the", "and", "for", "are", "but", "not", "you", "all", "can", "had", "her", "was",
        "one", "our", "out", "day", "get", "has", "him", "his", "how", "man", "new", "now",
        "old", "see", "two", "way", "who", "boy", "did", "its", "let", "put", "say", "she",
        "too", "use",
    }]

    # Create pattern from keywords
    pattern = re.compile("|".join(re.escape(k) for k in keywords), re.IGNORECASE)

    # Score each chunk
    scores = []
    for i in range(0, len(content), chunk_size):
        end = min(i + chunk_size, len(content))
        chunk = content[i:end]

        # Simple scoring: keyword frequency
        matches = len(pattern.findall(chunk))
        score = matches / (len(chunk) / 1000)  # Normalize per 1000 chars

        scores.append((i, end, score))

    # Sort by score descending
    scores.sort(key=lambda x: x[2], reverse=True)

    if top_k:
        scores = scores[:top_k]

    return scores


def main():
    parser = argparse.ArgumentParser(
        description="Rank chunks by relevance to a query"
    )
    parser.add_argument(
        "--state",
        default=".claude/rlm_state/state.pkl",
        help="Path to RLM state file",
    )
    parser.add_argument(
        "--query",
        required=True,
        help="Query to rank chunks against",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=200000,
        help="Chunk size in characters",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=None,
        help="Return only top K chunks",
    )
    parser.add_argument(
        "--output",
        "-o",
        help="Output file for chunk list (one path per line)",
    )
    parser.add_argument(
        "--chunks-dir",
        default=".claude/rlm_state/chunks",
        help="Directory containing chunk files",
    )

    args = parser.parse_args()

    # Load content
    content = load_context(args.state)

    # Rank chunks
    ranked = rank_chunks_by_query(
        content,
        args.query,
        args.chunk_size,
        args.top_k,
    )

    # Print results
    print(f"Query: {args.query}")
    print(f"Total chunks: {len(list(range(0, len(content), args.chunk_size)))}")
    print(f"Ranked chunks: {len(ranked)}")
    print()
    print(f"{'Rank':<6} {'Chunk':<10} {'Score':<10} {'Range':<20}")
    print("-" * 50)

    chunk_files = []
    for rank, (start, end, score) in enumerate(ranked, 1):
        chunk_idx = start // args.chunk_size
        chunk_file = f"{args.chunks_dir}/chunk_{chunk_idx:04d}.txt"
        chunk_files.append(chunk_file)
        print(f"{rank:<6} {chunk_idx:<10} {score:.2f}       {start:,}-{end:,}")

    # Write output file if requested
    if args.output:
        with open(args.output, "w") as f:
            for cf in chunk_files:
                f.write(f"{cf}\n")
        print(f"\nWrote {len(chunk_files)} chunk paths to {args.output}")


if __name__ == "__main__":
    main()


============================================================
END FILE: rank_chunks.py
============================================================

============================================================
FILE: semantic_chunk.py
============================================================

#!/usr/bin/env python3

"""
Semantic chunking for RLM workflow.
Splits content by semantic boundaries (headers, timestamps, etc.)
instead of fixed character counts.
"""

from __future__ import annotations

import argparse
import os
import pickle
import re
from pathlib import Path


def detect_content_type(content: str) -> str:
    """Auto-detect content type based on patterns."""
    # Check for markdown headers
    if re.search(r"^#{1,6} ", content, re.MULTILINE):
        return "markdown"

    # Check for log timestamps
    if re.search(r"^\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}", content, re.MULTILINE):
        return "log"

    # Check for JSON
    if content.strip().startswith(("{", "[")):
        try:
            import json
            json.loads(content[:10000])  # Test first 10K
            return "json"
        except json.JSONDecodeError:
            pass

    # Check for code (Python)
    if re.search(r"^(def |class |import |from )", content, re.MULTILINE):
        return "python"

    return "text"


def chunk_markdown(content: str, max_size: int = 200000) -> list[tuple[int, int, str]]:
    """Chunk by markdown headers."""
    # Find all headers
    headers = list(re.finditer(r"^(#{1,6}) ", content, re.MULTILINE))

    if not headers:
        # Fall back to size-based
        return chunk_by_size(content, max_size)

    chunks = []
    for i, header in enumerate(headers):
        start = header.start()
        end = headers[i + 1].start() if i + 1 < len(headers) else len(content)

        # If chunk is too big, split by sub-headers or size
        chunk_content = content[start:end]
        if len(chunk_content) > max_size:
            sub_chunks = chunk_by_size(chunk_content, max_size, start_offset=start)
            chunks.extend(sub_chunks)
        else:
            chunks.append((start, end, f"h{len(header.group(1))}"))

    return chunks


def chunk_logs(content: str, max_size: int = 200000) -> list[tuple[int, int, str]]:
    """Chunk by log entry timestamps."""
    # Common timestamp patterns
    patterns = [
        r"^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}",  # ISO 8601
        r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}",  # Common log format
        r"^\[\d{4}-\d{2}-\d{2}",  # Bracketed dates
        r"^\w{3} \d{1,2} \d{2}:\d{2}:\d{2}",  # Syslog format
    ]

    # Try each pattern
    timestamps = None
    for pattern in patterns:
        timestamps = list(re.finditer(pattern, content, re.MULTILINE))
        if len(timestamps) > 10:  # Need reasonable number of matches
            break

    if not timestamps or len(timestamps) < 10:
        return chunk_by_size(content, max_size)

    chunks = []
    for i, ts in enumerate(timestamps):
        start = ts.start()
        end = timestamps[i + 1].start() if i + 1 < len(timestamps) else len(content)

        chunk_content = content[start:end]
        if len(chunk_content) > max_size:
            sub_chunks = chunk_by_size(chunk_content, max_size, start_offset=start)
            chunks.extend(sub_chunks)
        else:
            chunks.append((start, end, "entry"))

    return chunks


def chunk_json(content: str, max_size: int = 200000) -> list[tuple[int, int, str]]:
    """Chunk by top-level JSON objects/arrays."""
    import json

    try:
        data = json.loads(content)

        if isinstance(data, list):
            # Split list into chunks
            items = data
            chunks = []
            current_start = 0
            current_items = []
            current_size = 0

            for item in items:
                item_str = json.dumps(item)
                item_size = len(item_str)

                if current_size + item_size > max_size and current_items:
                    # Save current chunk
                    chunk_content = json.dumps(current_items)
                    chunks.append((
                        content.find(chunk_content),
                        content.find(chunk_content) + len(chunk_content),
                        f"array[{len(current_items)}]"
                    ))
                    current_items = [item]
                    current_size = item_size
                else:
                    current_items.append(item)
                    current_size += item_size

            # Last chunk
            if current_items:
                chunk_content = json.dumps(current_items)
                chunks.append((
                    content.find(chunk_content),
                    content.find(chunk_content) + len(chunk_content),
                    f"array[{len(current_items)}]"
                ))

            return chunks

        elif isinstance(data, dict):
            # Split dict into key groups
            items = list(data.items())
            chunks = []
            current_keys = []
            current_size = 0

            for key, value in items:
                item_str = json.dumps({key: value})
                item_size = len(item_str)

                if current_size + item_size > max_size and current_keys:
                    chunk_dict = {k: data[k] for k in current_keys}
                    chunk_content = json.dumps(chunk_dict)
                    chunks.append((
                        content.find(chunk_content),
                        content.find(chunk_content) + len(chunk_content),
                        f"dict[{len(current_keys)}]"
                    ))
                    current_keys = [key]
                    current_size = item_size
                else:
                    current_keys.append(key)
                    current_size += item_size

            if current_keys:
                chunk_dict = {k: data[k] for k in current_keys}
                chunk_content = json.dumps(chunk_dict)
                chunks.append((
                    content.find(chunk_content),
                    content.find(chunk_content) + len(chunk_content),
                    f"dict[{len(current_keys)}]"
                ))

            return chunks

    except json.JSONDecodeError:
        pass

    return chunk_by_size(content, max_size)


def chunk_python(content: str, max_size: int = 200000) -> list[tuple[int, int, str]]:
    """Chunk by Python functions and classes."""
    # Find function and class definitions
    pattern = r"^(def |class )"
    definitions = list(re.finditer(pattern, content, re.MULTILINE))

    if len(definitions) < 5:
        return chunk_by_size(content, max_size)

    chunks = []
    for i, definition in enumerate(definitions):
        start = definition.start()
        end = definitions[i + 1].start() if i + 1 < len(definitions) else len(content)

        chunk_content = content[start:end]
        chunk_type = "class" if definition.group().startswith("class") else "function"

        if len(chunk_content) > max_size:
            sub_chunks = chunk_by_size(chunk_content, max_size, start_offset=start)
            chunks.extend(sub_chunks)
        else:
            chunks.append((start, end, chunk_type))

    return chunks


def chunk_by_size(
    content: str,
    size: int,
    overlap: int = 0,
    start_offset: int = 0,
) -> list[tuple[int, int, str]]:
    """Simple size-based chunking."""
    chunks = []
    step = size - overlap

    for i in range(0, len(content), step):
        start = i
        end = min(i + size, len(content))
        chunks.append((start + start_offset, end + start_offset, "size"))

        if end == len(content):
            break

    return chunks


def write_chunks(
    content: str,
    chunks: list[tuple[int, int, str]],
    output_dir: str,
    prefix: str = "chunk",
) -> list[str]:
    """Write chunks to files."""
    os.makedirs(output_dir, exist_ok=True)
    paths = []

    for i, (start, end, chunk_type) in enumerate(chunks):
        chunk_content = content[start:end]
        filename = f"{prefix}_{i:04d}_{chunk_type}.txt"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, "w") as f:
            f.write(f"<!-- Chunk {i}: bytes {start}-{end}, type={chunk_type} -->\n")
            f.write(chunk_content)

        paths.append(filepath)

    return paths


def main():
    parser = argparse.ArgumentParser(
        description="Create semantic chunks from content"
    )
    parser.add_argument(
        "--state",
        default=".claude/rlm_state/state.pkl",
        help="Path to RLM state file",
    )
    parser.add_argument(
        "--type",
        choices=["auto", "markdown", "log", "json", "python", "text"],
        default="auto",
        help="Content type (auto-detect if not specified)",
    )
    parser.add_argument(
        "--max-size",
        type=int,
        default=200000,
        help="Maximum chunk size in characters",
    )
    parser.add_argument(
        "--overlap",
        type=int,
        default=0,
        help="Overlap between chunks in characters",
    )
    parser.add_argument(
        "--output",
        "-o",
        default=".claude/rlm_state/chunks",
        help="Output directory for chunks",
    )
    parser.add_argument(
        "--prefix",
        default="chunk",
        help="Filename prefix for chunks",
    )

    args = parser.parse_args()

    # Load content
    with open(args.state, "rb") as f:
        state = pickle.load(f)
    content = state.get("content", "")

    # Detect content type
    content_type = args.type
    if content_type == "auto":
        content_type = detect_content_type(content)
        print(f"Auto-detected content type: {content_type}")

    # Chunk based on type
    chunkers = {
        "markdown": chunk_markdown,
        "log": chunk_logs,
        "json": chunk_json,
        "python": chunk_python,
        "text": chunk_by_size,
    }

    chunker = chunkers.get(content_type, chunk_by_size)

    if content_type == "text":
        chunks = chunker(content, args.max_size, args.overlap)
    else:
        chunks = chunker(content, args.max_size)

    # Write chunks
    paths = write_chunks(content, chunks, args.output, args.prefix)

    print(f"Created {len(paths)} semantic chunks in {args.output}")
    print(f"\nChunk breakdown:")
    type_counts = {}
    for _, _, ct in chunks:
        type_counts[ct] = type_counts.get(ct, 0) + 1
    for ct, count in sorted(type_counts.items()):
        print(f"  {ct}: {count}")


if __name__ == "__main__":
    main()


============================================================
END FILE: semantic_chunk.py
============================================================
