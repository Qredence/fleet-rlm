# fleet-rlm Environment Configuration
#
# Copy this file to .env and fill in your own values.
# The .env file is gitignored for securityâ€”never commit secrets!
#
# REQUIRED: LLM Configuration (one of the two API key formats below)
#
# The LLM model to use for the planner (DSPy's reasoning engine)
# Examples:
#   - Google: "google/gemini-3-flash-preview"
#   - OpenAI: "openai/gpt-4-turbo"
#   - Anthropic: "anthropic/claude-3-sonnet"
#   - Use a LiteLLM proxy: "openai/your-proxy-endpoint"
DSPY_LM_MODEL=openai/gemini-3-flash-preview

# API key for the LLM provider (if using standard OpenAI-compatible endpoint)
# Or use DSPY_LM_API_KEY below as an alternative
DSPY_LLM_API_KEY=sk-...

# Alternative API key name (some providers use this instead)
# Uncomment and use if DSPY_LLM_API_KEY doesn't work for your provider
# DSPY_LM_API_KEY=sk-...

#
# OPTIONAL: LLM Endpoint and Parameters
#

# Custom API endpoint (if using LiteLLM proxy or self-hosted model)
# Default: OpenAI-compatible endpoint
# Example: "https://api.together.ai/v1"
# DSPY_LM_API_BASE=https://your-litellm-proxy.com

# Maximum tokens for LLM response
# Default: 8192
# Increase for longer outputs, decrease to save tokens
# DSPY_LM_MAX_TOKENS=65536

#
# NOTES:
# - Modal credentials are per-user: run `modal setup` to configure
# - Modal secrets (API keys) are configured via: modal secret create LITELLM ...
# - Never commit this file once filled with real secrets
# - For team setups, use Modal's secret management instead of .env
#
