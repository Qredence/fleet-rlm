{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLM Evaluation Suite\n",
    "\n",
    "Comprehensive diagnostic and evaluation notebook for fleet-rlm.\n",
    "\n",
    "Use this notebook to:\n",
    "1. Validate Modal environment setup\n",
    "2. Test basic sandbox functionality\n",
    "3. Run full dspy.RLM evaluations\n",
    "4. Visualize and analyze trajectories\n",
    "5. Benchmark against baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RLM Environment Diagnostics ===\n",
      "Python: 3.13.9 (main, Nov 19 2025, 23:39:32) [Clang 21.1.4 ]\n",
      "Working directory: /Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/_WORLD/_RLM/fleet-rlm-dspy/notebooks\n",
      "\n",
      "Modal credentials: ✓ Configured\n",
      "\n",
      "LITELLM Secret Contents:\n",
      "  DSPY_LM_MODEL: ✓\n",
      "  DSPY_LM_API_BASE: ✓\n",
      "  DSPY_LLM_API_KEY: ✓\n",
      "  DSPY_LM_MAX_TOKENS: ✓\n",
      "\n",
      "Overall: ✓ Ready for RLM testing\n"
     ]
    }
   ],
   "source": [
    "# Check environment setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=== RLM Environment Diagnostics ===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check Modal credentials\n",
    "has_modal_token = bool(os.environ.get(\"MODAL_TOKEN_ID\")) and bool(\n",
    "    os.environ.get(\"MODAL_TOKEN_SECRET\")\n",
    ")\n",
    "print(f\"\\nModal credentials: {'✓ Configured' if has_modal_token else '✗ Missing'}\")\n",
    "\n",
    "if has_modal_token:\n",
    "    try:\n",
    "        from fleet_rlm.runners import check_secret_presence\n",
    "\n",
    "        secrets = check_secret_presence()\n",
    "        print(\"\\nLITELLM Secret Contents:\")\n",
    "        for key, present in secrets.items():\n",
    "            print(f\"  {key}: {'✓' if present else '✗'}\")\n",
    "\n",
    "        all_present = all(secrets.values())\n",
    "        print(\n",
    "            f\"\\nOverall: {'✓ Ready for RLM testing' if all_present else '✗ Missing required secrets'}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error checking secrets: {e}\")\n",
    "else:\n",
    "    print(\"\\nSet MODAL_TOKEN_ID and MODAL_TOKEN_SECRET to enable testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Sandbox Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sandbox Health Check ===\n",
      "Sandbox status: healthy\n",
      "Python version: [3, 12]\n",
      "\n",
      "✓ Sandbox health check passed\n"
     ]
    }
   ],
   "source": [
    "# Test basic sandbox functionality\n",
    "from fleet_rlm import ModalInterpreter\n",
    "\n",
    "print(\"=== Sandbox Health Check ===\")\n",
    "\n",
    "try:\n",
    "    interpreter = ModalInterpreter(timeout=60)\n",
    "    interpreter.start()\n",
    "\n",
    "    # Test basic execution\n",
    "    result = interpreter.execute(\"\"\"\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "SUBMIT(status=\"healthy\", python_version=sys.version_info[:2])\n",
    "\"\"\")\n",
    "\n",
    "    # Use attribute access for FinalOutput\n",
    "    output = getattr(result, \"output\", result)\n",
    "    print(f\"Sandbox status: {output.get('status')}\")\n",
    "    print(f\"Python version: {output.get('python_version')}\")\n",
    "\n",
    "    interpreter.shutdown()\n",
    "    print(\"\\n✓ Sandbox health check passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Sandbox health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Variable Space Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Variable Space Test ===\n",
      "✓ Variables set\n",
      "✓ Variables modified: {'output': {'items': [1, 2, 3, 4], 'count': 3}}\n",
      "\n",
      "✗ Variable space test failed: Variable persistence failed\n"
     ]
    }
   ],
   "source": [
    "# Test variable persistence\n",
    "print(\"=== Variable Space Test ===\")\n",
    "\n",
    "try:\n",
    "    interpreter = ModalInterpreter(timeout=60)\n",
    "    interpreter.start()\n",
    "\n",
    "    # Set variables\n",
    "    interpreter.execute(\"data = {'items': [1, 2, 3], 'count': 3}\")\n",
    "    print(\"✓ Variables set\")\n",
    "\n",
    "    # Modify and retrieve\n",
    "    result = interpreter.execute(\"data['items'].append(4)\\nSUBMIT(data)\")\n",
    "    # Use attribute access for FinalOutput\n",
    "    output = getattr(result, \"output\", result)\n",
    "\n",
    "    print(f\"✓ Variables modified: {output}\")\n",
    "    assert output.get(\"items\") == [1, 2, 3, 4], \"Variable persistence failed\"\n",
    "\n",
    "    interpreter.shutdown()\n",
    "    print(\"\\n✓ Variable space test passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Variable space test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Full dspy.RLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== dspy.RLM Integration Test ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 09:14:03 INFO dspy.predict.rlm: RLM iteration 1/5\n",
      "Reasoning: The question asks for the result of \"2 + 2\" calculated using Python. I will perform this calculation and print the result to confirm.\n",
      "Code:\n",
      "```python\n",
      "result = 2 + 2\n",
      "print(result)\n",
      "```\n",
      "2026/02/08 09:14:05 INFO dspy.predict.rlm: RLM iteration 2/5\n",
      "Reasoning: The calculation in the previous step confirmed that 2 + 2 = 4. I will now submit the answer.\n",
      "Code:\n",
      "```python\n",
      "SUBMIT(4)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 4\n",
      "Trajectory steps: 2\n",
      "\n",
      "✓ dspy.RLM integration test passed\n"
     ]
    }
   ],
   "source": [
    "# Test full dspy.RLM integration\n",
    "import dspy\n",
    "from fleet_rlm import ModalInterpreter, configure_planner_from_env\n",
    "\n",
    "print(\"=== dspy.RLM Integration Test ===\")\n",
    "\n",
    "try:\n",
    "    # Configure the LM from environment variables\n",
    "    configure_planner_from_env()\n",
    "\n",
    "    interpreter = ModalInterpreter(timeout=120)\n",
    "\n",
    "    rlm = dspy.RLM(\n",
    "        signature=\"question -> answer\",\n",
    "        interpreter=interpreter,\n",
    "        max_iterations=5,\n",
    "        max_llm_calls=10,\n",
    "        verbose=True,  # Show trajectory\n",
    "    )\n",
    "\n",
    "    result = rlm(question=\"What is 2 + 2? Calculate using Python.\")\n",
    "\n",
    "    print(f\"\\nAnswer: {result.answer}\")\n",
    "    print(f\"Trajectory steps: {len(getattr(result, 'trajectory', []))}\")\n",
    "\n",
    "    interpreter.shutdown()\n",
    "    print(\"\\n✓ dspy.RLM integration test passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ dspy.RLM integration test failed: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trajectory Visualization ===\n",
      "\n",
      "============================================================\n",
      "Step 1\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "I will begin by inspecting the content of the `text` variable to understand its structure and information, then I will use the `llm_query` function to generate a concise summary of the text....\n",
      "\n",
      "Code:\n",
      "print(f\"Text length: {len(text)}\")\n",
      "print(\"Content preview:\")\n",
      "print(text)\n",
      "\n",
      "prompt = f\"Summarize the following text in one short sentence:\\n\\n{text}\"\n",
      "summary = llm_query(prompt)\n",
      "print(f\"\\nGenerated Summ...\n",
      "\n",
      "Output: Text length: 158\n",
      "Content preview:\n",
      "\n",
      "    The quick brown fox jumps over the lazy dog.\n",
      "    This is a sample text for the RLM to analyze.\n",
      "    It should id...\n",
      "\n",
      "============================================================\n",
      "Step 2\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "The initial summary generated is accurate and captures the essence of the provided text. I have explored the input and verified the output from the `llm_query`. I am now ready to submit the final summary....\n",
      "\n",
      "Code:\n",
      "# The summary was generated in the previous step.\n",
      "# I will store it in a variable to ensure I pass the exact string to SUBMIT.\n",
      "final_summary = \"The text uses a famous pangram to demonstrate a language...\n",
      "\n",
      "Output: FINAL: {'summary': \"The text uses a famous pangram to demonstrate a language model's ability to identify and summarize information.\"}...\n"
     ]
    }
   ],
   "source": [
    "# Visualize RLM trajectory\n",
    "def visualize_trajectory(trajectory):\n",
    "    \"\"\"Pretty-print RLM trajectory for analysis.\"\"\"\n",
    "    if not trajectory:\n",
    "        print(\"No trajectory data\")\n",
    "        return\n",
    "\n",
    "    for i, step in enumerate(trajectory, 1):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Step {i}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        if \"reasoning\" in step:\n",
    "            reasoning = str(step[\"reasoning\"])[:300]\n",
    "            print(f\"\\nReasoning:\\n{reasoning}...\")\n",
    "\n",
    "        if \"code\" in step:\n",
    "            code = str(step[\"code\"])[:200]\n",
    "            print(f\"\\nCode:\\n{code}...\")\n",
    "\n",
    "        if \"output\" in step:\n",
    "            output = str(step[\"output\"])[:150]\n",
    "            print(f\"\\nOutput: {output}...\")\n",
    "\n",
    "\n",
    "# Run a task and visualize\n",
    "print(\"=== Trajectory Visualization ===\")\n",
    "\n",
    "try:\n",
    "    from fleet_rlm import configure_planner_from_env\n",
    "\n",
    "    # Configure the LM from environment variables\n",
    "    configure_planner_from_env()\n",
    "\n",
    "    interpreter = ModalInterpreter(timeout=120)\n",
    "\n",
    "    rlm = dspy.RLM(\n",
    "        signature=\"text -> summary\",\n",
    "        interpreter=interpreter,\n",
    "        max_iterations=10,\n",
    "        max_llm_calls=15,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Sample text\n",
    "    text = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog.\n",
    "    This is a sample text for the RLM to analyze.\n",
    "    It should identify key information and summarize.\n",
    "    \"\"\"\n",
    "\n",
    "    result = rlm(text=text)\n",
    "    trajectory = getattr(result, \"trajectory\", [])\n",
    "\n",
    "    visualize_trajectory(trajectory)\n",
    "\n",
    "    interpreter.shutdown()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Needle in Haystack Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Needle in Haystack Benchmark ===\n",
      "\n",
      "Result: /tmp/secret_needle.txt\n",
      "Iterations: 6\n",
      "Duration: 27.45s\n",
      "\n",
      "✗ Benchmark failed: Took 6 iterations, expected <= 5\n"
     ]
    }
   ],
   "source": [
    "# Classic RLM benchmark\n",
    "import time\n",
    "from fleet_rlm import configure_planner_from_env\n",
    "\n",
    "print(\"=== Needle in Haystack Benchmark ===\")\n",
    "\n",
    "try:\n",
    "    # Configure the LM from environment variables\n",
    "    configure_planner_from_env()\n",
    "\n",
    "    # Create large document\n",
    "    lines = [f\"Line {i}: filler content here\" for i in range(500)]\n",
    "    lines[250] = \"Line 250: SECRET_NEEDLE_FOUND_HERE\"\n",
    "    haystack = \"\\n\".join(lines)\n",
    "\n",
    "    interpreter = ModalInterpreter(timeout=180)\n",
    "    interpreter.start()\n",
    "    interpreter.execute(f\"docs = {haystack!r}\")\n",
    "\n",
    "    rlm = dspy.RLM(\n",
    "        signature=\"find -> location\",\n",
    "        interpreter=interpreter,\n",
    "        max_iterations=10,\n",
    "        max_llm_calls=15,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    result = rlm(find=\"SECRET_NEEDLE\")\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    trajectory = getattr(result, \"trajectory\", [])\n",
    "\n",
    "    print(f\"\\nResult: {result.location}\")\n",
    "    print(f\"Iterations: {len(trajectory)}\")\n",
    "    print(f\"Duration: {elapsed:.2f}s\")\n",
    "\n",
    "    # Validate\n",
    "    assert len(trajectory) <= 5, f\"Took {len(trajectory)} iterations, expected <= 5\"\n",
    "    assert elapsed < 60, f\"Took {elapsed}s, expected < 60s\"\n",
    "\n",
    "    interpreter.shutdown()\n",
    "    print(\"\\n✓ Benchmark passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Benchmark failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Volume Persistence Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Volume Persistence Test ===\n",
      "Session 1: Writing data...\n",
      "  Status: written\n",
      "\n",
      "Session 2: Reading data...\n",
      "  Data: {'output': {'test': 'persistence', 'timestamp': 1770538643.0586412}}\n",
      "\n",
      "✗ Volume persistence test failed: Volume persistence failed\n"
     ]
    }
   ],
   "source": [
    "# Test volume persistence across sessions\n",
    "print(\"=== Volume Persistence Test ===\")\n",
    "\n",
    "VOLUME_NAME = \"rlm-eval-test-volume\"\n",
    "\n",
    "try:\n",
    "    # Session 1: Write data\n",
    "    print(\"Session 1: Writing data...\")\n",
    "    interpreter1 = ModalInterpreter(timeout=60, volume_name=VOLUME_NAME)\n",
    "    interpreter1.start()\n",
    "\n",
    "    result1 = interpreter1.execute(\"\"\"\n",
    "import json\n",
    "data = {'test': 'persistence', 'timestamp': __import__('time').time()}\n",
    "with open('/data/persist_test.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "SUBMIT(status='written')\n",
    "\"\"\")\n",
    "\n",
    "    # Use attribute access for FinalOutput\n",
    "    output1 = getattr(result1, \"output\", result1)\n",
    "    print(f\"  Status: {output1.get('status')}\")\n",
    "    interpreter1.shutdown()\n",
    "\n",
    "    # Session 2: Read data\n",
    "    print(\"\\nSession 2: Reading data...\")\n",
    "    interpreter2 = ModalInterpreter(timeout=60, volume_name=VOLUME_NAME)\n",
    "    interpreter2.start()\n",
    "\n",
    "    result2 = interpreter2.execute(\"\"\"\n",
    "import json\n",
    "with open('/data/persist_test.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "SUBMIT(data)\n",
    "\"\"\")\n",
    "\n",
    "    # Use attribute access for FinalOutput\n",
    "    output2 = getattr(result2, \"output\", result2)\n",
    "    print(f\"  Data: {output2}\")\n",
    "    interpreter2.shutdown()\n",
    "\n",
    "    # Validate\n",
    "    assert output2.get(\"test\") == \"persistence\", \"Volume persistence failed\"\n",
    "    print(\"\\n✓ Volume persistence test passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Volume persistence test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Tool Registration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tool Registration Test ===\n",
      "Extracted headers: {'headers': ['Header 1', 'Header 2']}\n",
      "\n",
      "✓ Tool registration test passed\n"
     ]
    }
   ],
   "source": [
    "# Test custom tool registration\n",
    "print(\"=== Tool Registration Test ===\")\n",
    "\n",
    "try:\n",
    "    from fleet_rlm.tools import regex_extract\n",
    "\n",
    "    interpreter = ModalInterpreter(timeout=60)\n",
    "    # Register tools via the tools property BEFORE starting\n",
    "    interpreter.tools = {\"regex_extract\": regex_extract}\n",
    "    interpreter.start()\n",
    "\n",
    "    # Test built-in tool\n",
    "    text = \"\"\"\n",
    "# Header 1\n",
    "Content here\n",
    "# Header 2\n",
    "More content\n",
    "\"\"\"\n",
    "\n",
    "    result = interpreter.execute(f\"\"\"\n",
    "import re\n",
    "headers = regex_extract({text!r}, r'^# (.+)$', re.MULTILINE)\n",
    "SUBMIT(headers=headers)\n",
    "\"\"\")\n",
    "\n",
    "    # Use attribute access for FinalOutput\n",
    "    headers = getattr(result, \"output\", result)\n",
    "    print(f\"Extracted headers: {headers}\")\n",
    "\n",
    "    assert \"Header 1\" in str(headers)\n",
    "    assert \"Header 2\" in str(headers)\n",
    "\n",
    "    interpreter.shutdown()\n",
    "    print(\"\\n✓ Tool registration test passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Tool registration test failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fleet-rlm-dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
