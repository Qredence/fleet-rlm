{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f71b95b",
   "metadata": {},
   "source": [
    "# RLM with Modal Sandbox (DSPy 3.1.3)\n",
    "\n",
    "This tutorial shows how to use **`dspy.RLM`** (Recursive Language Model) with [Modal](https://modal.com) for secure, sandboxed code execution in the cloud.\n",
    "\n",
    "**What is RLM?** RLM is an inference strategy where the LLM writes Python code to programmatically explore data, call sub-LLMs over snippets, and iteratively build up answers — instead of feeding long contexts directly into the model.\n",
    "\n",
    "**Why Modal?** By default, `dspy.RLM` uses a local Deno/Pyodide WASM sandbox. Modal lets you run that code in an isolated cloud container with configurable resources, dependencies, and secrets.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Implement a `ModalInterpreter` that satisfies DSPy's `CodeInterpreter` protocol\n",
    "2. Use `modal.Sandbox` to execute code inside an ephemeral cloud container\n",
    "3. Run an RLM agent that writes and executes code remotely\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python 3.10+**\n",
    "- **Modal account**: Sign up at [modal.com](https://modal.com) and run `modal setup`\n",
    "- **Modal secret**: Create a secret named `LITELLM` that contains the environment variables used by DSPy/LiteLLM:\n",
    "  - `DSPY_LM_MODEL` (e.g., `openai/gemini-3-flash-preview`)\n",
    "  - `DSPY_LM_API_BASE` (your LiteLLM proxy base URL)\n",
    "  - `DSPY_LLM_API_KEY` (API key for the proxy/provider)\n",
    "  - optional: `DSPY_LM_MAX_TOKENS`\n",
    "\n",
    "  Example (run in a terminal):\n",
    "  ```bash\n",
    "  modal secret create LITELLM \\\n",
    "    DSPY_LM_MODEL=... \\\n",
    "    DSPY_LM_API_BASE=... \\\n",
    "    DSPY_LLM_API_KEY=... \\\n",
    "    DSPY_LM_MAX_TOKENS=...\n",
    "  ```\n",
    "\n",
    "- **Security note**: don’t hard-code API keys in notebooks, and don’t print them. If a key was ever pasted into a notebook/chat, rotate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc01761",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4649fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/_WORLD/_RLM/fleet-rlm-dspy/rlm_content/.venv/bin/python: No module named uv\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install -qU \"dspy==3.1.3\" modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6451ed",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "We configure one LM locally for the *planner* (the model that writes Python code each iteration).\n",
    "\n",
    "This notebook expects the following environment variables to be set **locally** (for the planner):\n",
    "- `DSPY_LM_MODEL`\n",
    "- `DSPY_LM_API_BASE`\n",
    "- `DSPY_LLM_API_KEY`\n",
    "- optional: `DSPY_LM_MAX_TOKENS`\n",
    "\n",
    "The same variables are also injected into the Modal sandbox via the `LITELLM` secret, so any sandbox-side LM calls (via tool-bridged `llm_query`) use identical credentials without hard-coding secrets in the notebook.\n",
    "\n",
    "**Important**: Modal secrets are only available *inside* Modal containers/sandboxes. They do **not** automatically set environment variables for your local notebook kernel.\n",
    "This notebook will try to load a local `.env` from the project root (if present) to configure the planner LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fe5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planner LM configured: openai/gemini-3-flash-preview\n",
      "(Tip: don’t print API keys.)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Iterator\n",
    "\n",
    "import dspy\n",
    "\n",
    "# ---- Load local .env (for the planner LM) ----\n",
    "# Modal secrets are only available *inside* Modal; they do not configure your local kernel.\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "def _load_dotenv(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        return\n",
    "    try:\n",
    "        for raw in path.read_text().splitlines():\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "                continue\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            k, v = k.strip(), v.strip()\n",
    "            if len(v) >= 2 and ((v[0] == v[-1] == '\\\"') or (v[0] == v[-1] == \"'\")):\n",
    "                v = v[1:-1]\n",
    "            if k and k not in os.environ:\n",
    "                os.environ[k] = v\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not load {path}: {e}\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root(Path.cwd())\n",
    "_load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# ---- Guard against module shadowing ----\n",
    "# A local `modal.py` (or even a stale compiled `__pycache__/modal.*.pyc`) in the\n",
    "# notebook's working directory can shadow the third-party `modal` package.\n",
    "shadow_py = Path.cwd() / \"modal.py\"\n",
    "shadow_pyc_dir = Path.cwd() / \"__pycache__\"\n",
    "shadow_pycs = list(shadow_pyc_dir.glob(\"modal.*.pyc\")) if shadow_pyc_dir.exists() else []\n",
    "\n",
    "if shadow_py.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Found {shadow_py} which shadows the 'modal' package. \"\n",
    "        \"Rename/delete it (e.g., modal_get_started.py) and restart the kernel.\"\n",
    "    )\n",
    "\n",
    "if shadow_pycs:\n",
    "    removed: list[str] = []\n",
    "    failed: list[str] = []\n",
    "    for p in shadow_pycs:\n",
    "        try:\n",
    "            p.unlink()\n",
    "            removed.append(str(p))\n",
    "        except Exception:\n",
    "            failed.append(str(p))\n",
    "\n",
    "    if removed:\n",
    "        print(\"Removed shadowing bytecode files:\\n\" + \"\\n\".join(removed))\n",
    "    if failed:\n",
    "        raise RuntimeError(\n",
    "            \"Found shadowing bytecode files but could not remove them:\\n\"\n",
    "            + \"\\n\".join(failed)\n",
    "            + \"\\nDelete them manually and restart the kernel.\"\n",
    "        )\n",
    "\n",
    "# If a previous import attempt loaded a bad `modal` module, clear modal-related\n",
    "# modules to avoid weird partially-initialized states.\n",
    "#\n",
    "# Note: Modal uses a generated `modal_proto` package under the hood; when upgrading\n",
    "# modal in a running kernel, stale `modal_proto` modules can cause type mismatches.\n",
    "MODULE_PREFIXES_TO_PURGE = (\n",
    "    \"modal\",\n",
    "    \"modal_proto\",\n",
    "    \"grpclib\",\n",
    ")\n",
    "\n",
    "for name in list(sys.modules.keys()):\n",
    "    if name in MODULE_PREFIXES_TO_PURGE or any(name.startswith(p + \".\") for p in MODULE_PREFIXES_TO_PURGE):\n",
    "        sys.modules.pop(name, None)\n",
    "\n",
    "import modal\n",
    "from dspy.primitives.code_interpreter import CodeInterpreterError, FinalOutput\n",
    "\n",
    "\n",
    "def configure_planner_from_env() -> bool:\n",
    "    \"\"\"Configure DSPy planner LM from environment variables.\n",
    "\n",
    "    Expected (local):\n",
    "      - DSPY_LM_MODEL\n",
    "      - DSPY_LLM_API_KEY (or DSPY_LM_API_KEY)\n",
    "      - optional: DSPY_LM_API_BASE, DSPY_LM_MAX_TOKENS\n",
    "\n",
    "    Returns True if configured, False if required env vars are missing.\n",
    "    \"\"\"\n",
    "\n",
    "    api_key = os.environ.get(\"DSPY_LLM_API_KEY\") or os.environ.get(\"DSPY_LM_API_KEY\")\n",
    "    missing: list[str] = []\n",
    "    if not os.environ.get(\"DSPY_LM_MODEL\"):\n",
    "        missing.append(\"DSPY_LM_MODEL\")\n",
    "    if not api_key:\n",
    "        # DSPy expects DSPY_LLM_API_KEY, but some setups use DSPY_LM_API_KEY.\n",
    "        missing.append(\"DSPY_LLM_API_KEY\")\n",
    "    if missing:\n",
    "        print(\n",
    "            \"Planner LM not configured yet. Missing env vars: \"\n",
    "            + \", \".join(missing)\n",
    "            + \"\\nSet them locally (e.g., export in your shell before starting Jupyter, or create a .env at the project root) and re-run this cell.\" \n",
    "        )\n",
    "        return False\n",
    "\n",
    "    planner_lm = dspy.LM(\n",
    "        os.environ[\"DSPY_LM_MODEL\"],\n",
    "        api_base=os.environ.get(\"DSPY_LM_API_BASE\"),\n",
    "        api_key=api_key,\n",
    "        max_tokens=int(os.environ.get(\"DSPY_LM_MAX_TOKENS\", \"16000\")),\n",
    "    )\n",
    "\n",
    "    dspy.configure(lm=planner_lm)\n",
    "    print(f\"Planner LM configured: {planner_lm.model}\")\n",
    "    print(\"(Tip: don’t print API keys.)\")\n",
    "    return True\n",
    "\n",
    "\n",
    "PLANNER_READY = configure_planner_from_env()\n",
    "\n",
    "# We’ll pass `modal.Secret.from_name('LITELLM')` into the sandbox so the *remote*\n",
    "# Python REPL can access the same environment variables without hard-coding them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d05203",
   "metadata": {},
   "source": [
    "### Optional: sanity-check the Modal secret (without leaking it)\n",
    "\n",
    "The snippet below confirms that the `LITELLM` secret is mounted in Modal by checking for the *presence* of environment variables. It deliberately does **not** print secret values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030754c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret env presence: {\"DSPY_LM_MODEL\": true, \"DSPY_LM_API_BASE\": true, \"DSPY_LLM_API_KEY\": true, \"DSPY_LM_MAX_TOKENS\": true}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import modal\n",
    "\n",
    "# Sandboxes require an App when created from a local environment.\n",
    "app = modal.App.lookup(\"dspy-rlm-secret-check\", create_if_missing=True)\n",
    "\n",
    "sb = modal.Sandbox.create(app=app, secrets=[modal.Secret.from_name(\"LITELLM\")])\n",
    "try:\n",
    "    code = r\"\"\"\n",
    "import json, os\n",
    "keys = [\n",
    "  'DSPY_LM_MODEL',\n",
    "  'DSPY_LM_API_BASE',\n",
    "  'DSPY_LLM_API_KEY',\n",
    "  'DSPY_LM_MAX_TOKENS',\n",
    "]\n",
    "print(json.dumps({k: bool(os.environ.get(k)) for k in keys}))\n",
    "\"\"\"\n",
    "    p = sb.exec(\"python\", \"-c\", code, timeout=60)\n",
    "    p.wait()\n",
    "    print(\"Secret env presence:\", p.stdout.read().strip())\n",
    "finally:\n",
    "    sb.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aca65b",
   "metadata": {},
   "source": [
    "### Don’t print secrets\n",
    "\n",
    "This is **unsafe**:\n",
    "- `print(os.environ[\"DSPY_LLM_API_KEY\"])`\n",
    "\n",
    "Instead, verify the secret is present (and optionally its length), without revealing the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fdf1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPY_LLM_API_KEY: {\"present\": true, \"length\": 67}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import modal\n",
    "\n",
    "app = modal.App.lookup(\"dspy-rlm-secret-check\", create_if_missing=True)\n",
    "\n",
    "sb = modal.Sandbox.create(app=app, secrets=[modal.Secret.from_name(\"LITELLM\")])\n",
    "try:\n",
    "    code = r\"\"\"\n",
    "import json, os\n",
    "key = os.environ.get('DSPY_LLM_API_KEY', '')\n",
    "print(json.dumps({'present': bool(key), 'length': len(key)}))\n",
    "\"\"\"\n",
    "    p = sb.exec(\"python\", \"-c\", code, timeout=60)\n",
    "    p.wait()\n",
    "    print(\"DSPY_LLM_API_KEY:\", p.stdout.read().strip())\n",
    "finally:\n",
    "    sb.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039c470",
   "metadata": {},
   "source": [
    "## 3. The Modal Sandbox Driver\n",
    "\n",
    "Modal Sandboxes are ephemeral containers. We use a **driver program** pattern (from [Modal's code interpreter example](https://modal.com/docs/examples/simple_code_interpreter)):\n",
    "\n",
    "1. A Python driver script runs inside the sandbox, reading JSON commands from `stdin`.\n",
    "2. For each command, it `exec()`s the code, captures stdout/stderr, and checks for `SUBMIT()` calls.\n",
    "3. It writes the result as JSON to `stdout`.\n",
    "\n",
    "This keeps state between iterations (variables persist in the `globals` dict) — exactly what RLM needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff761307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver function defined.\n"
     ]
    }
   ],
   "source": [
    "def sandbox_driver():\n",
    "    \"\"\"Driver program that runs inside the Modal sandbox container.\n",
    "\n",
    "    Protocol:\n",
    "    - Host sends one JSON line: {code, variables, tool_names, output_names}\n",
    "    - Driver executes `code` (stateful globals), capturing stdout/stderr.\n",
    "    - If executed code calls a tool like llm_query(), the driver emits a JSON\n",
    "      tool call request to *real* stdout, then blocks reading one JSON tool\n",
    "      response line from stdin.\n",
    "    - At the end, driver emits one JSON line: {stdout, stderr, final}\n",
    "\n",
    "    This mirrors DSPy's local sandbox tool-bridge pattern (see runner.js).\n",
    "    \"\"\"\n",
    "\n",
    "    import json\n",
    "    import sys\n",
    "    from contextlib import redirect_stderr, redirect_stdout\n",
    "    from io import StringIO\n",
    "    from typing import Any\n",
    "\n",
    "    # Persistent state across execute() calls.\n",
    "    sandbox_globals: dict[str, Any] = {}\n",
    "\n",
    "    # Protocol IO that bypasses redirected stdout/stderr.\n",
    "    proto_out = sys.__stdout__\n",
    "\n",
    "    # Set on each command by host.\n",
    "    output_names: list[str] = []\n",
    "    tool_names: list[str] = []\n",
    "\n",
    "    class _FinalOutput(BaseException):\n",
    "        pass\n",
    "\n",
    "    def _send(obj: dict) -> None:\n",
    "        proto_out.write(json.dumps(obj) + \"\\n\")\n",
    "        proto_out.flush()\n",
    "\n",
    "    def _tool_call(name: str, *args, **kwargs):\n",
    "        _send({\"tool_call\": {\"name\": name, \"args\": list(args), \"kwargs\": kwargs}})\n",
    "        # Host replies with {tool_result} or {tool_error}\n",
    "        reply = json.loads(input())\n",
    "        if reply.get(\"tool_error\"):\n",
    "            raise RuntimeError(reply[\"tool_error\"])\n",
    "        return reply.get(\"tool_result\")\n",
    "\n",
    "    def _register_tools(names: list[str]) -> None:\n",
    "        # Create callable stubs in sandbox_globals for every tool name.\n",
    "        for n in names:\n",
    "            if not n.isidentifier() or n in {\"SUBMIT\"}:\n",
    "                continue\n",
    "            if n in sandbox_globals:\n",
    "                continue\n",
    "\n",
    "            def _make(n_: str):\n",
    "                def _fn(*args, **kwargs):\n",
    "                    return _tool_call(n_, *args, **kwargs)\n",
    "\n",
    "                return _fn\n",
    "\n",
    "            sandbox_globals[n] = _make(n)\n",
    "\n",
    "    def SUBMIT(*args, **kwargs):\n",
    "        \"\"\"Signal completion.\n",
    "\n",
    "        DSPy generates SUBMIT(output1, output2, ...) with positional args,\n",
    "        where the position maps to the signature output fields.\n",
    "\n",
    "        We also support SUBMIT(field=value, ...) for convenience.\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            raise _FinalOutput(kwargs)\n",
    "\n",
    "        if not output_names:\n",
    "            # Fallback (should not happen if host provides output_names)\n",
    "            if len(args) == 1:\n",
    "                raise _FinalOutput({\"output\": args[0]})\n",
    "            raise _FinalOutput({\"output\": list(args)})\n",
    "\n",
    "        if len(args) != len(output_names):\n",
    "            raise _FinalOutput({\n",
    "                \"error\": f\"SUBMIT expected {len(output_names)} positional values ({output_names}), got {len(args)}\"\n",
    "            })\n",
    "\n",
    "        raise _FinalOutput(dict(zip(output_names, args)))\n",
    "\n",
    "    sandbox_globals[\"SUBMIT\"] = SUBMIT\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            line = input()  # Next command from host (or EOF)\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            command = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            _send({\"stdout\": \"\", \"stderr\": f\"[Error] Invalid JSON: {e}\", \"final\": None})\n",
    "            continue\n",
    "\n",
    "        code = command.get(\"code\")\n",
    "        variables = command.get(\"variables\", {}) or {}\n",
    "        tool_names = list(command.get(\"tool_names\", []) or [])\n",
    "        output_names = list(command.get(\"output_names\", []) or [])\n",
    "\n",
    "        if code is None:\n",
    "            _send({\"stdout\": \"\", \"stderr\": \"[Error] No code provided\", \"final\": None})\n",
    "            continue\n",
    "\n",
    "        # Inject variables and tool stubs into globals.\n",
    "        sandbox_globals.update(variables)\n",
    "        _register_tools(tool_names)\n",
    "\n",
    "        # Execute and capture stdout/stderr.\n",
    "        stdout_io, stderr_io = StringIO(), StringIO()\n",
    "        final_obj = None\n",
    "        with redirect_stdout(stdout_io), redirect_stderr(stderr_io):\n",
    "            try:\n",
    "                exec(code, sandbox_globals)\n",
    "            except _FinalOutput as e:\n",
    "                final_obj = e.args[0] if e.args else None\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {type(e).__name__}: {e}\", file=sys.stderr)\n",
    "\n",
    "        _send({\"stdout\": stdout_io.getvalue(), \"stderr\": stderr_io.getvalue(), \"final\": final_obj})\n",
    "\n",
    "\n",
    "print(\"Driver function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d3b08",
   "metadata": {},
   "source": [
    "## 4. Implement the `ModalInterpreter`\n",
    "\n",
    "This class implements DSPy's [`CodeInterpreter`](https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/code_interpreter.py) protocol. The protocol requires:\n",
    "\n",
    "| Method | Purpose |\n",
    "|---|---|\n",
    "| `tools` (property) | Dict of callable tools available in the sandbox |\n",
    "| `start()` | Initialize resources (idempotent) |\n",
    "| `execute(code, variables)` | Run code, return stdout or `FinalOutput` |\n",
    "| `shutdown()` | Release resources |\n",
    "\n",
    "Our implementation creates a `modal.Sandbox`, launches the driver program, and communicates via stdin/stdout JSON messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85af30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModalInterpreter defined.\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "# Modal sandbox image — add any packages your RLM code might need.\n",
    "# (The sandbox is just a Python REPL. Your RLM-written code can `import` these.)\n",
    "SANDBOX_IMAGE = modal.Image.debian_slim(python_version=\"3.12\").pip_install(\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    ")\n",
    "\n",
    "# Reference a pre-existing Modal App (creates if missing)\n",
    "MODAL_APP = modal.App.lookup(\"dspy-rlm-interpreter\", create_if_missing=True)\n",
    "\n",
    "\n",
    "class ModalInterpreter:\n",
    "    \"\"\"CodeInterpreter that executes code in a Modal Sandbox.\n",
    "\n",
    "    - Maintains sandbox state across `execute()` calls (a persistent driver process).\n",
    "    - Bridges DSPy tools (llm_query, llm_query_batched, and any custom tools) by\n",
    "      relaying tool-call requests from the sandbox back to the host.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image: modal.Image = SANDBOX_IMAGE,\n",
    "        app: modal.App = MODAL_APP,\n",
    "        secrets: list[modal.Secret] | None = None,\n",
    "        timeout: int = 600,\n",
    "    ):\n",
    "        self.image = image\n",
    "        self.app = app\n",
    "        self.secrets = secrets or [modal.Secret.from_name(\"LITELLM\")]\n",
    "        self.timeout = timeout\n",
    "\n",
    "        # Set by RLM on every forward() via _inject_execution_context\n",
    "        self.output_fields: list[dict] | None = None\n",
    "        self._tools_registered = False\n",
    "\n",
    "        # Interpreter state\n",
    "        self._sandbox: modal.Sandbox | None = None\n",
    "        self._proc = None\n",
    "        self._stdin = None\n",
    "        self._stdout_iter: Iterator[str] | None = None\n",
    "        self._stderr_iter: Iterator[str] | None = None\n",
    "        self._tools: dict[str, Callable[..., str]] = {}\n",
    "\n",
    "    # ── CodeInterpreter protocol ─────────────────────────────────────\n",
    "\n",
    "    @property\n",
    "    def tools(self) -> dict[str, Callable[..., str]]:\n",
    "        return self._tools\n",
    "\n",
    "    @tools.setter\n",
    "    def tools(self, value: dict[str, Callable[..., str]]) -> None:\n",
    "        self._tools = value\n",
    "\n",
    "    def start(self) -> None:\n",
    "        \"\"\"Create the Modal Sandbox and launch the driver process (idempotent).\"\"\"\n",
    "        if self._sandbox is not None:\n",
    "            return\n",
    "\n",
    "        driver_source = inspect.getsource(sandbox_driver)\n",
    "        driver_command = f\"{driver_source}\\n\\nsandbox_driver()\"\n",
    "\n",
    "        self._sandbox = modal.Sandbox.create(\n",
    "            app=self.app,\n",
    "            image=self.image,\n",
    "            secrets=self.secrets,\n",
    "        )\n",
    "\n",
    "        # Start a long-lived python process inside the sandbox.\n",
    "        # bufsize=1 enables line buffering for stdout.\n",
    "        self._proc = self._sandbox.exec(\n",
    "            \"python\",\n",
    "            \"-u\",\n",
    "            \"-c\",\n",
    "            driver_command,\n",
    "            bufsize=1,\n",
    "            timeout=self.timeout,\n",
    "        )\n",
    "\n",
    "        self._stdin = self._proc.stdin\n",
    "        self._stdout_iter = iter(self._proc.stdout)\n",
    "        self._stderr_iter = iter(getattr(self._proc, \"stderr\", []))\n",
    "\n",
    "    def _tool_names(self) -> list[str]:\n",
    "        return list(self._tools.keys()) if self._tools else []\n",
    "\n",
    "    def _output_names(self) -> list[str]:\n",
    "        if not self.output_fields:\n",
    "            return []\n",
    "        return [d[\"name\"] for d in self.output_fields if isinstance(d, dict) and d.get(\"name\")]\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        code: str,\n",
    "        variables: dict[str, Any] | None = None,\n",
    "    ) -> str | FinalOutput:\n",
    "        if self._sandbox is None:\n",
    "            self.start()\n",
    "\n",
    "        # Keep variables JSON-serializable\n",
    "        safe_vars: dict[str, Any] = {}\n",
    "        if variables:\n",
    "            for k, v in variables.items():\n",
    "                if isinstance(v, (str, int, float, bool, list, dict, type(None))):\n",
    "                    safe_vars[k] = v\n",
    "                else:\n",
    "                    safe_vars[k] = str(v)\n",
    "\n",
    "        payload = {\n",
    "            \"code\": code,\n",
    "            \"variables\": safe_vars,\n",
    "            \"tool_names\": self._tool_names(),\n",
    "            \"output_names\": self._output_names(),\n",
    "        }\n",
    "\n",
    "        self._stdin.write(json.dumps(payload) + \"\\n\")\n",
    "        self._stdin.drain()\n",
    "\n",
    "        # Read messages until we get the final result.\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(self._stdout_iter)\n",
    "            except StopIteration:\n",
    "                # Try to surface sandbox stderr for debugging\n",
    "                stderr_tail = \"\"\n",
    "                try:\n",
    "                    stderr_tail = \"\".join(list(self._stderr_iter)[:50])\n",
    "                except Exception:\n",
    "                    pass\n",
    "                raise CodeInterpreterError(\n",
    "                    \"Modal sandbox process exited unexpectedly.\" + (f\"\\nStderr: {stderr_tail}\" if stderr_tail else \"\")\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                msg = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Ignore non-JSON chatter\n",
    "                continue\n",
    "\n",
    "            # Tool call request from sandbox\n",
    "            if \"tool_call\" in msg:\n",
    "                call = msg[\"tool_call\"] or {}\n",
    "                name = call.get(\"name\")\n",
    "                args = call.get(\"args\") or []\n",
    "                kwargs = call.get(\"kwargs\") or {}\n",
    "\n",
    "                try:\n",
    "                    if not name or name not in self._tools:\n",
    "                        raise CodeInterpreterError(f\"Unknown tool: {name}\")\n",
    "                    result = self._tools[name](*args, **kwargs)\n",
    "                    # Ensure JSON serializable\n",
    "                    try:\n",
    "                        json.dumps(result)\n",
    "                        reply = {\"tool_result\": result}\n",
    "                    except TypeError:\n",
    "                        reply = {\"tool_result\": str(result)}\n",
    "                except Exception as e:\n",
    "                    reply = {\"tool_error\": f\"{type(e).__name__}: {e}\"}\n",
    "\n",
    "                self._stdin.write(json.dumps(reply) + \"\\n\")\n",
    "                self._stdin.drain()\n",
    "                continue\n",
    "\n",
    "            # Final result from sandbox\n",
    "            if \"stdout\" in msg or \"stderr\" in msg or \"final\" in msg:\n",
    "                stdout = msg.get(\"stdout\", \"\") or \"\"\n",
    "                stderr = msg.get(\"stderr\", \"\") or \"\"\n",
    "                final_obj = msg.get(\"final\")\n",
    "\n",
    "                if final_obj is not None:\n",
    "                    return FinalOutput(final_obj)\n",
    "\n",
    "                out = stdout\n",
    "                if stderr:\n",
    "                    out = out + (\"\\n\" if out else \"\") + stderr\n",
    "                return out\n",
    "\n",
    "            # Unknown message type; ignore\n",
    "\n",
    "    def shutdown(self) -> None:\n",
    "        if self._sandbox is not None:\n",
    "            try:\n",
    "                self._sandbox.terminate()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self._sandbox = None\n",
    "            self._proc = None\n",
    "            self._stdin = None\n",
    "            self._stdout_iter = None\n",
    "            self._stderr_iter = None\n",
    "\n",
    "\n",
    "print(\"ModalInterpreter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Basic RLM Demo: Code Generation\n",
    "\n",
    "A simple example showing RLM writing Python code to solve a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "basic-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 17:28:37 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: The question asks for the first 12 Fibonacci numbers, separated by commas. I will write a simple Python script to calculate these numbers. By convention, the Fibonacci sequence starts with 0 and 1, or sometimes 1 and 1. I will provide the sequence starting from 0, 1, 1, 2... and confirm if 12 numbers are generated.\n",
      "Code:\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    while len(fib_sequence) < n:\n",
      "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "    return fib_sequence[:n]\n",
      "\n",
      "first_12 = fibonacci(12)\n",
      "result = \", \".join(map(str, first_12))\n",
      "print(result)\n",
      "```\n",
      "2026/02/07 17:28:39 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The previous step successfully calculated the first 12 Fibonacci numbers starting from 0: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89. I will now submit this result as requested.\n",
      "Code:\n",
      "```python\n",
      "SUBMIT(\"0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\")\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL ANSWER: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\n"
     ]
    }
   ],
   "source": [
    "# Ensure the planner LM is configured\n",
    "if not PLANNER_READY and dspy.settings.lm is None:\n",
    "    raise RuntimeError(\"Planner LM not configured\")\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=\"question -> answer\",\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=15,\n",
    "    max_llm_calls=30,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(question=\"What are the first 12 Fibonacci numbers? Return as comma-separated.\")\n",
    "    print(\"\\nFINAL ANSWER:\", result.answer)\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Core Capability: Long Document Analysis\n",
    "\n",
    "RLM treats long documents as an external environment. The document lives in the sandbox,\n",
    "code navigates and extracts relevant sections, and only snippets are sent to llm_query().\n",
    "\n",
    "### Use Case: Extract DSPy Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "long-doc-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 81,397 chars, 3,124 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 17:31:30 INFO dspy.predict.rlm: RLM iteration 1/25\n",
      "Reasoning: The goal is to extract architectural information from the DSPy documentation: modules, optimizers, and design principles.\n",
      "\n",
      "I will start by exploring the `docs` string to understand its structure and content. The preview shows a directory structure, which suggests the documentation contains names of files and possibly their contents. I need to see how the content is organized to effectively extract the lists of modules and optimizers.\n",
      "\n",
      "I'll begin by printing the first 2000 characters and checking for specific keywords like \"Module\", \"Optimizer\", \"Teleprompter\" (the old name for optimizers), and \"Design Principles\". I'll also check if there are explicit sections for these.\n",
      "\n",
      "Plan:\n",
      "1. Examine the beginning and general structure of `docs`.\n",
      "2. Look for keywords related to modules and optimizers.\n",
      "3. Identify where design principles might be discussed.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Docs length: {len(docs)}\")\n",
      "print(\"--- First 2000 characters ---\")\n",
      "print(docs[:2000])\n",
      "\n",
      "# Search for potential sections or keywords\n",
      "keywords = [\"Module\", \"Optimizer\", \"Teleprompter\", \"Design Principle\"]\n",
      "for kw in keywords:\n",
      "    count = docs.lower().count(kw.lower())\n",
      "    print(f\"Keyword '{kw}' count: {count}\")\n",
      "```\n",
      "2026/02/07 17:31:37 INFO dspy.predict.rlm: RLM iteration 2/25\n",
      "Reasoning: The directory structure in the documentation preview is very revealing. It shows explicit subdirectories for `api/modules/` and `api/optimizers/`. \n",
      "\n",
      "Modules listed in the directory:\n",
      "- `BestOfN`, `ChainOfThought`, `Module`, `MultiChainComparison`, `Parallel`, `Predict`, `ProgramOfThought`, `ReAct`, `Refine`.\n",
      "\n",
      "Optimizers listed in the directory:\n",
      "- `BetterTogether`, `BootstrapFewShot`, `BootstrapFewShotWithRandomSearch`, `BootstrapFinetune`, `BootstrapRS`, `COPRO`, `Ensemble`, `InferRules`, `KNN`, `KNNFewShot`, `LabeledFewShot`.\n",
      "\n",
      "I also see `MIPRO.md` and `MIPROv2.md` likely exist further down (I should check the rest of the file list or search for \"Optimizer\" in the text).\n",
      "\n",
      "However, I need to find the \"Design Principles\". Since the keyword search for \"Design Principle\" returned 0, I'll search for synonyms like \"Philosophy\", \"Core\", \"Principles\", \"Programming\", \"Architecture\" to find the underlying design philosophy of DSPy (which usually revolves around separating logic from LM weights, modularity, and optimization/compilation).\n",
      "\n",
      "I will:\n",
      "1. Extract the full directory list to ensure no modules/optimizers were missed.\n",
      "2. Search for sections talking about the core principles of DSPy.\n",
      "Code:\n",
      "```python\n",
      "# Extract all lines that look like file paths in the api/modules and api/optimizers folders\n",
      "import re\n",
      "\n",
      "modules_files = re.findall(r'modules/([\\w\\d]+)\\.md', docs)\n",
      "optimizers_files = re.findall(r'optimizers/([\\w\\d]+)\\.md', docs)\n",
      "\n",
      "print(\"Found Modules in paths:\", set(modules_files))\n",
      "print(\"Found Optimizers in paths:\", set(optimizers_files))\n",
      "\n",
      "# Search for \"Design Principles\" or similar sections\n",
      "# Looking for headings or blocks of text that might contain philosophy\n",
      "search_terms = [\"philosophy\", \"principles\", \"how dspy works\", \"core concepts\"]\n",
      "for term in search_terms:\n",
      "    index = docs.lower().find(term)\n",
      "    if index != -1:\n",
      "        print(f\"\\n--- Context for '{term}' (index {index}) ---\")\n",
      "        print(docs[index:index+1000])\n",
      "\n",
      "# Also check for 'Teleprompter' since that's the old name for optimizers, \n",
      "# there might be more listed under that name in the text.\n",
      "teleprompters = re.findall(r'[\\w\\d]+(?:\\s+teleprompter|Teleprompter)', docs)\n",
      "print(\"\\nTeleprompters mentioned:\", set(teleprompters))\n",
      "```\n",
      "2026/02/07 17:31:43 INFO dspy.predict.rlm: RLM iteration 3/25\n",
      "Reasoning: The directory structure has provided a solid list of modules and optimizers. However, I noticed that `MIPRO` and `MIPROv2` are common DSPy optimizers and might be missing from the initial directory preview if the list was truncated. I also need to find the \"Design Principles.\" Since my previous keyword search for \"philosophy\" and \"principles\" didn't yield immediate results in the first part of the doc, I will search the entire `docs` for paragraphs containing \"principles\" or \"separation of\" (as DSPy often emphasizes the separation of logic and parameters). I'll also check for \"signature\" as it's a core concept.\n",
      "\n",
      "I will:\n",
      "1. Search for `MIPRO` to see if it's in the documentation text even if not in the first few directory lines.\n",
      "2. Use `llm_query` to identify the design principles from the text, as these are often semantic rather than a bulleted list titled \"Design Principles.\"\n",
      "3. Broaden the search for optimizers in the text content.\n",
      "Code:\n",
      "```python\n",
      "# Search for more optimizers that might not be in the directory list\n",
      "additional_optimizers = re.findall(r'MIPRO(?:v2)?', docs)\n",
      "print(f\"Additional optimizers found: {set(additional_optimizers)}\")\n",
      "\n",
      "# Search for potential design principle sections\n",
      "# I'll look for sections that talk about the core \"vision\" or \"architecture\"\n",
      "principles_query = \"\"\"\n",
      "Based on the following text, what are the core design principles of DSPy? \n",
      "Look for things like 'separation of concerns', 'declarative signatures', 'modular programs', and 'automated optimization'.\n",
      "Text:\n",
      "\"\"\" + docs[:10000] # Checking the intro/index first\n",
      "\n",
      "print(\"\\n--- Querying LLM for design principles ---\")\n",
      "design_principles = llm_query(principles_query)\n",
      "print(design_principles)\n",
      "\n",
      "# Check for a more complete list of optimizers and modules in the text\n",
      "search_text_for_lists = \"\"\"\n",
      "Extract a clean list of all DSPy Modules and all DSPy Optimizers mentioned in this text.\n",
      "Text:\n",
      "\"\"\" + docs[:15000]\n",
      "list_extraction = llm_query(search_text_for_lists)\n",
      "print(\"\\n--- LLM List Extraction ---\")\n",
      "print(list_extraction)\n",
      "```\n",
      "2026/02/07 17:31:57 INFO dspy.predict.rlm: RLM iteration 4/25\n",
      "Reasoning: The exploration of the documentation via the directory structure and initial LLM analysis has provided a comprehensive list of modules and optimizers. \n",
      "\n",
      "Modules identified: `BestOfN`, `ChainOfThought`, `Module`, `MultiChainComparison`, `Parallel`, `Predict`, `ProgramOfThought`, `ReAct`, `Refine`.\n",
      "Optimizers identified: `BetterTogether`, `BootstrapFewShot`, `BootstrapFewShotWithRandomSearch`, `BootstrapFinetune`, `BootstrapRS`, `COPRO`, `Ensemble`, `InferRules`, `KNN`, `KNNFewShot`, `LabeledFewShot`.\n",
      "\n",
      "Regarding \"Design Principles\", the LLM analysis of the documentation structure and content identified five core principles:\n",
      "1. Separation of Concerns (Logic vs. Implementation).\n",
      "2. Declarative Signatures (Defining 'what' instead of 'how').\n",
      "3. Modular Programs (Composable LM pipelines).\n",
      "4. Automated Optimization (Teleprompting/Optimizers instead of manual prompting).\n",
      "5. Systematic Evaluation (Metric-driven development).\n",
      "\n",
      "I have sufficient information to construct the final output. I will format the modules and optimizers as arrays and provide a summary of the design principles.\n",
      "Code:\n",
      "```python\n",
      "modules = [\n",
      "    \"BestOfN\", \n",
      "    \"ChainOfThought\", \n",
      "    \"Module\", \n",
      "    \"MultiChainComparison\", \n",
      "    \"Parallel\", \n",
      "    \"Predict\", \n",
      "    \"ProgramOfThought\", \n",
      "    \"ReAct\", \n",
      "    \"Refine\"\n",
      "]\n",
      "\n",
      "optimizers = [\n",
      "    \"BetterTogether\", \n",
      "    \"BootstrapFewShot\", \n",
      "    \"BootstrapFewShotWithRandomSearch\", \n",
      "    \"BootstrapFinetune\", \n",
      "    \"BootstrapRS\", \n",
      "    \"COPRO\", \n",
      "    \"Ensemble\", \n",
      "    \"InferRules\", \n",
      "    \"KNN\", \n",
      "    \"KNNFewShot\", \n",
      "    \"LabeledFewShot\"\n",
      "]\n",
      "\n",
      "design_principles = \"\"\"\n",
      "The core design principles of DSPy include:\n",
      "1. Separation of Concerns: Separating the program flow (modules) from the data/parameters (prompts/weights) and the optimization process.\n",
      "2. Declarative Signatures: Defining tasks via input/output behavior (Signatures) rather than manual prompt strings.\n",
      "3. Modularity: Building complex systems by composing modular, reusable components (Modules) that inherit from a common base.\n",
      "4. Automated Optimization: Using Optimizers (formerly Teleprompters) to automatically generate and refine prompts or fine-tune models based on a metric.\n",
      "5. Systematic Evaluation: Integrating rigorous metrics and evaluation scripts to drive the optimization loop.\n",
      "\"\"\"\n",
      "\n",
      "SUBMIT(modules, optimizers, design_principles)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules: ['BestOfN', 'ChainOfThought', 'Module', 'MultiChainComparison', 'Parallel', 'Predict', 'ProgramOfThought', 'ReAct', 'Refine']\n",
      "Optimizers: ['BetterTogether', 'BootstrapFewShot', 'BootstrapFewShotWithRandomSearch', 'BootstrapFinetune', 'BootstrapRS', 'COPRO', 'Ensemble', 'InferRules', 'KNN', 'KNNFewShot', 'LabeledFewShot']\n"
     ]
    }
   ],
   "source": [
    "class ExtractArchitecture(dspy.Signature):\n",
    "    \"\"\"Extract architectural information from DSPy documentation.\"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"Full DSPy documentation text\")\n",
    "    query: str = dspy.InputField(desc=\"What to extract\")\n",
    "    modules: list = dspy.OutputField(desc=\"List of DSPy modules\")\n",
    "    optimizers: list = dspy.OutputField(desc=\"List of optimizers\")\n",
    "    design_principles: str = dspy.OutputField(desc=\"Key design principles\")\n",
    "\n",
    "\n",
    "with open(\"../rlm_content/dspy-knowledge/dspy-doc.txt\", \"r\") as f:\n",
    "    dspy_docs = f.read()\n",
    "\n",
    "print(f\"Loaded: {len(dspy_docs):,} chars, {len(dspy_docs.splitlines()):,} lines\")\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=ExtractArchitecture,\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=25,\n",
    "    max_llm_calls=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(\n",
    "        docs=dspy_docs,\n",
    "        query=\"Extract all modules and optimizers from documentation\",\n",
    "    )\n",
    "    print(f\"Modules: {result.modules}\")\n",
    "    print(f\"Optimizers: {result.optimizers}\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. Parallel Processing with llm_query_batched()\n",
    "\n",
    "Process multiple chunks in parallel for dramatic speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "batched-queries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 17:34:02 INFO dspy.predict.rlm: RLM iteration 1/20\n",
      "Reasoning: The goal is to extract API endpoints from the `docs` variable. The preview shows a directory structure of the DSPy documentation. API endpoints in this context likely refer to class/method definitions or REST endpoints, but given this is a Python library (DSPy), it's more likely to be documentation of programmatic APIs (classes, functions, etc.).\n",
      "\n",
      "I will start by exploring the contents of `docs` to understand the structure and how the endpoints are documented. Since the documentation is large (~81k characters), I'll print the first few thousand characters and check if there are explicit lists of endpoints or if they are scattered across markdown files.\n",
      "\n",
      "Plan:\n",
      "1. Print the beginning of `docs` to understand the format.\n",
      "2. Search for common API keywords (e.g., \"GET\", \"POST\", \"class\", \"def\", \"endpoint\").\n",
      "3. Determine if the documentation contains REST endpoints or programmatic API references.\n",
      "4. Use `llm_query` or `llm_query_batched` to extract the specific endpoints.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Total length: {len(docs)}\")\n",
      "print(\"First 2000 characters:\")\n",
      "print(docs[:2000])\n",
      "\n",
      "# Look for patterns that might indicate where the actual content starts after the file list\n",
      "if \"docs/api\" in docs:\n",
      "    print(\"\\nFound api directory in docs. Searching for content...\")\n",
      "```\n",
      "2026/02/07 17:34:09 INFO dspy.predict.rlm: RLM iteration 2/20\n",
      "Reasoning: The `docs` variable starts with a large directory structure representing the documentation files of the `dspy` library. Given the context of a Python library, \"API endpoints\" typically refers to the public classes, functions, or modules documented. The directory structure itself reveals a significant list of API components under `docs/api/`. \n",
      "\n",
      "I need to see the actual content of the files to determine what constitutes an \"endpoint\" (e.g., specific URLs or just the class/function names). I'll search for where the file list ends and the content begins, and then I'll use `llm_query` to extract the relevant API endpoints from the content.\n",
      "\n",
      "Plan:\n",
      "1. Locate the transition from the file tree to the actual documentation content.\n",
      "2. Split the document into chunks if it's too large for a single query, focusing on the content part.\n",
      "3. Use a batch query to extract API endpoints (classes/methods/functions) from the content.\n",
      "\n",
      "Let's find where the content starts first.\n",
      "Code:\n",
      "```python\n",
      "# The directory structure seems to be quite long. Let's find where it ends.\n",
      "# Usually, there's a file content marker or just a shift in format.\n",
      "lines = docs.split('\\n')\n",
      "content_start_idx = 0\n",
      "for i, line in enumerate(lines):\n",
      "    if 'File: ' in line or '---' in line or (i > 100 and not any(c in line for c in ['│', '├──', '└──'])):\n",
      "        content_start_idx = i\n",
      "        break\n",
      "\n",
      "print(f\"Likely content start at line {content_start_idx}\")\n",
      "print(\"Sample content:\")\n",
      "print('\\n'.join(lines[content_start_idx:content_start_idx+20]))\n",
      "\n",
      "# Also check for mentions of \"HTTP\" or URLs to see if there are REST endpoints\n",
      "import re\n",
      "urls = re.findall(r'https?://[^\\s<>\"]+|/[a-zA-Z0-9/_-]+', docs[content_start_idx:])\n",
      "print(f\"\\nFound {len(urls)} potential paths/URLs in content.\")\n",
      "```\n",
      "2026/02/07 17:34:15 INFO dspy.predict.rlm: RLM iteration 3/20\n",
      "Reasoning: The documentation starts with a directory structure followed by the content of various files marked by `FILE: <path>`. Since the library is `dspy`, I am looking for the API endpoints. In the context of a library documentation, these are typically the public classes and methods. The file list showed many files under `docs/api/`, such as `Predict.md`, `ChainOfThought.md`, `BootstrapFewShot.md`, etc.\n",
      "\n",
      "I will scan the content for files located in the `docs/api/` directory and use the LLM to extract the specific API endpoints (classes and methods) defined in those files. I'll focus on sections of the document that contain the actual API definitions.\n",
      "\n",
      "Plan:\n",
      "1. Identify all file sections related to the API (starting with `FILE: docs/api/`).\n",
      "2. Extract the text of these sections.\n",
      "3. Use `llm_query_batched` to process these sections and extract the formal API endpoints.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Split the document by the FILE: markers\n",
      "file_sections = re.split(r'={48}\\nFILE: ', docs)\n",
      "\n",
      "api_sections = []\n",
      "for section in file_sections:\n",
      "    # We are interested in files under docs/api/\n",
      "    if section.startswith('docs/api/'):\n",
      "        api_sections.append(section)\n",
      "\n",
      "print(f\"Found {len(api_sections)} API-related file sections.\")\n",
      "\n",
      "# Take a look at one to see the structure\n",
      "if api_sections:\n",
      "    print(\"\\nExample API section (first 500 chars):\")\n",
      "    print(api_sections[0][:500])\n",
      "\n",
      "# Group sections to avoid too many small queries, but keep them within LLM limits\n",
      "# We'll batch them and ask the LLM to extract the class/function/method names.\n",
      "prompts = []\n",
      "current_batch = \"\"\n",
      "for section in api_sections:\n",
      "    if len(current_batch) + len(section) > 10000:\n",
      "        prompts.append(f\"Extract all API endpoints (class names, method names, or functions) documented in the following text. Return them as a JSON list of strings:\\n\\n{current_batch}\")\n",
      "        current_batch = section\n",
      "    else:\n",
      "        current_batch += \"\\n\" + section\n",
      "\n",
      "if current_batch:\n",
      "    prompts.append(f\"Extract all API endpoints (class names, method names, or functions) documented in the following text. Return them as a JSON list of strings:\\n\\n{current_batch}\")\n",
      "\n",
      "print(f\"Prepared {len(prompts)} prompts for batched analysis.\")\n",
      "```\n",
      "2026/02/07 17:34:18 INFO dspy.predict.rlm: RLM iteration 4/20\n",
      "Reasoning: The previous attempt to find API sections using `FILE: docs/api/` failed because the file path might not be formatted exactly that way or the directory structure in the documentation content differs from the initial tree view. I need to re-examine the file markers in the `docs` variable to identify how files are actually delimited and named. I'll search for the `FILE:` pattern more broadly and inspect the file names to correctly identify API-related documentation.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Let's see how the files are actually named and where the API documentation starts.\n",
      "# I'll look for any \"FILE: \" occurrence and print the first 10 matches.\n",
      "file_markers = re.findall(r'FILE: .+', docs)\n",
      "print(f\"Total file markers found: {len(file_markers)}\")\n",
      "print(\"First 20 file markers:\")\n",
      "for marker in file_markers[:20]:\n",
      "    print(marker)\n",
      "\n",
      "# Based on the initial tree, API files are likely under stanfordnlp-dspy/docs/docs/api/\n",
      "# Let's try searching for that pattern.\n",
      "api_markers = [m for m in file_markers if 'api/' in m]\n",
      "print(f\"\\nFound {len(api_markers)} markers containing 'api/'.\")\n",
      "for marker in api_markers[:10]:\n",
      "    print(marker)\n",
      "\n",
      "# If the markers exist, I will extract the content between them.\n",
      "# The content seems to be separated by \"================================================\"\n",
      "# followed by \"FILE: <path>\"\n",
      "```\n",
      "2026/02/07 17:34:25 INFO dspy.predict.rlm: RLM iteration 5/20\n",
      "Reasoning: The file markers in the document use the format `FILE: docs/docs/api/...`. I have identified 61 such markers. These files likely contain the definitions for the DSPy API (classes, methods, and functions). I will now extract the content of these specific sections and use `llm_query_batched` to parse the API endpoints from them. I'll group the sections into appropriately sized prompts to ensure efficient processing while staying within the LLM's context limit.\n",
      "\n",
      "Plan:\n",
      "1. Split the `docs` string into individual file contents based on the `================================================\\nFILE: ` delimiter.\n",
      "2. Filter for sections where the filename starts with `docs/docs/api/`.\n",
      "3. Use `llm_query_batched` to extract the formal API endpoints (class names, methods, and functions) from these sections.\n",
      "4. Consolidate the results into a final list.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "import json\n",
      "\n",
      "# Split by the header pattern\n",
      "parts = re.split(r'={48}\\nFILE: ', docs)\n",
      "\n",
      "api_docs_content = []\n",
      "for part in parts:\n",
      "    # Check if this part corresponds to an API file\n",
      "    if part.startswith('docs/docs/api/'):\n",
      "        api_docs_content.append(part)\n",
      "\n",
      "print(f\"Number of API sections to process: {len(api_docs_content)}\")\n",
      "\n",
      "# Group content into chunks for the LLM\n",
      "prompts = []\n",
      "current_chunk = \"\"\n",
      "for content in api_docs_content:\n",
      "    if len(current_chunk) + len(content) > 15000:\n",
      "        prompts.append(f\"Extract a list of all API endpoints (class names, functions, or methods) documented in the text below. Focus on the formal definitions provided in the documentation. Return only a JSON list of strings.\\n\\nText:\\n{current_chunk}\")\n",
      "        current_chunk = content\n",
      "    else:\n",
      "        current_chunk += \"\\n\\n\" + content\n",
      "\n",
      "if current_chunk:\n",
      "    prompts.append(f\"Extract a list of all API endpoints (class names, functions, or methods) documented in the text below. Focus on the formal definitions provided in the documentation. Return only a JSON list of strings.\\n\\nText:\\n{current_chunk}\")\n",
      "\n",
      "# Execute batched query\n",
      "results = llm_query_batched(prompts)\n",
      "\n",
      "all_endpoints = set()\n",
      "for res in results:\n",
      "    try:\n",
      "        # Clean the response if it contains markdown code blocks\n",
      "        clean_res = res.strip()\n",
      "        if \"```json\" in clean_res:\n",
      "            clean_res = clean_res.split(\"```json\")[1].split(\"```\")[0].strip()\n",
      "        elif \"```\" in clean_res:\n",
      "            clean_res = clean_res.split(\"```\")[1].split(\"```\")[0].strip()\n",
      "        \n",
      "        endpoints = json.loads(clean_res)\n",
      "        if isinstance(endpoints, list):\n",
      "            all_endpoints.update(endpoints)\n",
      "    except Exception as e:\n",
      "        print(f\"Error parsing response: {e}\")\n",
      "        print(f\"Response starts with: {res[:100]}\")\n",
      "\n",
      "final_list = sorted(list(all_endpoints))\n",
      "print(f\"Extracted {len(final_list)} unique API endpoints.\")\n",
      "print(\"Sample endpoints:\")\n",
      "print(final_list[:10])\n",
      "\n",
      "SUBMIT(final_list)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 359 endpoints\n",
      "  - __call__\n",
      "  - acall\n",
      "  - adapt_to_native_lm_feature\n",
      "  - aforward\n",
      "  - append\n"
     ]
    }
   ],
   "source": [
    "class ExtractAPIEndpoints(dspy.Signature):\n",
    "    \"\"\"Extract API endpoints using batched analysis.\"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"API documentation\")\n",
    "    api_endpoints: list = dspy.OutputField(desc=\"List of API endpoints\")\n",
    "\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=ExtractAPIEndpoints,\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=20,\n",
    "    max_llm_calls=30,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(docs=dspy_docs)\n",
    "    print(f\"Found {len(result.api_endpoints)} endpoints\")\n",
    "    for ep in result.api_endpoints[:5]:\n",
    "        print(f\"  - {ep}\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "## 8. Stateful Multi-Step Reasoning\n",
    "\n",
    "RLM maintains state across iterations. Variables persist, enabling multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "multi-step-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 17:36:14 INFO dspy.predict.rlm: RLM iteration 1/30\n",
      "Reasoning: The documentation provided is for the `dspy` library. To find and categorize error patterns, I first need to understand the structure of the `docs` variable and what it contains. It seems to be a collection of directory structures and likely file contents. I will start by printing a larger sample of `docs` and checking if there are specific sections or files that mention common errors, troubleshooting, or bugs. I'll also look for keywords like \"Error\", \"Exception\", \"Fail\", \"Issue\", \"Fix\", \"Troubleshoot\", and \"Warning\".\n",
      "\n",
      "Plan:\n",
      "1. Explore the structure of `docs`.\n",
      "2. Extract sections or lines containing error-related keywords.\n",
      "3. Use `llm_query` to categorize these errors if the data is dense or complex.\n",
      "Code:\n",
      "```python\n",
      "# Initial exploration of the docs content\n",
      "print(f\"Total length of docs: {len(docs)}\")\n",
      "print(\"First 2000 characters of docs:\")\n",
      "print(docs[:2000])\n",
      "\n",
      "# Look for specific keywords related to errors\n",
      "keywords = [\"error\", \"exception\", \"fail\", \"issue\", \"troubleshoot\", \"warning\", \"fix\", \"bug\"]\n",
      "import re\n",
      "\n",
      "found_keywords = {}\n",
      "for kw in keywords:\n",
      "    matches = re.findall(kw, docs, re.IGNORECASE)\n",
      "    found_keywords[kw] = len(matches)\n",
      "\n",
      "print(\"\\nKeyword frequency:\")\n",
      "print(found_keywords)\n",
      "\n",
      "# Let's try to find some snippets containing 'error' or 'exception' to see context\n",
      "snippets = []\n",
      "for match in re.finditer(r\".{0,50}(error|exception|fail|issue|warning).{0,50}\", docs, re.IGNORECASE):\n",
      "    snippets.append(match.group(0))\n",
      "    if len(snippets) >= 10:\n",
      "        break\n",
      "\n",
      "print(\"\\nSample snippets containing keywords:\")\n",
      "for s in snippets:\n",
      "    print(f\"- {s}\")\n",
      "```\n",
      "2026/02/07 17:36:22 INFO dspy.predict.rlm: RLM iteration 2/30\n",
      "Reasoning: The keyword search revealed some potential leads (e.g., `CodeInterpreterError`, `ValueError`, and issue templates), but the overall counts for keywords like \"error\" are quite low (8 hits) given the 81k character length. This suggests that errors might be documented more implicitly or within specific function definitions as exceptions.\n",
      "\n",
      "I need to see the actual content of the documentation files rather than just the directory tree. I will split the `docs` string by file headers (if present) to understand the content of the API documentation and source code snippets provided. I'll search for typical error-handling patterns like `raise`, `try/except`, and specific error messages in the docstrings.\n",
      "\n",
      "Plan:\n",
      "1. Identify how files are separated in `docs` (it looks like `FILE: <path>` or similar headers might be used).\n",
      "2. Extract the content of relevant sections like `api/`, `modules/`, and `optimizers/`.\n",
      "3. Search for error-related patterns in the content.\n",
      "4. Use `llm_query` to help categorize and identify patterns from the extracted relevant text.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Let's see how the files are delineated. \n",
      "# Looking at the previous output, I saw \"FILE: .github/ISSUE_TEMPLATE/bug_report.yml\"\n",
      "# I'll check for this pattern.\n",
      "\n",
      "files = re.split(r'FILE: ', docs)\n",
      "print(f\"Number of 'files' found: {len(files)}\")\n",
      "\n",
      "# Print the first few characters of a few files to verify\n",
      "for i, f in enumerate(files[:5]):\n",
      "    print(f\"\\n--- File {i} (First 100 chars) ---\")\n",
      "    print(f[:100])\n",
      "\n",
      "# I'll search for common error/exception raising patterns and docstrings\n",
      "error_patterns = [\n",
      "    r\"raise \\w+\",\n",
      "    r\"Exception\",\n",
      "    r\"Error\",\n",
      "    r\"fail\",\n",
      "    r\"invalid\",\n",
      "    r\"assertion\"\n",
      "]\n",
      "\n",
      "all_matches = []\n",
      "for file_content in files:\n",
      "    lines = file_content.split('\\n')\n",
      "    filename = lines[0] if lines else \"Unknown\"\n",
      "    for line_no, line in enumerate(lines):\n",
      "        if any(re.search(p, line, re.IGNORECASE) for p in error_patterns):\n",
      "            all_matches.append(f\"[{filename} line {line_no}]: {line.strip()}\")\n",
      "\n",
      "print(f\"\\nFound {len(all_matches)} lines with error-related patterns.\")\n",
      "print(\"Sample matches:\")\n",
      "for m in all_matches[:20]:\n",
      "    print(m)\n",
      "\n",
      "# Since the number of errors might be small, I'll ask the LLM to identify the categories\n",
      "# from the matches found across the document.\n",
      "context_for_llm = \"\\n\".join(all_matches)\n",
      "```\n",
      "2026/02/07 17:36:28 INFO dspy.predict.rlm: RLM iteration 3/30\n",
      "Reasoning: The keyword search yielded a relatively small number of direct matches (12), but they point toward specific areas:\n",
      "1.  **Code Execution Errors**: `CodeInterpreterError` suggests issues related to running code via DSPy primitives.\n",
      "2.  **Signature/Schema Errors**: `ValueError` related to field types indicates potential issues when defining DSPy Signatures.\n",
      "3.  **Assertion Failures**: The \"assertions\" in the test JSON files suggest that validation of outputs (specifically boolean success/failure) is a pattern.\n",
      "4.  **Implementation/Interface Errors**: `NotImplementedError` in teleprompters.\n",
      "5.  **External/Tool Errors**: `ValueError(\"error!\")` in a mock server test.\n",
      "\n",
      "However, 12 matches out of 81k characters suggest I might be missing errors described in natural language (docstrings, guides) rather than just code. I will now look for common DSPy-specific failure modes like \"hallucination\", \"refusal\", \"formatting\", \"token limits\", or \"connection\" issues that might not be explicitly labeled as \"Error\" in the code but are \"error patterns\" in the context of LLM applications. I'll also perform a broader search for \"invalid\", \"missing\", \"incorrect\", and \"failed\".\n",
      "\n",
      "Plan:\n",
      "1. Scan for semantic error patterns using `llm_query` on chunks of the documentation that likely contain logic (e.g., adapters, modules, optimizers).\n",
      "2. Look for \"failure\" cases in the `evaluation` and `tests` sections.\n",
      "3. Consolidate these into categories.\n",
      "Code:\n",
      "```python\n",
      "# Focus on areas where error logic or failure modes are likely described\n",
      "relevant_files = []\n",
      "target_paths = [\"adapters\", \"evaluation\", \"modules\", \"optimizers\", \"signatures\"]\n",
      "\n",
      "for file_content in files:\n",
      "    header = file_content.split('\\n')[0]\n",
      "    if any(path in header for path in target_paths):\n",
      "        relevant_files.append(file_content)\n",
      "\n",
      "# Use llm_query to find and categorize error patterns in these relevant sections\n",
      "context = \"\\n\\n\".join(relevant_files)[:15000] # Stay within prompt limits\n",
      "prompt = f\"\"\"\n",
      "Analyze the following DSPy documentation snippets and identify common error patterns or failure modes. \n",
      "Categorize them (e.g., Validation Errors, Configuration Errors, Execution Failures) and count how many distinct types you find.\n",
      "\n",
      "Snippets:\n",
      "{context}\n",
      "\"\"\"\n",
      "\n",
      "print(\"Querying LLM for semantic error patterns...\")\n",
      "analysis = llm_query(prompt)\n",
      "print(analysis)\n",
      "\n",
      "# Also check for 'assertion' context in tests to see what is being validated\n",
      "assertion_snippets = re.findall(r\".{0,100}assertion.{0,100}\", docs, re.IGNORECASE)\n",
      "print(\"\\nAssertion contexts:\")\n",
      "for s in assertion_snippets[:5]:\n",
      "    print(f\"- {s}\")\n",
      "```\n",
      "2026/02/07 17:36:42 INFO dspy.predict.rlm: RLM iteration 4/30\n",
      "Reasoning: The previous analysis successfully identified 7 distinct error types across 3 categories (Validation, Configuration, and Execution) by analyzing the module and adapter documentation. However, the initial search for keyword \"error\" in Step 2 only found 12 specific instances, and the LLM analysis was based on a subset of the documentation. To ensure a comprehensive count and categorization, I need to:\n",
      "1. Examine the `evaluation` and `optimizer` sections more closely, as these often contain logic for handling failures (e.g., \"BootstrapFewShot\" failing to find examples).\n",
      "2. Look at the `tests` directory content in the `docs` variable more thoroughly to find what specific \"assertions\" are checking for, as these represent expected error patterns or quality failures.\n",
      "3. Check the `CodeInterpreterError` found earlier to see if there are other specific custom exceptions defined in the library.\n",
      "4. I will perform a broader scan of the `docs` to find any other unique error messages or \"raise\" statements I might have missed by looking at a larger sample of the matches.\n",
      "\n",
      "Plan:\n",
      "- Extract all \"raise\" and \"Error\" occurrences from the entire `docs` string to find unique exception types.\n",
      "- Query the LLM specifically on the `optimizers` and `evaluation` docs to find performance-related \"failure modes\" (e.g., optimization failing to converge or find valid examples).\n",
      "- Synthesize all findings into the final categories.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Extract all 'raise' statements and Exception classes\n",
      "all_raises = re.findall(r\"raise\\s+(\\w+)(?:\\((.*)\\))?\", docs)\n",
      "unique_exceptions = set([r[0] for r in all_raises])\n",
      "\n",
      "# Extract specific error messages\n",
      "error_messages = re.findall(r\"['\\\"]([^'\\\"]*(?:Error|Fail|Invalid|Required|Missing)[^'\\\"]*)['\\\"]\", docs, re.IGNORECASE)\n",
      "\n",
      "print(f\"Unique Exceptions raised in code: {unique_exceptions}\")\n",
      "print(f\"Sample error-related strings: {error_messages[:15]}\")\n",
      "\n",
      "# Look at evaluation/optimizers content for \"failure\" patterns\n",
      "opt_eval_files = [f for f in files if \"optimizer\" in f.lower() or \"evaluation\" in f.lower()]\n",
      "opt_eval_context = \"\\n\\n\".join(opt_eval_files)\n",
      "\n",
      "prompt_2 = f\"\"\"\n",
      "Identify error patterns, failure modes, or validation checks specifically in the DSPy Optimizers and Evaluation metrics. \n",
      "Look for things like:\n",
      "- Why an optimizer might fail to improve a program.\n",
      "- Common reasons for evaluation metrics to return 0 or fail.\n",
      "- Requirements for data or models that, if not met, cause errors.\n",
      "\n",
      "Context:\n",
      "{opt_eval_context[:10000]}\n",
      "\"\"\"\n",
      "\n",
      "print(\"\\nQuerying LLM for Optimizer/Evaluation error patterns...\")\n",
      "analysis_2 = llm_query(prompt_2)\n",
      "print(analysis_2)\n",
      "```\n",
      "2026/02/07 17:36:56 INFO dspy.predict.rlm: RLM iteration 5/30\n",
      "Reasoning: The analysis has identified several distinct error categories and failure modes. From the manual code scan and the LLM semantic analysis, I have found:\n",
      "1.  **Validation Errors**: Parsing mismatches (JSON/Chat), missing signature fields, and invalid task descriptions.\n",
      "2.  **Configuration Errors**: State serialization mismatches (load/save), missing LM context (essential for metrics/modules), and parameter desynchronization.\n",
      "3.  **Execution Failures**: Synchronicity mismatches (sync vs. async calls), and state leakage during optimization (deepcopy issues).\n",
      "4.  **Optimization/Learning Failures**: Bootstrapping \"deadlocks\" (teacher model failing to generate any valid traces), metric non-differentiability (lack of signal), and overfitting/contamination.\n",
      "5.  **Runtime/Environment Errors**: `CodeInterpreterError` for sandboxed execution and connection/retrial failures in setup scripts.\n",
      "\n",
      "The count of unique \"error patterns\" or \"failure modes\" identified so far is roughly 10-12 across these categories. I will do one final sweep of the `tests` and `primitives` content to see if there are any specific logic-related errors mentioned (e.g., token limits or refusal handling) to finalize the list.\n",
      "Code:\n",
      "```python\n",
      "# Final sweep for specific error strings in primitives and tests\n",
      "final_targets = [\"primitives\", \"tests\"]\n",
      "final_context = []\n",
      "for file_content in files:\n",
      "    header = file_content.split('\\n')[0]\n",
      "    if any(target in header for target in final_targets):\n",
      "        final_context.append(file_content)\n",
      "\n",
      "prompt_final = f\"\"\"\n",
      "Based on the following code and test snippets, list any additional specific error patterns or failure modes not previously covered (like token limits, empty outputs, or specific tool failures).\n",
      "Then, provide a final consolidated categorization of all error patterns found in the entire DSPy documentation provided.\n",
      "\n",
      "Context:\n",
      "{\"\\n\\n\".join(final_context)[:15000]}\n",
      "\"\"\"\n",
      "\n",
      "print(\"Final sweep for specific error patterns...\")\n",
      "final_analysis = llm_query(prompt_final)\n",
      "print(final_analysis)\n",
      "```\n",
      "2026/02/07 17:37:10 INFO dspy.predict.rlm: RLM iteration 6/30\n",
      "Reasoning: I have completed a comprehensive review of the DSPy documentation, including the directory structure, API documentation, and reliability tests. Through keyword searches, manual code inspection, and semantic analysis using the LLM across multiple specialized sections (Adapters, Modules, Optimizers, and Primitives), I have identified and categorized the core error patterns. \n",
      "\n",
      "The errors fall into five major categories:\n",
      "1. **Input & Serialization Failures**: Issues with media loading, MIME normalization, and type mapping.\n",
      "2. **Model Output & Parsing Failures**: Schema violations, structural drift in nested types, and token-limit truncation.\n",
      "3. **Execution & Logic Failures**: Runtime errors in the CodeInterpreter and tool call mismatches.\n",
      "4. **Evaluation & Reliability Failures**: Metric mismatches (Exact Match failures) and assertion violations in complex formats like Markdown.\n",
      "5. **Configuration & Meta-Programming Failures**: State serialization conflicts (Load/Save), missing LM contexts, and teleprompter compilation issues.\n",
      "\n",
      "I have identified approximately 18 distinct error types across these 5 categories. I am now ready to submit the final results.\n",
      "Code:\n",
      "```python\n",
      "error_categories = {\n",
      "    \"Input & Serialization Failures\": [\n",
      "        \"Audio/Image Protocol Errors (loading from URLs or local files)\",\n",
      "        \"MIME/Format Normalization Failures (e.g., wav vs x-wav mismatch)\",\n",
      "        \"Custom Type Mapping Errors (failure to extract schema from Python annotations)\"\n",
      "    ],\n",
      "    \"Model Output & Parsing Failures\": [\n",
      "        \"Schema Non-Compliance (missing required fields in LM response)\",\n",
      "        \"Structural Drift in Deep Nesting (losing track of keys in levels 3+)\",\n",
      "        \"JSON/Markdown Parsing Errors (malformed structure or heading hierarchy failure)\",\n",
      "        \"Token Limit/Truncation (incomplete JSON or code blocks)\",\n",
      "        \"Empty or Null Responses during streaming/parsing\"\n",
      "    ],\n",
      "    \"Execution & Logic Failures\": [\n",
      "        \"CodeInterpreterError (runtime errors in generated Python code)\",\n",
      "        \"Tool Call Schema Mismatches (invalid litellm function call formatting)\",\n",
      "        \"Synchronicity Mismatches (blocking calls in async loops or unawaited coroutines)\",\n",
      "        \"Context/History Bloat (exceeding window limits causing instruction loss)\"\n",
      "    ],\n",
      "    \"Evaluation & Reliability Failures\": [\n",
      "        \"Exact Match (EM) Misses (semantic equivalence vs. literal discrepancy)\",\n",
      "        \"Assertion Violations (failures in programmatic constraints defined in dspy.Assert)\",\n",
      "        \"Input Format Hallucination (including unintended content in structured output like TOCs)\"\n",
      "    ],\n",
      "    \"Configuration & Meta-Programming Failures\": [\n",
      "        \"State Serialization Conflict (mismatched signatures during load/save)\",\n",
      "        \"Unconfigured LM Context (executing metrics/modules without set_lm)\",\n",
      "        \"Teleprompter Compilation Failures (mapping student parameters to teacher outputs)\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "total_errors_found = 18\n",
      "\n",
      "SUBMIT(error_categories, total_errors_found)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 error patterns\n",
      "Input & Serialization Failures: 3 errors\n",
      "Model Output & Parsing Failures: 5 errors\n",
      "Execution & Logic Failures: 4 errors\n",
      "Evaluation & Reliability Failures: 3 errors\n",
      "Configuration & Meta-Programming Failures: 3 errors\n"
     ]
    }
   ],
   "source": [
    "class FindErrorPatterns(dspy.Signature):\n",
    "    \"\"\"Find and categorize error patterns.\"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"Documentation text\")\n",
    "    error_categories: dict = dspy.OutputField(desc=\"Error types mapped to solutions\")\n",
    "    total_errors_found: int = dspy.OutputField(desc=\"Total errors identified\")\n",
    "\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=FindErrorPatterns,\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=30,\n",
    "    max_llm_calls=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(docs=dspy_docs)\n",
    "    print(f\"Found {result.total_errors_found} error patterns\")\n",
    "    for cat, errors in result.error_categories.items():\n",
    "        print(f\"{cat}: {len(errors)} errors\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "## 9. Inspecting the Trajectory\n",
    "\n",
    "Every RLM result includes a trajectory - complete history of reasoning, code, and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "trajectory-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory (3 steps):\n",
      "\n",
      "\n",
      "Step 1:\n",
      "  Reasoning: I need to summarize the content provided in the `text` variable, which appears to be a directory str...\n",
      "  Code: print(f\"Total length: {len(text)}\")\n",
      "print(\"--- Full Text Sta...\n",
      "\n",
      "Step 2:\n",
      "  Reasoning: The text provides a detailed directory structure of the `stanfordnlp-dspy` repository, specifically ...\n",
      "  Code: prompt = f\"\"\"Summarize the purpose and structure of the proj...\n",
      "\n",
      "Step 3:\n",
      "  Reasoning: The previous step successfully generated a comprehensive and well-structured summary of the `stanfor...\n",
      "  Code: # The summary was already generated in the previous step.\n",
      "# ...\n"
     ]
    }
   ],
   "source": [
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=\"text -> summary\",\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=10,\n",
    "    max_llm_calls=10,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    text_sample = dspy_docs[:3000]\n",
    "    result = rlm(text=text_sample)\n",
    "    \n",
    "    print(f\"Trajectory ({len(result.trajectory)} steps):\\n\")\n",
    "    for i, step in enumerate(result.trajectory):\n",
    "        print(f\"\\nStep {i+1}:\")\n",
    "        print(f\"  Reasoning: {step.get(\"reasoning\", \"N/A\")[:100]}...\")\n",
    "        print(f\"  Code: {step.get(\"code\", \"\")[:60]}...\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-tools",
   "metadata": {},
   "source": [
    "## 10. Advanced: Custom Tools in the Sandbox\n",
    "\n",
    "RLM supports custom tools that run inside the sandbox. This extends capabilities beyond built-in llm_query().\n",
    "\n",
    "### Example: Regex Pattern Matcher Tool\n",
    "We'll create a tool that efficiently extracts patterns from text using compiled regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "custom-tools",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 17:37:49 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: I will start by exploring the content of `docs` to understand its structure and how headers and code blocks are formatted. This will help me craft accurate regex patterns. I'll print the first 2000 characters and check for common markdown patterns.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Total length: {len(docs)}\")\n",
      "print(\"First 2000 characters:\")\n",
      "print(docs[:2000])\n",
      "```\n",
      "2026/02/07 17:37:53 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The initial exploration shows a directory structure but not the actual markdown content yet. I need to see more of the document to find the markdown headers (e.g., `# Header`, `## Header`) and code blocks (e.g., ` ```python ... ``` `). I'll print a segment from the middle and the end of the string to see where the actual content starts.\n",
      "Code:\n",
      "```python\n",
      "# The first part was just a directory tree. Let's look further in.\n",
      "print(\"Middle segment (4000-6000):\")\n",
      "print(docs[4000:6000])\n",
      "print(\"\\nEnd segment (8000-10000):\")\n",
      "print(docs[8000:])\n",
      "```\n",
      "2026/02/07 17:37:58 INFO dspy.predict.rlm: RLM iteration 3/15\n",
      "Reasoning: The document contains a directory listing followed by a series of file contents separated by `================================================` dividers. My goal is to extract markdown headers and code blocks.\n",
      "\n",
      "Looking at the content:\n",
      "1. Headers use the standard `#` and `##` format (e.g., `# dspy.Adapter`).\n",
      "2. Code blocks in markdown usually use ` ``` ` or sometimes indented blocks or specific tags like `:::`. In this specific document, there are `::: dspy.Adapter` blocks which act as reference documentation, but standard markdown code blocks (triple backticks) are the primary target. I need to scan for both.\n",
      "\n",
      "I will use `regex_extract` to find all matches for:\n",
      "- Headers: `^#+\\s+.+$` (with multiline flag).\n",
      "- Code blocks: ` ```[\\s\\S]*?``` `.\n",
      "\n",
      "I'll start by extracting headers and checking for any standard code blocks in the provided text.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Extract headers: # Header, ## Header, etc.\n",
      "# Using ^ at the start of lines\n",
      "headers = regex_extract(docs, r\"(?m)^#+\\s+.+$\")\n",
      "\n",
      "# Extract code blocks: ```language ... ```\n",
      "code_blocks = regex_extract(docs, r\"```[\\s\\S]*?```\")\n",
      "\n",
      "print(f\"Found {len(headers)} headers.\")\n",
      "print(f\"Found {len(code_blocks)} code blocks.\")\n",
      "\n",
      "if headers:\n",
      "    print(\"Sample headers:\", headers[:5])\n",
      "if code_blocks:\n",
      "    print(\"Sample code blocks:\", code_blocks[:2])\n",
      "else:\n",
      "    # If no triple backtick blocks, maybe they use other formats or I need to look closer.\n",
      "    # The preview showed some yaml in .pre-commit-config.yaml and ::: blocks.\n",
      "    print(\"No triple-backtick code blocks found yet. Checking for other patterns...\")\n",
      "```\n",
      "2026/02/07 17:38:03 INFO dspy.predict.rlm: RLM iteration 4/15\n",
      "Reasoning: The previous extraction found headers like `# dspy.Adapter` but zero triple-backtick code blocks. However, looking at the initial `docs` preview, there were code-like sections in the \"Directory structure\" and within `.pre-commit-config.yaml`. While the directory tree isn't a markdown code block, the YAML content under `FILE: .pre-commit-config.yaml` and the `::: dspy.Adapter` blocks are significant structural elements. \n",
      "\n",
      "I need to:\n",
      "1. Re-verify if any triple-backtick blocks exist in the parts of `docs` I haven't printed yet.\n",
      "2. Check for the `:::` blocks which seem to be used for API documentation.\n",
      "3. Check for the YAML content which, while not in backticks in this raw dump, represents code-like configuration.\n",
      "4. Finalize the structure summary based on the directory-tree start and the file-by-file content.\n",
      "\n",
      "I will perform a more thorough search for any backticks and also capture the `:::` blocks.\n",
      "Code:\n",
      "```python\n",
      "# Check for any backticks at all to see if my regex missed them\n",
      "backtick_check = regex_extract(docs, r\"```\")\n",
      "print(f\"Backtick occurrences: {len(backtick_check)}\")\n",
      "\n",
      "# Extract the ::: blocks as they represent the main \"code/config\" content in this doc\n",
      "api_blocks = regex_extract(docs, r\"(?s)::: dspy\\..*?:::\")\n",
      "print(f\"Found {len(api_blocks)} API reference blocks (:::).\")\n",
      "\n",
      "# Also look for the file dividers to understand the document structure better\n",
      "files = regex_extract(docs, r\"FILE: (.*)\")\n",
      "print(f\"Files documented: {files}\")\n",
      "\n",
      "# Let's see one of the ::: blocks to decide if it counts as a code block for the task\n",
      "if api_blocks:\n",
      "    print(\"Sample API block:\")\n",
      "    print(api_blocks[0])\n",
      "\n",
      "# Just in case, check for indented blocks (4 spaces) following a paragraph\n",
      "# though usually headers/triple-backticks are the target.\n",
      "```\n",
      "2026/02/07 17:38:08 INFO dspy.predict.rlm: RLM iteration 5/15\n",
      "Reasoning: The exploration reveals that while the document is a compilation of markdown files, it currently contains zero standard triple-backtick (```) code blocks. Instead, it uses `::: dspy.ClassName` blocks to define API documentation. The headers are standard markdown `#` headers. \n",
      "\n",
      "I will:\n",
      "1. Extract all markdown headers using `regex_extract`.\n",
      "2. Treat the `::: ... :::` blocks as \"code blocks\" in the context of this documentation extraction, as they contain structured configuration/code parameters for the API.\n",
      "3. Check if there are any other blocks that should be included (like the YAML in `.pre-commit-config.yaml`). Looking at the instructions, \"find all code blocks\" usually implies markdown code blocks. Since there are no backticks, I will include the `:::` blocks and the YAML configuration content which is presented as code.\n",
      "4. Synthesize the structure summary.\n",
      "\n",
      "Let's do one final sweep to ensure I haven't missed any headers or structured blocks in the middle of the document.\n",
      "Code:\n",
      "```python\n",
      "# Extract all markdown headers\n",
      "headers = regex_extract(docs, r\"(?m)^#+\\s+.+$\")\n",
      "\n",
      "# Extract the API reference blocks (:::) as they represent the code/structure here\n",
      "api_blocks = regex_extract(docs, r\"(?s):::\\s+dspy\\..*?:::\")\n",
      "\n",
      "# Extract YAML content from the .pre-commit-config.yaml section\n",
      "# It starts after the divider and ends before the next FILE divider or end of doc\n",
      "yaml_content = regex_extract(docs, r\"FILE: \\.pre-commit-config\\.yaml\\n=+\\n([\\s\\S]*?)(?=\\n=+)\")\n",
      "\n",
      "# Combine API blocks and the YAML content into code_blocks\n",
      "code_blocks = yaml_content + api_blocks\n",
      "\n",
      "print(f\"Headers found: {headers}\")\n",
      "print(f\"Code blocks found: {len(code_blocks)}\")\n",
      "\n",
      "# Prepare structure summary\n",
      "summary = \"The document consists of a directory tree followed by specific file contents for the stanfordnlp-dspy project. It includes API documentation for dspy.Adapter and dspy.ChatAdapter using custom ::: blocks and configuration files like .pre-commit-config.yaml.\"\n",
      "\n",
      "SUBMIT(headers, code_blocks, summary)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 headers\n",
      "Found 2 code blocks\n",
      "\n",
      "Structure: The document consists of a directory tree followed by specific file contents for the stanfordnlp-dspy project. It includes API documentation for dspy.Adapter and dspy.ChatAdapter using custom ::: bloc...\n"
     ]
    }
   ],
   "source": [
    "# Define a custom tool function\n",
    "def regex_extract(text: str, pattern: str, flags: int = 0) -> list:\n",
    "    \"\"\"Extract all matches of regex pattern from text.\n",
    "    \n",
    "    Args:\n",
    "        text: Source text to search\n",
    "        pattern: Regex pattern string\n",
    "        flags: Regex flags (e.g., re.IGNORECASE=2)\n",
    "    \n",
    "    Returns:\n",
    "        List of match groups or full matches\n",
    "    \"\"\"\n",
    "    import re\n",
    "    compiled = re.compile(pattern, flags)\n",
    "    matches = compiled.findall(text)\n",
    "    return matches\n",
    "\n",
    "\n",
    "class ExtractWithCustomTool(dspy.Signature):\n",
    "    \"\"\"Extract specific patterns using custom regex tool.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Use regex_extract() to find all markdown headers\n",
    "    2. Use regex_extract() to find all code blocks\n",
    "    3. Summarize structure\n",
    "    \"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"Documentation to analyze\")\n",
    "    headers: list = dspy.OutputField(desc=\"All markdown headers found\")\n",
    "    code_blocks: list = dspy.OutputField(desc=\"All code block languages found\")\n",
    "    structure_summary: str = dspy.OutputField(desc=\"Summary of document structure\")\n",
    "\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=ExtractWithCustomTool,\n",
    "    interpreter=interpreter,\n",
    "    tools=[regex_extract],  # Pass custom tool here\n",
    "    max_iterations=15,\n",
    "    max_llm_calls=20,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(docs=dspy_docs[:10000])  # First 10KB for demo\n",
    "    print(f\"\\nFound {len(result.headers)} headers\")\n",
    "    print(f\"Found {len(result.code_blocks)} code blocks\")\n",
    "    print(f\"\\nStructure: {result.structure_summary[:200]}...\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-comparison",
   "metadata": {},
   "source": [
    "## 11. RLM vs Direct LLM Comparison\n",
    "\n",
    "| Aspect | Direct LLM | RLM |\n",
    "|--------|-----------|-----|\n",
    "| **Context size** | ~128K tokens | Virtually unlimited |\n",
    "| **Attention** | Dilutes over long context | Focused (code selects snippets) |\n",
    "| **Cost** | High (all tokens in context) | Lower (targeted sub-LLM calls) |\n",
    "| **Accuracy** | Lower on long docs | Higher (targeted analysis) |\n",
    "| **Verifiability** | Black box | Transparent (full trajectory) |\n",
    "| **Tool use** | Limited | Full Python + custom tools |\n",
    "| **Iterative refinement** | Manual (chat) | Automated (code loops) |\n",
    "| **Structured output** | Prompt-dependent | Type-enforced via Signature |\n",
    "\n",
    "### When to use RLM:\n",
    "- ✅ Documents > 50KB\n",
    "- ✅ Need structured extraction (lists, dicts, nested data)\n",
    "- ✅ Multi-step analysis (filter → extract → validate)\n",
    "- ✅ Need programmatic validation or computation\n",
    "- ✅ Repetitive analysis across many documents\n",
    "\n",
    "### When NOT to use RLM:\n",
    "- ❌ Simple Q&A on short text (< 1K tokens)\n",
    "- ❌ Creative writing or brainstorming\n",
    "- ❌ Single-turn classification tasks\n",
    "- ❌ Real-time low-latency requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-best-practices",
   "metadata": {},
   "source": [
    "## 12. RLM Best Practices\n",
    "\n",
    "### Signature Design\n",
    "\n",
    "1. **Describe the strategy** in the docstring:\n",
    "   ```python\n",
    "   \"\"\"First use regex to find X, then llm_query() on relevant sections,\n",
    "   finally aggregate results with llm_query_batched().\"\"\"\n",
    "   ```\n",
    "\n",
    "2. **Explicit type annotations**: Use `list`, `dict`, `int` for structured outputs\n",
    "\n",
    "3. **Input field descriptions**: Help RLM understand what data it's working with\n",
    "\n",
    "### Parameter Tuning\n",
    "\n",
    "| Parameter | Typical Range | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| `max_iterations` | 10-50 | Complex docs need more iterations |\n",
    "| `max_llm_calls` | 20-100 | Primary cost control |\n",
    "| `max_output_chars` | 10K-100K | Prevents output flooding |\n",
    "\n",
    "### Debugging Workflow\n",
    "\n",
    "1. **Start with `verbose=True`**: See real-time reasoning and code\n",
    "2. **Inspect `result.trajectory`**: Full execution history\n",
    "3. **Test on subsets**: Use `docs[:5000]` before full runs\n",
    "4. **Check sandbox logs**: Modal shows actual execution\n",
    "5. **Validate tools**: Test custom tools independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-conclusion",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook demonstrated the full capabilities of **dspy.RLM**:\n",
    "\n",
    "1. **Basic code generation** - LLM writes and executes Python\n",
    "2. **Long document analysis** - Process 80KB+ documents efficiently\n",
    "3. **Parallel processing** - `llm_query_batched()` for speed\n",
    "4. **Stateful reasoning** - Multi-step workflows with persistent variables\n",
    "5. **Trajectory inspection** - Full transparency into reasoning\n",
    "6. **Custom tools** - Extend sandbox capabilities\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- RLM treats long context as an **environment**, not input\n",
    "- Code navigates data; `llm_query()` understands semantics\n",
    "- The **trajectory** provides unprecedented observability\n",
    "- **Modal sandbox** provides secure, scalable execution\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try RLM on your own long documents\n",
    "- Build custom tools for your domain\n",
    "- Experiment with different strategies in Signature docstrings\n",
    "- Use trajectory data to iteratively improve prompts\n",
    "\n",
    "---\n",
    "\n",
    "**Reference**: [Recursive Language Models](https://arxiv.org/abs/2501.123) (Zhang, Kraska, Khattab, 2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlm-dspy-modal (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
