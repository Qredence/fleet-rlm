{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f71b95b",
   "metadata": {},
   "source": [
    "# RLM with Modal Sandbox (DSPy 3.1.3)\n",
    "\n",
    "This tutorial shows how to use **`dspy.RLM`** (Recursive Language Model) with [Modal](https://modal.com) for secure, sandboxed code execution in the cloud.\n",
    "\n",
    "**What is RLM?** RLM is an inference strategy where the LLM writes Python code to programmatically explore data, call sub-LLMs over snippets, and iteratively build up answers — instead of feeding long contexts directly into the model.\n",
    "\n",
    "**Why Modal?** By default, `dspy.RLM` uses a local Deno/Pyodide WASM sandbox. Modal lets you run that code in an isolated cloud container with configurable resources, dependencies, and secrets.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Implement a `ModalInterpreter` that satisfies DSPy's `CodeInterpreter` protocol\n",
    "2. Use `modal.Sandbox` to execute code inside an ephemeral cloud container\n",
    "3. Run an RLM agent that writes and executes code remotely\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python 3.10+**\n",
    "- **Modal account**: Sign up at [modal.com](https://modal.com) and run `modal setup`\n",
    "- **Modal secret**: Create a secret named `LITELLM` that contains the environment variables used by DSPy/LiteLLM:\n",
    "  - `DSPY_LM_MODEL` (e.g., `openai/gemini-3-flash-preview`)\n",
    "  - `DSPY_LM_API_BASE` (your LiteLLM proxy base URL)\n",
    "  - `DSPY_LLM_API_KEY` (API key for the proxy/provider)\n",
    "  - optional: `DSPY_LM_MAX_TOKENS`\n",
    "\n",
    "  Example (run in a terminal):\n",
    "  ```bash\n",
    "  modal secret create LITELLM \\\n",
    "    DSPY_LM_MODEL=... \\\n",
    "    DSPY_LM_API_BASE=... \\\n",
    "    DSPY_LLM_API_KEY=... \\\n",
    "    DSPY_LM_MAX_TOKENS=...\n",
    "  ```\n",
    "\n",
    "- **Security note**: don’t hard-code API keys in notebooks, and don’t print them. If a key was ever pasted into a notebook/chat, rotate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc01761",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qU \"dspy==3.1.3\" modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6451ed",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "We configure one LM locally for the *planner* (the model that writes Python code each iteration).\n",
    "\n",
    "This notebook expects the following environment variables to be set **locally** (for the planner):\n",
    "- `DSPY_LM_MODEL`\n",
    "- `DSPY_LM_API_BASE`\n",
    "- `DSPY_LLM_API_KEY`\n",
    "- optional: `DSPY_LM_MAX_TOKENS`\n",
    "\n",
    "The same variables are also injected into the Modal sandbox via the `LITELLM` secret, so any sandbox-side LM calls (via tool-bridged `llm_query`) use identical credentials without hard-coding secrets in the notebook.\n",
    "\n",
    "**Important**: Modal secrets are only available *inside* Modal containers/sandboxes. They do **not** automatically set environment variables for your local notebook kernel.\n",
    "This notebook will try to load a local `.env` from the project root (if present) to configure the planner LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fe5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Planner LM configured: openai/gemini-3-flash-preview\n",
      "  (Tip: never print API keys.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "\n",
    "\n",
    "# ---- Locate project root and add src/ to path ----\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _find_project_root(Path.cwd())\n",
    "src_path = PROJECT_ROOT / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# ---- Configure the planner LM using the package ----\n",
    "from fleet_rlm import configure_planner_from_env  # noqa: E402\n",
    "\n",
    "PLANNER_READY = configure_planner_from_env(env_file=PROJECT_ROOT / \".env\")\n",
    "\n",
    "if PLANNER_READY:\n",
    "    print(f\"✓ Planner LM configured: {dspy.settings.lm.model}\")\n",
    "    print(\"  (Tip: never print API keys.)\")\n",
    "else:\n",
    "    print(\n",
    "        \"⚠ Planner LM not configured. Set DSPY_LM_MODEL and DSPY_LLM_API_KEY\\n\"\n",
    "        \"  in your .env file or environment, then re-run this cell.\"\n",
    "    )\n",
    "\n",
    "# We'll pass `modal.Secret.from_name('LITELLM')` into the sandbox so the *remote*\n",
    "# Python REPL can access the same environment variables without hard-coding them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d05203",
   "metadata": {},
   "source": [
    "### Optional: sanity-check the Modal secret (without leaking it)\n",
    "\n",
    "The snippet below confirms that the `LITELLM` secret is mounted in Modal by checking for the *presence* of environment variables. It deliberately does **not** print secret values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030754c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret env presence: {\"DSPY_LM_MODEL\": true, \"DSPY_LM_API_BASE\": true, \"DSPY_LLM_API_KEY\": true, \"DSPY_LM_MAX_TOKENS\": true}\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "\n",
    "# Sandboxes require an App when created from a local environment.\n",
    "app = modal.App.lookup(\"dspy-rlm-secret-check\", create_if_missing=True)\n",
    "\n",
    "sb = modal.Sandbox.create(\n",
    "    app=app,\n",
    "    secrets=[modal.Secret.from_name(\"LITELLM\")],\n",
    "    timeout=60,\n",
    ")\n",
    "try:\n",
    "    code = r\"\"\"\n",
    "import json, os\n",
    "keys = [\n",
    "  'DSPY_LM_MODEL',\n",
    "  'DSPY_LM_API_BASE',\n",
    "  'DSPY_LLM_API_KEY',\n",
    "  'DSPY_LM_MAX_TOKENS',\n",
    "]\n",
    "print(json.dumps({k: bool(os.environ.get(k)) for k in keys}))\n",
    "\"\"\"\n",
    "    p = sb.exec(\"python\", \"-c\", code, timeout=60)\n",
    "    p.wait()\n",
    "    print(\"Secret env presence:\", p.stdout.read().strip())\n",
    "finally:\n",
    "    sb.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aca65b",
   "metadata": {},
   "source": [
    "### Don’t print secrets\n",
    "\n",
    "This is **unsafe**:\n",
    "- `print(os.environ[\"DSPY_LLM_API_KEY\"])`\n",
    "\n",
    "Instead, verify the secret is present (and optionally its length), without revealing the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2fdf1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPY_LLM_API_KEY: {\"present\": true, \"length\": 67}\n"
     ]
    }
   ],
   "source": [
    "app = modal.App.lookup(\"dspy-rlm-secret-check\", create_if_missing=True)\n",
    "\n",
    "sb = modal.Sandbox.create(\n",
    "    app=app,\n",
    "    secrets=[modal.Secret.from_name(\"LITELLM\")],\n",
    "    timeout=60,\n",
    ")\n",
    "try:\n",
    "    code = r\"\"\"\n",
    "import json, os\n",
    "key = os.environ.get('DSPY_LLM_API_KEY', '')\n",
    "print(json.dumps({'present': bool(key), 'length': len(key)}))\n",
    "\"\"\"\n",
    "    p = sb.exec(\"python\", \"-c\", code, timeout=60)\n",
    "    p.wait()\n",
    "    print(\"DSPY_LLM_API_KEY:\", p.stdout.read().strip())\n",
    "finally:\n",
    "    sb.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039c470",
   "metadata": {},
   "source": [
    "## 3. The Modal Sandbox Driver\n",
    "\n",
    "Modal Sandboxes are ephemeral containers. We use a **driver program** pattern (from [Modal's code interpreter example](https://modal.com/docs/examples/simple_code_interpreter)):\n",
    "\n",
    "1. A Python driver script runs inside the sandbox, reading JSON commands from `stdin`.\n",
    "2. For each command, it `exec()`s the code, captures stdout/stderr, and checks for `SUBMIT()` calls.\n",
    "3. It writes the result as JSON to `stdout`.\n",
    "\n",
    "This keeps state between iterations (variables persist in the `globals` dict) — exactly what RLM needs.\n",
    "\n",
    "The driver implementation is in the `fleet_rlm.driver` module, which we'll import and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff761307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports from fleet_rlm package:\n",
      "  ModalInterpreter   – fleet_rlm.interpreter\n",
      "  sandbox_driver     – fleet_rlm.driver\n",
      "  Signatures         – ExtractArchitecture, ExtractAPIEndpoints,\n",
      "                       FindErrorPatterns, ExtractWithCustomTool,\n",
      "                       AnalyzeLongDocument, SummarizeLongDocument\n",
      "  Tools              – regex_extract\n",
      "  Chunking           – chunk_by_size, chunk_by_headers,\n",
      "                       chunk_by_timestamps, chunk_by_json_keys\n",
      "\n",
      "✓ Modal config: App=dspy-rlm-modal, Image=debian_slim + numpy, pandas\n"
     ]
    }
   ],
   "source": [
    "# Import the full fleet_rlm package\n",
    "from fleet_rlm.interpreter import ModalInterpreter\n",
    "from fleet_rlm.driver import sandbox_driver\n",
    "from fleet_rlm.signatures import (\n",
    "    ExtractArchitecture,\n",
    "    ExtractAPIEndpoints,\n",
    "    FindErrorPatterns,\n",
    "    ExtractWithCustomTool,\n",
    "    AnalyzeLongDocument,\n",
    "    SummarizeLongDocument,\n",
    ")\n",
    "from fleet_rlm.tools import regex_extract\n",
    "from fleet_rlm.chunking import (\n",
    "    chunk_by_size,\n",
    "    chunk_by_headers,\n",
    "    chunk_by_timestamps,\n",
    "    chunk_by_json_keys,\n",
    ")\n",
    "\n",
    "import modal\n",
    "\n",
    "# Create the sandbox Image (DSPy is NOT needed inside — llm_query() is bridged to the host)\n",
    "# NOTE: We no longer create a global MODAL_APP here.\n",
    "# modal.App.lookup() returns a transient handle whose _client expires between\n",
    "# Jupyter cells.  Instead, ModalInterpreter defers the lookup to start() time\n",
    "# via the `app_name` parameter, keeping the client fresh.\n",
    "SANDBOX_IMAGE = modal.Image.debian_slim().pip_install(\"numpy\", \"pandas\")\n",
    "MODAL_APP_NAME = \"dspy-rlm-modal\"\n",
    "\n",
    "print(\"✓ Imports from fleet_rlm package:\")\n",
    "print(f\"  ModalInterpreter   – {ModalInterpreter.__module__}\")\n",
    "print(f\"  sandbox_driver     – {sandbox_driver.__module__}\")\n",
    "print(\"  Signatures         – ExtractArchitecture, ExtractAPIEndpoints,\")\n",
    "print(\"                       FindErrorPatterns, ExtractWithCustomTool,\")\n",
    "print(\"                       AnalyzeLongDocument, SummarizeLongDocument\")\n",
    "print(\"  Tools              – regex_extract\")\n",
    "print(\"  Chunking           – chunk_by_size, chunk_by_headers,\")\n",
    "print(\"                       chunk_by_timestamps, chunk_by_json_keys\")\n",
    "print()\n",
    "print(f\"✓ Modal config: App={MODAL_APP_NAME}, Image=debian_slim + numpy, pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d3b08",
   "metadata": {},
   "source": [
    "## 4. The `ModalInterpreter`\n",
    "\n",
    "This class implements DSPy's [`CodeInterpreter`](https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/code_interpreter.py) protocol. The protocol requires:\n",
    "\n",
    "| Method | Purpose |\n",
    "|---|---|\n",
    "| `tools` (property) | Dict of callable tools available in the sandbox |\n",
    "| `start()` | Initialize resources (idempotent) |\n",
    "| `execute(code, variables)` | Run code, return stdout or `FinalOutput` |\n",
    "| `shutdown()` | Release resources |\n",
    "\n",
    "Our implementation creates a `modal.Sandbox`, launches the driver program, and communicates via stdin/stdout JSON messages.\n",
    "\n",
    "### Key Features\n",
    "- **Context manager** — use `with ModalInterpreter(...) as interp:` for automatic cleanup\n",
    "- **Sandbox-side helpers** — `peek()`, `grep()`, `chunk_by_size()`, `chunk_by_headers()`, buffers, volume I/O are injected into the sandbox automatically\n",
    "- **Volume support** — Volumes V2 mount at `/data/` for persistent storage\n",
    "- **Sensitive data redaction** — API keys are automatically masked in logs\n",
    "- **Timeout control** — configurable `timeout` and `idle_timeout` for sandboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85af30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModalInterpreter Configuration\n",
      "============================================================\n",
      "Image: Image(<function _Image.pip_install.<locals>.build_dockerfile at 0x1156034c0>)\n",
      "App name: dspy-rlm-modal\n",
      "\n",
      "Features:\n",
      "  • Context manager support (with ... as interp:)\n",
      "  • Deferred App.lookup() — avoids stale _client across cells\n",
      "  • Modal sandbox lifecycle management (idempotent start)\n",
      "  • JSON protocol communication with driver\n",
      "  • Sandbox-side helpers (peek, grep, chunk, buffers, volume)\n",
      "  • Custom tools support\n",
      "  • Volume support (Volumes V2) mounted at /data/\n",
      "  • upload_to_volume() — batch upload local dirs/files\n",
      "  • Timeout & idle_timeout configuration\n",
      "  • Sensitive text redaction (API keys masked)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# The ModalInterpreter class has been imported from the package above.\n",
    "# It implements DSPy's CodeInterpreter protocol with these key features:\n",
    "\n",
    "print(\"ModalInterpreter Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Image: {SANDBOX_IMAGE}\")\n",
    "print(f\"App name: {MODAL_APP_NAME}\")\n",
    "print()\n",
    "print(\"Features:\")\n",
    "print(\"  • Context manager support (with ... as interp:)\")\n",
    "print(\"  • Deferred App.lookup() — avoids stale _client across cells\")\n",
    "print(\"  • Modal sandbox lifecycle management (idempotent start)\")\n",
    "print(\"  • JSON protocol communication with driver\")\n",
    "print(\"  • Sandbox-side helpers (peek, grep, chunk, buffers, volume)\")\n",
    "print(\"  • Custom tools support\")\n",
    "print(\"  • Volume support (Volumes V2) mounted at /data/\")\n",
    "print(\"  • upload_to_volume() — batch upload local dirs/files\")\n",
    "print(\"  • Timeout & idle_timeout configuration\")\n",
    "print(\"  • Sensitive text redaction (API keys masked)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea592a",
   "metadata": {},
   "source": [
    "### Sandbox-Side Helpers\n",
    "\n",
    "The driver automatically injects these helper functions into the sandbox's `globals`, so the LLM-generated code can call them directly:\n",
    "\n",
    "| Helper | Purpose |\n",
    "|---|---|\n",
    "| `peek(text, start, length)` | Inspect a slice of a large string |\n",
    "| `grep(text, pattern, context)` | Case-insensitive line search with optional context |\n",
    "| `chunk_by_size(text, size, overlap)` | Fixed-size chunking inside the sandbox |\n",
    "| `chunk_by_headers(text, pattern)` | Header-based section splitting |\n",
    "| `add_buffer(name, value)` | Accumulate data across iterations |\n",
    "| `get_buffer(name)` | Retrieve buffered data |\n",
    "| `clear_buffer(name)` | Reset a named buffer |\n",
    "| `save_to_volume(path, content)` | Persist data to `/data/` volume |\n",
    "| `load_from_volume(path)` | Load data from `/data/` volume |\n",
    "\n",
    "These helpers follow the RLM principle: the LLM writes code that **navigates** data programmatically, sending only relevant snippets to `llm_query()` for semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87bac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. peek(): peek(doc, 0, 80): 'Line 1: Introduction to DSPy\\nLine 2: DSPy is a framework for programming languag'\n",
      "\n",
      "2. grep(): grep(doc, 'dspy'): ['Line 1: Introduction to DSPy', 'Line 2: DSPy is a framework for programming language models']\n",
      "\n",
      "3. chunk_by_size(): chunk_by_size: 4 chunks\n",
      "  Chunk 0: 80 chars\n",
      "  Chunk 1: 80 chars\n",
      "  Chunk 2: 80 chars\n",
      "  Chunk 3: 40 chars\n",
      "\n",
      "4. Buffers: Buffered findings: ['Found module: ChainOfThought', 'Found optimizer: BootstrapFewShot']\n",
      "After clear: []\n",
      "\n",
      "\n",
      "✓ All sandbox-side helpers working correctly\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate sandbox-side helpers by running code that uses them directly.\n",
    "# This cell shows how the LLM-generated code can use these helpers inside the sandbox.\n",
    "\n",
    "with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interp:\n",
    "    interp.start()\n",
    "\n",
    "    # 1. peek() — inspect a slice of the document\n",
    "    code_peek = '''\n",
    "doc = \"\"\"Line 1: Introduction to DSPy\n",
    "Line 2: DSPy is a framework for programming language models\n",
    "Line 3: It provides modules like ChainOfThought, Predict\n",
    "Line 4: Optimizers include BootstrapFewShot, MIPRO\n",
    "Line 5: RLM treats context as an external environment\"\"\"\n",
    "\n",
    "result = peek(doc, 0, 80)\n",
    "print(\"peek(doc, 0, 80):\", repr(result))\n",
    "'''\n",
    "    out = interp.execute(code_peek)\n",
    "    print(\"1. peek():\", out)\n",
    "\n",
    "    # 2. grep() — search for patterns\n",
    "    code_grep = \"\"\"\n",
    "matches = grep(doc, \"dspy\", context=0)\n",
    "print(\"grep(doc, 'dspy'):\", matches)\n",
    "\"\"\"\n",
    "    out = interp.execute(code_grep)\n",
    "    print(\"2. grep():\", out)\n",
    "\n",
    "    # 3. chunk_by_size() — split text into fixed-size chunks\n",
    "    code_chunk = \"\"\"\n",
    "chunks = chunk_by_size(doc, 80, 10)\n",
    "print(f\"chunk_by_size: {len(chunks)} chunks\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"  Chunk {i}: {len(c)} chars\")\n",
    "\"\"\"\n",
    "    out = interp.execute(code_chunk)\n",
    "    print(\"3. chunk_by_size():\", out)\n",
    "\n",
    "    # 4. Buffers — accumulate data across iterations\n",
    "    code_buffers = \"\"\"\n",
    "add_buffer(\"findings\", \"Found module: ChainOfThought\")\n",
    "add_buffer(\"findings\", \"Found optimizer: BootstrapFewShot\")\n",
    "result = get_buffer(\"findings\")\n",
    "print(\"Buffered findings:\", result)\n",
    "clear_buffer(\"findings\")\n",
    "print(\"After clear:\", get_buffer(\"findings\"))\n",
    "\"\"\"\n",
    "    out = interp.execute(code_buffers)\n",
    "    print(\"4. Buffers:\", out)\n",
    "\n",
    "print(\"\\n✓ All sandbox-side helpers working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d1da1",
   "metadata": {},
   "source": [
    "### Host-Side Chunking Strategies\n",
    "\n",
    "The `fleet_rlm.chunking` module provides four pure-function chunking strategies that work both on the host and inside the sandbox (stdlib-only, no external dependencies):\n",
    "\n",
    "| Strategy | Function | Best for |\n",
    "|---|---|---|\n",
    "| Fixed-size | `chunk_by_size(text, size, overlap)` | Generic text, logs |\n",
    "| Header-based | `chunk_by_headers(text, pattern)` | Markdown, structured docs |\n",
    "| Timestamp-based | `chunk_by_timestamps(text, pattern)` | Log files with timestamps |\n",
    "| JSON keys | `chunk_by_json_keys(text)` | JSON objects |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "441775b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. chunk_by_size(text, 80, overlap=20):\n",
      "   Chunk 0 (80 chars): # Introduction\n",
      "DSPy is a framework for programming...\n",
      "   Chunk 1 (80 chars): models.\n",
      "\n",
      "## Modules\n",
      "ChainOfThought, Predict, and R...\n",
      "   Chunk 2 (80 chars): ore modules.\n",
      "\n",
      "## Optimizers\n",
      "BootstrapFewShot, MIPR...\n",
      "   Chunk 3 (80 chars): esianSignatureOptimizer help tune prompts.\n",
      "\n",
      "## Adv...\n",
      "   Chunk 4 (48 chars): treats long context as an external environment.\n",
      "...\n",
      "\n",
      "2. chunk_by_headers(text):\n",
      "   Section 0: # Introduction (52 chars)\n",
      "   Section 1: ## Modules (52 chars)\n",
      "   Section 2: ## Optimizers (74 chars)\n",
      "   Section 3: ## Advanced (51 chars)\n",
      "\n",
      "3. chunk_by_timestamps(log_text):\n",
      "   Entry 0: ts='2024-01-15' (72 chars)\n",
      "   Entry 1: ts='2024-01-15' (56 chars)\n",
      "   Entry 2: ts='2024-01-15' (36 chars)\n",
      "\n",
      "4. chunk_by_json_keys(json_text):\n",
      "   name (str): \"DSPy\"\n",
      "   version (str): \"3.1\"\n",
      "   modules (list): [\n",
      "  \"Predict\",\n",
      "  \"CoT\"\n",
      "]\n",
      "\n",
      "✓ All chunking strategies demonstrated\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate host-side chunking strategies from fleet_rlm.chunking\n",
    "\n",
    "sample_md = \"\"\"# Introduction\n",
    "DSPy is a framework for programming language models.\n",
    "\n",
    "## Modules\n",
    "ChainOfThought, Predict, and ReAct are core modules.\n",
    "\n",
    "## Optimizers\n",
    "BootstrapFewShot, MIPRO, and BayesianSignatureOptimizer help tune prompts.\n",
    "\n",
    "## Advanced\n",
    "RLM treats long context as an external environment.\n",
    "\"\"\"\n",
    "\n",
    "# 1. chunk_by_size — fixed-size chunks with overlap (returns list[str])\n",
    "print(\"1. chunk_by_size(text, 80, overlap=20):\")\n",
    "chunks = chunk_by_size(sample_md, 80, overlap=20)\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"   Chunk {i} ({len(c)} chars): {c[:50]}...\")\n",
    "\n",
    "# 2. chunk_by_headers — split by markdown headers (returns list[dict])\n",
    "print(\"\\n2. chunk_by_headers(text):\")\n",
    "sections = chunk_by_headers(sample_md)\n",
    "for i, s in enumerate(sections):\n",
    "    print(f\"   Section {i}: {s['header'] or '(preamble)'} ({len(s['content'])} chars)\")\n",
    "\n",
    "# 3. chunk_by_timestamps — split log text by timestamp (returns list[dict])\n",
    "sample_log = \"\"\"2024-01-15 10:00:00 Starting process\n",
    "Processing item 1\n",
    "Processing item 2\n",
    "2024-01-15 10:01:00 Checkpoint reached\n",
    "Processing item 3\n",
    "2024-01-15 10:02:00 Process complete\n",
    "\"\"\"\n",
    "print(\"\\n3. chunk_by_timestamps(log_text):\")\n",
    "log_chunks = chunk_by_timestamps(sample_log)\n",
    "for i, c in enumerate(log_chunks):\n",
    "    print(f\"   Entry {i}: ts={c['timestamp']!r} ({len(c['content'])} chars)\")\n",
    "\n",
    "# 4. chunk_by_json_keys — split JSON (returns list[dict])\n",
    "sample_json = '{\"name\": \"DSPy\", \"version\": \"3.1\", \"modules\": [\"Predict\", \"CoT\"]}'\n",
    "print(\"\\n4. chunk_by_json_keys(json_text):\")\n",
    "json_chunks = chunk_by_json_keys(sample_json)\n",
    "for c in json_chunks:\n",
    "    print(f\"   {c['key']} ({c['value_type']}): {c['content']}\")\n",
    "\n",
    "print(\"\\n✓ All chunking strategies demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Basic RLM Demo: Code Generation\n",
    "\n",
    "A simple example showing RLM writing Python code to solve a problem.\n",
    "\n",
    "> **Note:** We use `with ModalInterpreter(...) as interp:` context manager for automatic cleanup.\n",
    "> This ensures the sandbox is shut down even if an error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "basic-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:17 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: The question asks for the first 12 Fibonacci numbers, separated by commas. I will write a simple Python script to calculate these numbers. By convention, the Fibonacci sequence starts with 0 and 1, or sometimes 1 and 1. I will provide the sequence starting from 0, 1, 1, 2... and confirm if 12 numbers are generated.\n",
      "Code:\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    while len(fib_sequence) < n:\n",
      "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "    return fib_sequence[:n]\n",
      "\n",
      "first_12 = fibonacci(12)\n",
      "result = \", \".join(map(str, first_12))\n",
      "print(result)\n",
      "```\n",
      "2026/02/08 12:38:17 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The previous step successfully calculated the first 12 Fibonacci numbers starting from 0: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89. I will now submit this result as requested.\n",
      "Code:\n",
      "```python\n",
      "SUBMIT(\"0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\")\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL ANSWER: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\n"
     ]
    }
   ],
   "source": [
    "# Ensure the planner LM is configured\n",
    "if not PLANNER_READY and dspy.settings.lm is None:\n",
    "    raise RuntimeError(\"Planner LM not configured\")\n",
    "\n",
    "# Context manager pattern — interpreter.shutdown() is called automatically\n",
    "with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interpreter:\n",
    "    rlm = dspy.RLM(\n",
    "        signature=\"question -> answer\",\n",
    "        interpreter=interpreter,\n",
    "        max_iterations=15,\n",
    "        max_llm_calls=30,\n",
    "        verbose=True,\n",
    "    )\n",
    "    result = rlm(\n",
    "        question=\"What are the first 12 Fibonacci numbers? Return as comma-separated.\"\n",
    "    )\n",
    "    print(\"\\nFINAL ANSWER:\", result.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Core Capability: Long Document Analysis\n",
    "\n",
    "RLM treats long documents as an external environment. The document lives in the sandbox,\n",
    "code navigates and extracts relevant sections, and only snippets are sent to llm_query().\n",
    "\n",
    "### Use Case: Extract DSPy Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "long-doc-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractArchitecture signature fields:\n",
      "  ← docs: str\n",
      "  ← query: str\n",
      "  → modules: list\n",
      "  → optimizers: list\n",
      "  → design_principles: str\n",
      "\n",
      "✓ Loaded DSPy docs: 81,397 chars from dspy-doc.txt\n"
     ]
    }
   ],
   "source": [
    "# ExtractArchitecture was imported from fleet_rlm.signatures above.\n",
    "# Let's inspect its fields:\n",
    "print(\"ExtractArchitecture signature fields:\")\n",
    "for name, field in ExtractArchitecture.model_fields.items():\n",
    "    prefix = (\n",
    "        \"→\"\n",
    "        if field.json_schema_extra\n",
    "        and field.json_schema_extra.get(\"__dspy_field_type\") == \"output\"\n",
    "        else \"←\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  {prefix} {name}: {field.annotation.__name__ if hasattr(field.annotation, '__name__') else field.annotation}\"\n",
    "    )\n",
    "\n",
    "docs_path = PROJECT_ROOT / \"rlm_content\" / \"dspy-knowledge\" / \"dspy-doc.txt\"\n",
    "with open(docs_path, \"r\") as f:\n",
    "    dspy_docs = f.read()\n",
    "\n",
    "print(f\"\\n✓ Loaded DSPy docs: {len(dspy_docs):,} chars from {docs_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. Parallel Processing with llm_query_batched()\n",
    "\n",
    "Process multiple chunks in parallel for dramatic speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "batched-queries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:19 INFO dspy.predict.rlm: RLM iteration 1/20\n",
      "Reasoning: I will start by exploring the content of `docs` to understand how the API documentation is structured. Since the total length is about 81k characters, I can read a significant portion of it, but I'll begin with the first few thousand characters to get a sense of the formatting and identify where the actual endpoint/method definitions are located. I'll also check if there are distinct sections for classes, methods, or REST endpoints.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Total length: {len(docs)}\")\n",
      "print(\"First 2000 characters:\")\n",
      "print(docs[:2000])\n",
      "```\n",
      "2026/02/08 12:38:20 INFO dspy.predict.rlm: RLM iteration 2/20\n",
      "Reasoning: The `docs` variable appears to be a directory listing followed by the content of various Markdown files. Since the goal is to extract API endpoints (classes, methods, and their parameters/usage), I need to split the document into logical sections (likely per file) and then use the LLM to process these sections. \n",
      "\n",
      "First, I'll search for where the actual file content begins, as the initial 2000 characters are just a file tree. I'll look for markers like \"File: \" or typical Markdown headers that indicate the start of a document's content.\n",
      "Code:\n",
      "```python\n",
      "# Check more of the docs to see how file contents are delimited\n",
      "print(docs[2000:6000])\n",
      "```\n",
      "2026/02/08 12:38:20 INFO dspy.predict.rlm: RLM iteration 3/20\n",
      "Reasoning: The documentation structure uses `================================================\\nFILE: <filename>\\n================================================` as a delimiter between files. I will identify all files under the `docs/api/` directory, as these are most likely to contain the API endpoint (class/method) definitions. I'll split the document into segments corresponding to these files and then use a batched LLM query to extract the detailed endpoint information from each.\n",
      "\n",
      "First, I'll extract all file contents and filter for those in `docs/api/`. I'll also look at a sample of one of these API files to confirm the internal structure (parameters, usage, etc.).\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Split the docs by the file delimiter\n",
      "file_sections = re.split(r'={48,}\\nFILE: (.*?)\\n={48,}', docs)\n",
      "\n",
      "# The first element is the directory tree, skip it.\n",
      "# The rest will be [filename, content, filename, content, ...]\n",
      "api_docs = {}\n",
      "for i in range(1, len(file_sections), 2):\n",
      "    filename = file_sections[i].strip()\n",
      "    content = file_sections[i+1].strip()\n",
      "    if 'docs/api/' in filename:\n",
      "        api_docs[filename] = content\n",
      "\n",
      "print(f\"Found {len(api_docs)} API documentation files.\")\n",
      "# Print keys to see the structure\n",
      "print(\"Sample API files:\")\n",
      "for k in list(api_docs.keys())[:5]:\n",
      "    print(f\" - {k}\")\n",
      "\n",
      "# Check the content of one specific API file to understand the format\n",
      "sample_key = next(iter(api_docs))\n",
      "print(f\"\\n--- Content of {sample_key} ---\")\n",
      "print(api_docs[sample_key][:1000])\n",
      "```\n",
      "2026/02/08 12:38:20 INFO dspy.predict.rlm: RLM iteration 4/20\n",
      "Reasoning: I have identified 61 API-related documentation files. The files are delimited by a consistent header pattern. I will now examine a few more substantial API files (like `Predict.md` or `BootstrapFewShot.md`) to understand how parameters and usage patterns are described. Then, I will prepare a batched LLM query to process all relevant files and extract the API endpoints, parameters, and usage patterns as requested.\n",
      "\n",
      "Plan:\n",
      "1. Examine a representative sample of API files to see the format of parameters and usage.\n",
      "2. Group the files for batched analysis.\n",
      "3. Construct a prompt for the LLM to extract the specific fields: endpoint name, description, parameters, and usage patterns.\n",
      "Code:\n",
      "```python\n",
      "# Look at a few more specific API files to see the structure of class/method definitions\n",
      "target_files = [\n",
      "    'docs/docs/api/modules/Predict.md',\n",
      "    'docs/docs/api/optimizers/BootstrapFewShot.md',\n",
      "    'docs/docs/api/primitives/Example.md'\n",
      "]\n",
      "\n",
      "for file_path in target_files:\n",
      "    if file_path in api_docs:\n",
      "        print(f\"\\n==================== {file_path} ====================\")\n",
      "        print(api_docs[file_path][:1500])\n",
      "    else:\n",
      "        print(f\"\\nFile {file_path} not found in api_docs keys.\")\n",
      "\n",
      "```\n",
      "2026/02/08 12:38:20 INFO dspy.predict.rlm: RLM iteration 5/20\n",
      "Reasoning: The exploration of the `docs` variable reveals that the API documentation is structured as a collection of Markdown files, where many files use the `::: dspy.ClassName` directive (likely for `mkdocstrings`) to reference the actual implementation and docstrings. However, since the goal is to extract endpoints, parameters, and usage patterns from the provided *text*, I need to find the files that actually contain descriptive text, parameters, and examples.\n",
      "\n",
      "The directory listing shows several sections: `adapters`, `evaluation`, `experimental`, `models`, `modules`, `optimizers`, `primitives`, `signatures`, `tools`, and `utils`. The files like `docs/docs/api/modules/Predict.md` seen so far contain placeholders (`::: dspy.Predict`) which might not have the full parameter lists if the tool that renders them wasn't run on this specific text block. However, I should check if other parts of the document (perhaps in the source `dspy/` directory or later in the file) contain the docstrings or if the `llm_query` can infer the details from the context of the files or other descriptive sections.\n",
      "\n",
      "I will now scan the `dspy/` source directory content (which was also in the `docs` variable) to see if the actual docstrings are available there, or search for files in `docs/` that have more descriptive content than just the `::: dspy` directives. I will also use `llm_query` to extract info from a few files to see if it can resolve the \"endpoints\" from the context provided in the whole `docs` string.\n",
      "\n",
      "I'll start by checking the content of a `utils` file and a `signature` file to see if they are more descriptive. Then I will proceed to batch process the files.\n",
      "Code:\n",
      "```python\n",
      "# Let's check a few more files that might have more text content than the mkdocstrings placeholders\n",
      "check_files = [\n",
      "    'docs/docs/api/signatures/Signature.md',\n",
      "    'docs/docs/api/utils/inspect_history.md',\n",
      "    'docs/docs/api/modules/ChainOfThought.md'\n",
      "]\n",
      "\n",
      "for f in check_files:\n",
      "    if f in api_docs:\n",
      "        print(f\"\\n--- {f} ---\")\n",
      "        print(api_docs[f])\n",
      "\n",
      "# Also check if the dspy/ source code files are included in 'docs' variable content\n",
      "# Search for a source file like dspy/predict/predict.py\n",
      "source_match = re.search(r'={48,}\\nFILE: dspy/predict/predict.py\\n={48,}', docs)\n",
      "if source_match:\n",
      "    print(\"\\nSource code for dspy/predict/predict.py is available.\")\n",
      "else:\n",
      "    print(\"\\nSource code files do not appear to be included in the 'docs' variable content.\")\n",
      "\n",
      "# Let's try to extract from one file using the LLM to see what it can find\n",
      "sample_prompt = f\"\"\"Analyze the following API documentation segment and extract API endpoints (classes/functions).\n",
      "For each, provide: name, description, parameters (name, type, description), and a usage pattern.\n",
      "\n",
      "Doc Content:\n",
      "{api_docs.get('docs/docs/api/modules/Predict.md', 'Empty')}\n",
      "\"\"\"\n",
      "print(\"\\n--- LLM Sample Extraction ---\")\n",
      "print(llm_query(sample_prompt))\n",
      "```\n",
      "2026/02/08 12:38:20 INFO dspy.predict.rlm: RLM iteration 6/20\n",
      "Reasoning: The previous extraction shows that while the Markdown files themselves contain placeholders like `::: dspy.Predict`, the LLM is capable of leveraging its internal knowledge of the `dspy` library or inferring details from the context of the directory structure and file names to provide accurate API endpoint information (including parameters and usage). \n",
      "\n",
      "The goal is to extract API endpoints, their parameters, and usage patterns. I have identified 61 API-related documentation files. I will now group these files and use `llm_query_batched` to process them. Since many files represent a single class or utility, I will focus on the most important ones: `adapters`, `evaluation`, `models`, `modules`, `optimizers`, `primitives`, `signatures`, and `tools`. I'll exclude `index.md`.\n",
      "\n",
      "I will structure the prompts to ensure the LLM returns a structured list of endpoints for each file. Then I will combine the results into the final list.\n",
      "\n",
      "Plan:\n",
      "1. Filter the 61 files to remove non-essential ones (like `index.md`).\n",
      "2. Create batches of files to send to the LLM.\n",
      "3. Construct a specific prompt that requests JSON-like output (though I will parse the results) for name, description, parameters, and usage.\n",
      "4. Collect and format the final `api_endpoints` list.\n",
      "Code:\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Filter api_docs to remove index files and focus on core API components\n",
      "filtered_api_files = {k: v for k, v in api_docs.items() if not k.endswith('index.md')}\n",
      "\n",
      "# Organize files into batches for llm_query_batched\n",
      "# We have 60 files, so we can process them in batches of 10-15\n",
      "file_list = list(filtered_api_files.keys())\n",
      "prompts = []\n",
      "\n",
      "for file_path in file_list:\n",
      "    content = filtered_api_files[file_path]\n",
      "    prompt = f\"\"\"Extract API endpoint information from this documentation file: {file_path}.\n",
      "Content:\n",
      "{content}\n",
      "\n",
      "For each class or primary function defined in this file, extract:\n",
      "1. name (the full name, e.g., 'dspy.Predict')\n",
      "2. description (a brief summary of what it does)\n",
      "3. parameters (a list of objects with 'name', 'type', and 'description')\n",
      "4. usage (a short Python code snippet demonstrating usage)\n",
      "\n",
      "Return the data as a JSON list of objects.\n",
      "\"\"\"\n",
      "    prompts.append(prompt)\n",
      "\n",
      "# Execute batched query\n",
      "# Note: Since there are many files, I'll start with a subset to ensure quality and stay within limits\n",
      "results = llm_query_batched(prompts)\n",
      "\n",
      "# Parse results and aggregate\n",
      "all_endpoints = []\n",
      "for res in results:\n",
      "    try:\n",
      "        # Try to find JSON in the response\n",
      "        json_match = re.search(r'\\[.*\\]', res, re.DOTALL)\n",
      "        if json_match:\n",
      "            data = json.loads(json_match.group(0))\n",
      "            all_endpoints.extend(data)\n",
      "        else:\n",
      "            # If no JSON block, it might just be the text. \n",
      "            # I'll use a second pass or manual parsing if needed, \n",
      "            # but usually LLMs follow the \"JSON list\" instruction well.\n",
      "            print(f\"No JSON found in response: {res[:200]}...\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error parsing response: {e}\")\n",
      "\n",
      "print(f\"Extracted {len(all_endpoints)} endpoints.\")\n",
      "if all_endpoints:\n",
      "    print(\"Sample endpoint:\", json.dumps(all_endpoints[0], indent=2))\n",
      "\n",
      "# Final submission\n",
      "SUBMIT(all_endpoints)\n",
      "```\n",
      "2026/02/08 12:38:21 INFO dspy.predict.rlm: RLM iteration 7/20\n",
      "Reasoning: The previous attempt failed because I tried to make 60 LLM calls in a single batch, which exceeded the limit of 30. I need to be more selective about which files to analyze or group multiple files into a single prompt to reduce the total number of calls. \n",
      "\n",
      "I will categorize the files by their parent directory (e.g., `modules`, `optimizers`, `primitives`) and process each category in a single prompt. This will significantly reduce the number of LLM calls while still covering the breadth of the API. I'll also focus on the core directories that define the primary DSPy API.\n",
      "\n",
      "Categories to process:\n",
      "1. `adapters`\n",
      "2. `evaluation`\n",
      "3. `models`\n",
      "4. `modules`\n",
      "5. `optimizers`\n",
      "6. `primitives`\n",
      "7. `signatures`\n",
      "8. `tools`\n",
      "9. `utils`\n",
      "\n",
      "I will combine the content of all files in each category into one prompt.\n",
      "Code:\n",
      "```python\n",
      "import json\n",
      "import re\n",
      "\n",
      "# Categorize the API documentation files\n",
      "categories = {}\n",
      "for path, content in api_docs.items():\n",
      "    if path.endswith('index.md'):\n",
      "        continue\n",
      "    # Extract category from path: docs/docs/api/{category}/{file}.md\n",
      "    parts = path.split('/')\n",
      "    if len(parts) >= 4:\n",
      "        category = parts[3]\n",
      "        if category not in categories:\n",
      "            categories[category] = []\n",
      "        categories[category].append((path, content))\n",
      "\n",
      "# Prepare prompts for each category\n",
      "prompts = []\n",
      "category_names = sorted(categories.keys())\n",
      "\n",
      "for cat in category_names:\n",
      "    cat_content = \"\"\n",
      "    for path, content in categories[cat]:\n",
      "        cat_content += f\"\\n--- File: {path} ---\\n{content}\\n\"\n",
      "    \n",
      "    prompt = f\"\"\"Analyze the following DSPy API documentation for the '{cat}' category.\n",
      "Extract the API endpoints (classes and functions).\n",
      "\n",
      "For each endpoint, provide:\n",
      "1. \"name\": Full name (e.g., 'dspy.Predict' or 'dspy.evaluate.Evaluate')\n",
      "2. \"description\": Brief summary\n",
      "3. \"parameters\": List of objects with \"name\", \"type\", and \"description\"\n",
      "4. \"usage\": A short Python code snippet\n",
      "\n",
      "Return the results as a single JSON list of objects.\n",
      "\n",
      "Documentation:\n",
      "{cat_content}\n",
      "\"\"\"\n",
      "    prompts.append(prompt)\n",
      "\n",
      "# This will use ~9-10 LLM calls, well within the 30 call limit\n",
      "results = llm_query_batched(prompts)\n",
      "\n",
      "all_endpoints = []\n",
      "for res in results:\n",
      "    try:\n",
      "        # Extract JSON list from the response\n",
      "        match = re.search(r'\\[\\s*\\{.*\\}\\s*\\]', res, re.DOTALL)\n",
      "        if match:\n",
      "            data = json.loads(match.group(0))\n",
      "            all_endpoints.extend(data)\n",
      "        else:\n",
      "            # Fallback if the LLM didn't wrap in a clean block\n",
      "            print(\"Could not find JSON list in response segment.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error parsing JSON: {e}\")\n",
      "\n",
      "print(f\"Total endpoints extracted: {len(all_endpoints)}\")\n",
      "if all_endpoints:\n",
      "    # Final verification and submission\n",
      "    SUBMIT(all_endpoints)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 endpoints\n",
      "  - {'name': 'dspy.Adapter', 'description': 'The base class for DSPy adapters, responsible for formatting signatures, demos, and interactions into prompts and parsing the model outputs back into structured data.', 'parameters': [], 'usage': 'import dspy\\n# Creating a custom adapter by inheriting from dspy.Adapter\\nclass MyAdapter(dspy.Adapter):\\n    def format(self, signature, demos, inputs):\\n        # Custom formatting logic\\n        pass'}\n",
      "  - {'name': 'dspy.ChatAdapter', 'description': 'An adapter designed for Chat-based Language Models. It formats the signature and data into a structured conversation history (system, user, and assistant messages).', 'parameters': [], 'usage': 'import dspy\\n# Using ChatAdapter with a model\\nchat_adapter = dspy.ChatAdapter()\\n# Usually passed to dspy.settings.configure(adapter=chat_adapter)'}\n",
      "  - {'name': 'dspy.JSONAdapter', 'description': 'An adapter that formats the input and output requirements into JSON structures, often used to enforce stricter schema adherence for models that support JSON mode or instruction following.', 'parameters': [], 'usage': 'import dspy\\n# Configure DSPy to use the JSONAdapter for structured data tasks\\ndspy.settings.configure(adapter=dspy.JSONAdapter())'}\n",
      "  - {'name': 'dspy.TwoStepAdapter', 'description': 'An adapter that implements a two-step reasoning process, typically separating the thought/reasoning phase from the final answer generation.', 'parameters': [], 'usage': 'import dspy\\n# Configure the environment to use a two-step prompting approach\\ntwo_step = dspy.TwoStepAdapter()\\ndspy.settings.configure(adapter=two_step)'}\n",
      "  - {'name': 'dspy.evaluate.answer_exact_match', 'description': 'A metric function that checks if the predicted answer exactly matches the gold answer after normalization.', 'parameters': [{'name': 'example', 'type': 'dspy.Example', 'description': \"The ground truth example containing the 'answer' field.\"}, {'name': 'pred', 'type': 'dspy.Prediction', 'description': \"The prediction containing the generated 'answer' field.\"}, {'name': 'trace', 'type': 'Optional[Any]', 'description': 'Optional trace for optimization/compilation (defaults to None).'}], 'usage': 'import dspy\\nfrom dspy.evaluate import answer_exact_match\\n\\nexample = dspy.Example(answer=\"Berlin\")\\npred = dspy.Prediction(answer=\"berlin\")\\nscore = answer_exact_match(example, pred) # Returns True'}\n"
     ]
    }
   ],
   "source": [
    "# ExtractAPIEndpoints was imported from fleet_rlm.signatures above.\n",
    "# Uses context manager for automatic cleanup.\n",
    "\n",
    "with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interpreter:\n",
    "    rlm = dspy.RLM(\n",
    "        signature=ExtractAPIEndpoints,\n",
    "        interpreter=interpreter,\n",
    "        max_iterations=20,\n",
    "        max_llm_calls=30,\n",
    "        verbose=True,\n",
    "    )\n",
    "    result = rlm(docs=dspy_docs)\n",
    "    print(f\"Found {len(result.api_endpoints)} endpoints\")\n",
    "    for ep in result.api_endpoints[:5]:\n",
    "        print(f\"  - {ep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "## 8. Stateful Multi-Step Reasoning\n",
    "\n",
    "RLM maintains state across iterations. Variables persist, enabling multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "multi-step-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:23 INFO dspy.predict.rlm: RLM iteration 1/30\n",
      "Reasoning: I need to find and categorize error patterns from the documentation provided in `docs`. The `docs` variable seems to contain a directory structure followed by actual documentation content. \n",
      "\n",
      "First, I will explore the content of `docs` to identify sections related to troubleshooting, common errors, or guides that might contain error information. I'll print the first and last few thousand characters and look for keywords like \"error\", \"exception\", \"troubleshoot\", \"fix\", or \"issue\".\n",
      "\n",
      "Plan:\n",
      "1.  Read the first 5000 and last 5000 characters of `docs` to understand the structure.\n",
      "2.  Search for keywords related to errors.\n",
      "3.  Identify specific files or sections that contain error patterns.\n",
      "4.  Use `llm_query` to extract structured data from those sections.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Total length of docs: {len(docs)}\")\n",
      "print(\"--- Start of docs ---\")\n",
      "print(docs[:2000])\n",
      "print(\"\\n--- End of docs ---\")\n",
      "print(docs[-2000:])\n",
      "\n",
      "# Check for keywords\n",
      "keywords = [\"error\", \"troubleshoot\", \"exception\", \"fail\", \"issue\", \"solution\", \"fix\"]\n",
      "for kw in keywords:\n",
      "    count = docs.lower().count(kw)\n",
      "    print(f\"Keyword '{kw}' count: {count}\")\n",
      "```\n",
      "2026/02/08 12:38:24 INFO dspy.predict.rlm: RLM iteration 2/30\n",
      "Reasoning: The initial check showed a low count of explicit keywords like \"error\" or \"troubleshoot\" in the start and end sections, but the total length of the document is over 81,000 characters. The directory structure suggests there are many Markdown files (API docs, optimizers, modules). Error patterns might be described within these files (e.g., common failure modes for specific optimizers or modules).\n",
      "\n",
      "I will scan the document for file boundaries (marked by `================================================\\nFILE: ...`) and look for files that are likely to contain troubleshooting information or error descriptions. Since the directory listing is at the start, I'll extract the full list of files first to see if there's a dedicated `FAQ.md`, `troubleshooting.md`, or similar, and then search for error-related content within the file contents.\n",
      "\n",
      "Plan:\n",
      "1. Extract all file paths from the document.\n",
      "2. Search for sections/files that contain keywords like \"error\", \"fail\", \"issue\", \"raise\", \"exception\", \"incorrect\", \"debug\".\n",
      "3. Use `llm_query` on promising sections to extract error-category-solution mappings.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Extract all file markers and their positions\n",
      "file_markers = list(re.finditer(r\"================================================\\nFILE: (.*?)\\n================================================\", docs))\n",
      "\n",
      "print(f\"Found {len(file_markers)} files in the document.\")\n",
      "\n",
      "# List the first 20 files to get an idea of the content\n",
      "for m in file_markers[:20]:\n",
      "    print(m.group(1))\n",
      "\n",
      "# Let's search for \"error\" in the whole text with context to see where they appear\n",
      "error_indices = [m.start() for m in re.finditer(r\"error\", docs, re.IGNORECASE)]\n",
      "print(f\"Total occurrences of 'error': {len(error_indices)}\")\n",
      "\n",
      "# Check for specific files that might contain troubleshooting or FAQs\n",
      "for m in file_markers:\n",
      "    filename = m.group(1).lower()\n",
      "    if any(k in filename for k in [\"faq\", \"trouble\", \"error\", \"guide\", \"common\"]):\n",
      "        print(f\"Potential interesting file: {m.group(1)}\")\n",
      "\n",
      "# Since there aren't many \"error\" keywords, maybe the docs use \"fails\", \"doesn't work\", \"issue\"\n",
      "# Let's look for \"raise \" which often indicates error handling documentation\n",
      "raise_indices = [m.start() for m in re.finditer(r\"raise \", docs)]\n",
      "print(f\"Total occurrences of 'raise ': {len(raise_indices)}\")\n",
      "```\n",
      "2026/02/08 12:38:24 INFO dspy.predict.rlm: RLM iteration 3/30\n",
      "Reasoning: The word \"error\" appears only 8 times and \"raise\" only 3 times in over 80,000 characters, which is surprisingly low for documentation. This suggests that error patterns might not be explicitly labeled as such in a centralized troubleshooting guide. Instead, they might be described as \"limitations,\" \"failure modes,\" or \"best practices\" within the individual API and module documentation.\n",
      "\n",
      "I will now:\n",
      "1. Extract the specific locations (surrounding context) of the 8 \"error\" occurrences and 3 \"raise\" occurrences to see if they point to common patterns.\n",
      "2. Search for other synonyms that might indicate error/solution patterns, such as \"failure\", \"incorrect\", \"fix\", \"missing\", \"invalid\", \"ensure\", \"avoid\".\n",
      "3. Check if there are any files with names like `FAQ`, `Tips`, or `Common Issues` that I might have missed (although my previous search didn't find \"common\").\n",
      "4. Inspect the content of the \"adapters\" and \"optimizers\" files more closely, as these are common sources of integration errors.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "def get_context(text, index, window=500):\n",
      "    start = max(0, index - window)\n",
      "    end = min(len(text), index + window)\n",
      "    return text[start:end]\n",
      "\n",
      "print(\"--- Context for 'error' ---\")\n",
      "for m in re.finditer(r\"error\", docs, re.IGNORECASE):\n",
      "    print(f\"Index {m.start()}:\\n{get_context(docs, m.start(), 200)}\\n{'-'*40}\")\n",
      "\n",
      "print(\"\\n--- Context for 'raise' ---\")\n",
      "for m in re.finditer(r\"raise\", docs, re.IGNORECASE):\n",
      "    print(f\"Index {m.start()}:\\n{get_context(docs, m.start(), 200)}\\n{'-'*40}\")\n",
      "\n",
      "# Search for common problem-solution indicators\n",
      "other_keywords = [\"failure\", \"incorrect\", \"missing\", \"invalid\", \"ensure\", \"avoid\", \"debug\"]\n",
      "for kw in other_keywords:\n",
      "    indices = [m.start() for m in re.finditer(kw, docs, re.IGNORECASE)]\n",
      "    print(f\"Keyword '{kw}' count: {len(indices)}\")\n",
      "    if indices:\n",
      "        print(f\"Sample for '{kw}': {get_context(docs, indices[0], 150)}\")\n",
      "        print('-' * 20)\n",
      "\n",
      "# List some optimizer and adapter files to see if they have \"Notes\" or \"Tips\"\n",
      "potential_files = [\n",
      "    \"docs/docs/api/adapters/JSONAdapter.md\",\n",
      "    \"docs/docs/api/optimizers/BootstrapFewShot.md\",\n",
      "    \"docs/docs/api/modules/Predict.md\"\n",
      "]\n",
      "\n",
      "for pf in potential_files:\n",
      "    # Find file content\n",
      "    pattern = rf\"================================================\\nFILE: {re.escape(pf)}\\n================================================\\n(.*?)(?===|$)\"\n",
      "    match = re.search(pattern, docs, re.DOTALL)\n",
      "    if match:\n",
      "        print(f\"\\n--- Content Preview of {pf} ---\")\n",
      "        print(match.group(1)[:1000])\n",
      "```\n",
      "2026/02/08 12:38:24 INFO dspy.predict.rlm: RLM iteration 4/30\n",
      "Reasoning: The initial keyword search for \"error\" and \"raise\" yielded very few direct hits in the code and templates, but it's clear the documentation is extensive (81k characters). Error patterns in DSPy documentation are often found in the conceptual or API guides where they discuss \"limitations,\" \"best practices,\" or how to handle specific edge cases (like JSON formatting, parsing failures, or optimization issues).\n",
      "\n",
      "I noticed that many files in the `docs/docs/api/` directory (like `JSONAdapter.md`, `BootstrapFewShot.md`, `Evaluate.md`) likely contain implicit error patterns—for instance, how the system handles malformed output or why an optimizer might fail to improve a program. \n",
      "\n",
      "I will now:\n",
      "1.  Read the content of several key API files to identify patterns related to failures, validation, or troubleshooting.\n",
      "2.  Use `llm_query` to scan these specific files for common errors, causes, and solutions.\n",
      "3.  Synthesize the results into the required categories.\n",
      "\n",
      "Let's start by reading the content of a few representative documentation files.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Files that are likely to contain \"best practices\" or \"troubleshooting\" hints\n",
      "target_files = [\n",
      "    \"docs/docs/api/adapters/JSONAdapter.md\",\n",
      "    \"docs/docs/api/optimizers/BootstrapFewShot.md\",\n",
      "    \"docs/docs/api/modules/Predict.md\",\n",
      "    \"docs/docs/api/evaluation/Evaluate.md\",\n",
      "    \"docs/docs/api/adapters/ChatAdapter.md\",\n",
      "    \"docs/docs/api/modules/ProgramOfThought.md\"\n",
      "]\n",
      "\n",
      "content_to_analyze = \"\"\n",
      "for pf in target_files:\n",
      "    pattern = rf\"================================================\\nFILE: {re.escape(pf)}\\n================================================\\n(.*?)(?===|$)\"\n",
      "    match = re.search(pattern, docs, re.DOTALL)\n",
      "    if match:\n",
      "        content_to_analyze += f\"\\n\\n--- FILE: {pf} ---\\n\" + match.group(1)\n",
      "\n",
      "# Also check for \"Note\", \"Warning\", \"Important\" which often precede error-related info\n",
      "notes_indices = [m.start() for m in re.finditer(r\"Note:|Warning:|Important:\", docs, re.IGNORECASE)]\n",
      "print(f\"Found {len(notes_indices)} Notes/Warnings.\")\n",
      "\n",
      "# Sample some notes\n",
      "for idx in notes_indices[:5]:\n",
      "    print(f\"Context: {get_context(docs, idx, 150)}\\n\")\n",
      "\n",
      "# Prompt the LLM to find error patterns in the gathered documentation\n",
      "prompt = f\"\"\"\n",
      "Identify common errors, their causes, and solutions from the following documentation.\n",
      "Look for:\n",
      "- Common mistakes users make.\n",
      "- Why certain modules or optimizers might fail.\n",
      "- Parsing or formatting errors.\n",
      "- Requirements for inputs/outputs.\n",
      "\n",
      "Documentation:\n",
      "{content_to_analyze}\n",
      "\n",
      "Output the results in a structured format:\n",
      "Category: [Error Type]\n",
      "Cause: [Why it happens]\n",
      "Solution: [How to fix it]\n",
      "\"\"\"\n",
      "\n",
      "analysis = llm_query(prompt)\n",
      "print(\"\\n--- LLM Analysis ---\")\n",
      "print(analysis)\n",
      "```\n",
      "2026/02/08 12:38:24 INFO dspy.predict.rlm: RLM iteration 5/30\n",
      "Reasoning: The previous iteration used `llm_query` on a selection of key documentation files to identify common error patterns. The LLM successfully categorized several types of errors including:\n",
      "1. **Adapter Formatting & Parsing Errors**: Caused by schema mismatches in JSON/Chat formats.\n",
      "2. **Optimizer Compilation Failure**: Caused by insufficient datasets or undefined metrics.\n",
      "3. **State Persistence & Configuration Mismatch**: Caused by loading incompatible state dicts into modified architectures.\n",
      "4. **Program Flow & Execution Errors**: Caused by input formatting issues or incorrect async handling.\n",
      "5. **Evaluation & Metric Failures**: Caused by mismatches between module outputs and metric function requirements.\n",
      "\n",
      "I have identified at least 5 distinct error categories with their causes and solutions. The total number of errors found is also clear from these categories. I will now structure the final `error_categories` dictionary and count the total errors. Each category described by the LLM represents a distinct pattern of error/solution.\n",
      "\n",
      "I will perform one final check of the documentation to see if there are any other glaringly obvious \"troubleshooting\" or \"faq\" sections I missed in the 117 files to ensure high recall, though the current categories are already very robust for this dataset. Specifically, I'll search for \"Troubleshooting\" or \"FAQ\" in the filenames again. Then I will submit.\n",
      "\n",
      "Plan:\n",
      "1. Verify if any dedicated FAQ/Troubleshooting files exist (re-check with case-insensitive search).\n",
      "2. Format the `error_categories` dictionary.\n",
      "3. Count the errors (5 categories identified).\n",
      "4. Submit.\n",
      "Code:\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Re-checking for any missed FAQ/Troubleshooting files\n",
      "interesting_files = []\n",
      "for m in file_markers:\n",
      "    fname = m.group(1).lower()\n",
      "    if \"faq\" in fname or \"trouble\" in fname or \"common\" in fname:\n",
      "        interesting_files.append(m.group(1))\n",
      "\n",
      "print(f\"Interesting files found: {interesting_files}\")\n",
      "\n",
      "# Based on the LLM analysis and file exploration, I'll compile the results.\n",
      "# The LLM identified 5 major error patterns.\n",
      "\n",
      "error_categories = {\n",
      "    \"Adapter Formatting & Parsing Errors\": {\n",
      "        \"cause\": \"Input data doesn't match expected schema or failure to follow format_field_structure/user_message_output_requirements.\",\n",
      "        \"solution\": \"Validate inputs against schema, ensure LLM receives correct system instructions via format_system_message, and use the parse method to debug string-to-object conversion.\"\n",
      "    },\n",
      "    \"Optimizer Compilation Failure\": {\n",
      "        \"cause\": \"Insufficient dataset, missing metrics, or module parameters (Predict) not having clearly defined input/output fields.\",\n",
      "        \"solution\": \"Ensure training data covers all required fields, verify program has named parameters, and check if the optimizer can identify layers via get_params.\"\n",
      "    },\n",
      "    \"State Persistence & Configuration Mismatch\": {\n",
      "        \"cause\": \"Attempting to load_state with a dictionary that doesn't match the current program architecture or Signature.\",\n",
      "        \"solution\": \"Use dump_state/save frequently, verify structure with named_predictors before loading, and use update_config to reset LM/prompt settings.\"\n",
      "    },\n",
      "    \"Program Flow & Execution Errors\": {\n",
      "        \"cause\": \"Incorrectly formatted inputs passed to forward/call or unhandled asynchronous execution with acall/aforward.\",\n",
      "        \"solution\": \"Use inspect_history to debug LLM communication, ensure signatures allow for intermediate thought steps in ProgramOfThought, and verify async handling.\"\n",
      "    },\n",
      "    \"Evaluation & Metric Failures\": {\n",
      "        \"cause\": \"Module output keys do not match keys expected by the metric function or improperly formatted evaluation datasets.\",\n",
      "        \"solution\": \"Standardize predictor outputs to match metric requirements and use reset_copy on modules to prevent data leakage during evaluation.\"\n",
      "    }\n",
      "}\n",
      "\n",
      "total_errors_found = len(error_categories)\n",
      "\n",
      "SUBMIT(error_categories, total_errors_found)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 error patterns\n",
      "Adapter Formatting & Parsing Errors: 2 errors\n",
      "Optimizer Compilation Failure: 2 errors\n",
      "State Persistence & Configuration Mismatch: 2 errors\n",
      "Program Flow & Execution Errors: 2 errors\n",
      "Evaluation & Metric Failures: 2 errors\n"
     ]
    }
   ],
   "source": [
    "# FindErrorPatterns was imported from fleet_rlm.signatures above.\n",
    "\n",
    "\n",
    "def get_errors(docs: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"Find and categorize error patterns in documentation using RLM.\n",
    "\n",
    "    Args:\n",
    "        docs: The documentation text to analyze\n",
    "        verbose: Whether to print RLM traces\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'total' count and 'categories' dict\n",
    "    \"\"\"\n",
    "    with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interpreter:\n",
    "        rlm = dspy.RLM(\n",
    "            signature=FindErrorPatterns,\n",
    "            interpreter=interpreter,\n",
    "            max_iterations=30,\n",
    "            max_llm_calls=40,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        result = rlm(docs=docs)\n",
    "        return {\n",
    "            \"total\": result.total_errors_found,\n",
    "            \"categories\": result.error_categories,\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "error_data = get_errors(dspy_docs)\n",
    "\n",
    "print(f\"Found {error_data['total']} error patterns\")\n",
    "for cat, errors in error_data[\"categories\"].items():\n",
    "    print(f\"{cat}: {len(errors)} errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "## 9. Inspecting the Trajectory\n",
    "\n",
    "Every RLM result includes a trajectory - complete history of reasoning, code, and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "trajectory-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory (3 steps):\n",
      "\n",
      "\n",
      "Step 1:\n",
      "  Reasoning: I need to summarize the content provided in the `text` variable, which appears to be a directory str...\n",
      "  Code: print(f\"Total length: {len(text)}\")\n",
      "print(\"--- Full Text Sta...\n",
      "\n",
      "Step 2:\n",
      "  Reasoning: The text provides a detailed directory structure of the `stanfordnlp-dspy` repository, specifically ...\n",
      "  Code: prompt = f\"\"\"Summarize the purpose and structure of the proj...\n",
      "\n",
      "Step 3:\n",
      "  Reasoning: The previous step successfully generated a comprehensive and well-structured summary of the `stanfor...\n",
      "  Code: # The summary was already generated in the previous step.\n",
      "# ...\n"
     ]
    }
   ],
   "source": [
    "with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interpreter:\n",
    "    rlm = dspy.RLM(\n",
    "        signature=\"text -> summary\",\n",
    "        interpreter=interpreter,\n",
    "        max_iterations=10,\n",
    "        max_llm_calls=10,\n",
    "        verbose=False,\n",
    "    )\n",
    "    text_sample = dspy_docs[:3000]\n",
    "    result = rlm(text=text_sample)\n",
    "\n",
    "    print(f\"Trajectory ({len(result.trajectory)} steps):\\n\")\n",
    "    for i, step in enumerate(result.trajectory):\n",
    "        print(f\"\\nStep {i + 1}:\")\n",
    "        print(f\"  Reasoning: {step.get('reasoning', 'N/A')[:100]}...\")\n",
    "        print(f\"  Code: {step.get('code', '')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ac8fd",
   "metadata": {},
   "source": [
    "## 10. Long-Context Analysis with Dedicated Signatures\n",
    "\n",
    "The package provides two specialized signatures for long-document workflows:\n",
    "\n",
    "| Signature | Purpose |\n",
    "|---|---|\n",
    "| `AnalyzeLongDocument` | Navigate → query → synthesize findings from a long document |\n",
    "| `SummarizeLongDocument` | Chunk-based summarization with controllable focus |\n",
    "\n",
    "Both signatures are designed so the **LLM-generated sandbox code** uses helpers like `peek()`, `grep()`, `chunk_by_headers()`, and `llm_query()` to explore the document programmatically — only sending relevant snippets to sub-LLMs for semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03efee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:27 INFO dspy.predict.rlm: RLM iteration 1/20\n",
      "Reasoning: I will start by exploring the document's structure. The preview shows a directory structure, suggesting the document might be a collection of files or a repository overview. I'll print the first 2000 characters to understand the format and then look for sections likely to contain information about \"main design decisions\" and \"core abstractions\" (e.g., introduction, architecture, or core modules).\n",
      "Code:\n",
      "```python\n",
      "print(f\"Document length: {len(document)}\")\n",
      "print(\"--- Document Start ---\")\n",
      "print(document[:2000])\n",
      "```\n",
      "2026/02/08 12:38:29 INFO dspy.predict.rlm: RLM iteration 2/20\n",
      "Reasoning: The document appears to be a documentation dump for the `dspy` library, starting with a directory structure of its API. To understand the \"main design decisions and core abstractions,\" I need to find the high-level introductory or conceptual sections. Based on the file tree, `docs/api/index.md`, `docs/api/modules/Module.md`, `docs/api/primitives/Example.md`, and potentially any early \"Introduction\" or \"README\" style text following the directory structure would be most useful. I'll search for where the actual text content begins after the directory listing and look for key architectural terms like \"Signature\", \"Module\", \"Optimizer\", and \"Program\".\n",
      "Code:\n",
      "```python\n",
      "# Look for the end of the directory structure and start of actual content\n",
      "# Based on the preview, it's a long list. Let's find where it ends.\n",
      "import re\n",
      "\n",
      "# Search for the first occurrence of a header or a common start of a markdown file\n",
      "# after the directory tree which uses └── and │\n",
      "lines = document.split('\\n')\n",
      "start_index = 0\n",
      "for i, line in enumerate(lines):\n",
      "    if not any(char in line for char in ['─', '│', '├──', '└──']):\n",
      "        # Simple heuristic: first line with significant text that isn't tree-like\n",
      "        if len(line.strip()) > 20 and i > 50: \n",
      "            start_index = i\n",
      "            break\n",
      "\n",
      "print(f\"Content likely starts around line {start_index}\")\n",
      "print(\"--- Content Preview ---\")\n",
      "print('\\n'.join(lines[start_index:start_index+50]))\n",
      "\n",
      "# Also grep for \"Design\" or \"Abstractions\" to find relevant sections quickly\n",
      "print(\"\\n--- Grep Results ---\")\n",
      "for i, line in enumerate(lines):\n",
      "    if any(keyword in line.lower() for keyword in [\"design\", \"abstraction\", \"signature\", \"optimizer\", \"module\"]):\n",
      "        if len(line) < 100: # avoid printing huge lines\n",
      "            print(f\"Line {i}: {line}\")\n",
      "        if i > 500: # limit output\n",
      "            break\n",
      "```\n",
      "2026/02/08 12:38:29 INFO dspy.predict.rlm: RLM iteration 3/20\n",
      "Reasoning: The document contains multiple files separated by markers like `FILE: <path>`. The directory structure and initial grep suggest that the core abstractions are likely `Signatures`, `Modules`, and `Optimizers`. I need to find the specific files that define these concepts. I'll search for the content of `docs/api/index.md` or any README/Introduction file that explains the high-level design. I will also look for files under `docs/api/signatures/Signature.md`, `docs/api/modules/Module.md`, and `docs/api/optimizers/` to extract the main design decisions.\n",
      "\n",
      "I'll start by listing all the file markers to identify where the documentation for these core components is located. Then I'll read the relevant sections.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Find all file boundaries\n",
      "file_markers = re.findall(r\"================================================\\nFILE: (.*?)\\n================================================\", document)\n",
      "print(\"Files found in document:\")\n",
      "for marker in file_markers:\n",
      "    if \"index.md\" in marker or \"Signature\" in marker or \"Module\" in marker or \"Optimizer\" in marker:\n",
      "        print(marker)\n",
      "\n",
      "# Let's extract and read the content of docs/api/index.md if it exists, \n",
      "# or look for a general introduction.\n",
      "def get_file_content(filename):\n",
      "    pattern = rf\"================================================\\nFILE: {re.escape(filename)}\\n================================================\\n(.*?)(?=\\n================================================|$)\"\n",
      "    match = re.search(pattern, document, re.DOTALL)\n",
      "    return match.group(1) if match else None\n",
      "\n",
      "# Check for index.md or other potential intro files\n",
      "intro_files = [\"docs/api/index.md\", \"docs/api/modules/Module.md\", \"docs/api/signatures/Signature.md\"]\n",
      "for file in intro_files:\n",
      "    content = get_file_content(file)\n",
      "    if content:\n",
      "        print(f\"\\n--- Content of {file} ---\")\n",
      "        print(content[:1500]) # Print first 1500 chars of each\n",
      "```\n",
      "2026/02/08 12:38:29 INFO dspy.predict.rlm: RLM iteration 4/20\n",
      "Reasoning: The previous step successfully identified the file markers but the `get_file_content` function didn't return output for `Signature.md`, likely due to a path mismatch (the file tree showed `docs/docs/api/...` while I tried `docs/api/...`). I need to use the exact paths found in the file markers. I will extract the content of `docs/docs/api/index.md` and `docs/docs/api/modules/Module.md` to identify core abstractions. I will also check for the exact path of the Signature documentation and extract it. These files should contain the definitions of the main design decisions and core abstractions like Signatures, Modules, and Teleprompters/Optimizers.\n",
      "Code:\n",
      "```python\n",
      "# Identify the exact paths from the previously seen directory structure and file markers\n",
      "# Re-scanning for all file markers to be sure about paths\n",
      "import re\n",
      "\n",
      "file_markers = re.findall(r\"FILE: (.*?)\\n\", document)\n",
      "print(\"Actual file paths in document:\")\n",
      "relevant_paths = [path for path in file_markers if any(keyword in path for keyword in [\"index.md\", \"Module.md\", \"Signature.md\", \"Bootstrap\"])]\n",
      "for path in relevant_paths:\n",
      "    print(path)\n",
      "\n",
      "def get_file_content(filename):\n",
      "    # Using a simpler split-based approach to avoid regex escaping issues with long strings\n",
      "    marker = f\"================================================\\nFILE: {filename}\\n================================================\"\n",
      "    if marker not in document:\n",
      "        return None\n",
      "    start_pos = document.find(marker) + len(marker)\n",
      "    end_pos = document.find(\"================================================\", start_pos)\n",
      "    if end_pos == -1:\n",
      "        return document[start_pos:]\n",
      "    return document[start_pos:end_pos]\n",
      "\n",
      "# Extract content from the main intro and core abstraction files\n",
      "for path in relevant_paths[:5]: # Get the first few relevant ones\n",
      "    content = get_file_content(path)\n",
      "    if content:\n",
      "        print(f\"\\n--- CONTENT FROM: {path} ---\")\n",
      "        # Use llm_query to summarize design decisions and abstractions from each section\n",
      "        summary = llm_query(f\"Identify the main design decisions and core abstractions described in this text: {content[:4000]}\")\n",
      "        print(summary)\n",
      "\n",
      "```\n",
      "2026/02/08 12:38:30 INFO dspy.predict.rlm: RLM iteration 5/20\n",
      "Reasoning: The previous exploration successfully identified the core design decisions and abstractions from `docs/docs/api/index.md` and `docs/docs/api/modules/Module.md`. I have a solid understanding of:\n",
      "1. **Design Decisions**: Declarative programming, separation of logic and implementation, systematic optimization (compiling), and the PyTorch-inspired modular paradigm.\n",
      "2. **Core Abstractions**: Signatures, Modules, Optimizers (formerly Teleprompters), Data (Examples), and LM/RM clients.\n",
      "\n",
      "I still need to look more closely at \"Signatures\" and \"Optimizers\" to provide a complete answer. I'll search for `docs/docs/api/signatures/Signature.md` (which I missed earlier due to pathing) and look at the \"Optimizers\" file I found (`docs/docs/api/optimizers/BootstrapFewShot.md`) to see if there's a more general \"Optimizer\" base class or overview. I'll also check if there are any other high-level files that might discuss \"Assertions\" or \"Typed Signatures\" which are often part of DSPy's core.\n",
      "\n",
      "Plan:\n",
      "1. Locate and extract content for Signatures.\n",
      "2. Look for a general Optimizer/Teleprompter introduction.\n",
      "3. Synthesize the findings into the final answer.\n",
      "Code:\n",
      "```python\n",
      "# List all files again to find the exact Signature and Optimizer base paths\n",
      "import re\n",
      "\n",
      "file_markers = re.findall(r\"FILE: (.*?)\\n\", document)\n",
      "target_files = []\n",
      "for path in file_markers:\n",
      "    if \"signatures/Signature.md\" in path or \"optimizers/\" in path:\n",
      "        target_files.append(path)\n",
      "\n",
      "print(\"Target files found:\")\n",
      "for f in target_files:\n",
      "    print(f)\n",
      "\n",
      "def get_file_content(filename):\n",
      "    marker = f\"================================================\\nFILE: {filename}\\n================================================\"\n",
      "    start_pos = document.find(marker)\n",
      "    if start_pos == -1: return None\n",
      "    start_pos += len(marker)\n",
      "    end_pos = document.find(\"================================================\", start_pos)\n",
      "    return document[start_pos:end_pos] if end_pos != -1 else document[start_pos:]\n",
      "\n",
      "# Query specifically about Signatures and Optimizers\n",
      "sig_content = get_file_content(\"docs/docs/api/signatures/Signature.md\")\n",
      "if sig_content:\n",
      "    print(\"\\n--- Analyzing Signatures ---\")\n",
      "    print(llm_query(f\"What are the core design decisions and abstractions related to Signatures in DSPy? {sig_content[:3000]}\"))\n",
      "\n",
      "# Check for a general optimizer file or use BootstrapFewShot as a proxy for the design\n",
      "opt_content = get_file_content(\"docs/docs/api/optimizers/BootstrapFewShot.md\")\n",
      "if opt_content:\n",
      "    print(\"\\n--- Analyzing Optimizers ---\")\n",
      "    print(llm_query(f\"Based on this optimizer documentation, what is the design philosophy of DSPy Optimizers? {opt_content[:3000]}\"))\n",
      "```\n",
      "2026/02/08 12:38:30 INFO dspy.predict.rlm: RLM iteration 6/20\n",
      "Reasoning: I have gathered extensive information about the design decisions and core abstractions of DSPy. \n",
      "1. From `index.md`: I learned about declarative programming, separation of logic from implementation, systematic optimization, and the PyTorch-inspired modularity.\n",
      "2. From `Module.md`: I found details on state management (`load_state`, `save`), the hierarchical structure of modules, and the concept of parameters (prompts/demonstrations as weights).\n",
      "3. From the optimizer analysis: I understood the \"bootstrapping\" philosophy, where models generate their own training data, and the shift from prompt engineering to system optimization via metrics.\n",
      "4. I've identified the core abstractions: Signatures (intent), Modules (structure), Optimizers (compilation/learning), and Data (Examples).\n",
      "\n",
      "One minor detail remains: I didn't get a specific look at \"Signatures\" from a file called `Signature.md` because it wasn't in the specific list I grepped in Step 5 (it might have been missed due to the directory nesting `docs/docs/api/signatures/Signature.md`). However, the `index.md` summary already provided a strong definition. I will quickly verify if I can find the actual file content for Signatures to see if there are additional nuances like \"Typed Signatures\" or \"Field constraints\" which are common design decisions in newer DSPy versions. Then I will synthesize the final answer.\n",
      "Code:\n",
      "```python\n",
      "# Check for the Signature file one last time to ensure no missing design nuances\n",
      "signature_file = \"docs/docs/api/signatures/Signature.md\"\n",
      "content = get_file_content(signature_file)\n",
      "if content:\n",
      "    print(\"--- Signature Details ---\")\n",
      "    print(llm_query(f\"Identify any specific design decisions or nuances for Signatures in this text: {content[:3000]}\"))\n",
      "else:\n",
      "    # If not found at that path, look for it\n",
      "    all_files = re.findall(r\"FILE: (.*?)\\n\", document)\n",
      "    for f in all_files:\n",
      "        if \"Signature.md\" in f:\n",
      "            print(f\"Found Signature file at: {f}\")\n",
      "            print(llm_query(f\"Identify design decisions for Signatures: {get_file_content(f)[:3000]}\"))\n",
      "\n",
      "# Final synthesis of findings\n",
      "```\n",
      "2026/02/08 12:38:30 INFO dspy.predict.rlm: RLM iteration 7/20\n",
      "Reasoning: In the previous step, I failed to print the output of the signature check. I also want to double-check for \"Assertions\" and \"Typed Signatures\" as these are significant design decisions in the framework. I will explicitly print the findings for Signatures and check for any mention of \"Assertions\" or \"Constraints\" to ensure a comprehensive overview. After this, I will have all the necessary components: Declarative philosophy, PyTorch-inspired structure (Modules/Parameters), Signatures (Separation of intent), and Optimizers (Data-driven compilation).\n",
      "Code:\n",
      "```python\n",
      "# Check for the Signature file and Assertions\n",
      "signature_file = \"docs/docs/api/signatures/Signature.md\"\n",
      "sig_content = get_file_content(signature_file)\n",
      "if sig_content:\n",
      "    print(\"--- Signature Details ---\")\n",
      "    print(llm_query(f\"Identify any specific design decisions or nuances for Signatures (like typing or field descriptions): {sig_content[:4000]}\"))\n",
      "else:\n",
      "    # Look for it in the full list if path was slightly off\n",
      "    all_files = [f for f in re.findall(r\"FILE: (.*?)\\n\", document) if \"Signature.md\" in f]\n",
      "    if all_files:\n",
      "        print(f\"Found Signature file at: {all_files[0]}\")\n",
      "        print(llm_query(f\"Identify design decisions for Signatures: {get_file_content(all_files[0])[:4000]}\"))\n",
      "\n",
      "# Briefly check for Assertions/Constraints as they are a core abstraction for reliability\n",
      "assertion_files = [f for f in re.findall(r\"FILE: (.*?)\\n\", document) if \"Assert\" in f or \"Suggest\" in f]\n",
      "if assertion_files:\n",
      "    print(f\"Found Assertion files: {assertion_files}\")\n",
      "    print(llm_query(f\"What role do Assertions and Suggestions play in DSPy's design? {get_file_content(assertion_files[0])[:3000]}\"))\n",
      "\n",
      "# Collect all findings for the final synthesis\n",
      "```\n",
      "2026/02/08 12:38:30 INFO dspy.predict.rlm: RLM iteration 8/20\n",
      "Reasoning: The previous steps have established that DSPy is built on a PyTorch-inspired paradigm, shifting from manual prompting to systematic optimization. I have gathered information on **Modules**, **Optimizers**, and the general **Declarative** philosophy. I attempted to find more specific details on **Signatures** and **Assertions** in the previous step but failed to print the output. I will now explicitly print the analysis for the Signature file (which I found at `docs/docs/api/signatures/Signature.md` via earlier directory listings) and look for any mention of \"Assertions\" or \"Typed\" components to finalize the list of core abstractions and design decisions. Then I will be ready to synthesize the final answer.\n",
      "Code:\n",
      "```python\n",
      "# Re-running the search and ensuring output is printed this time.\n",
      "# Also looking for \"Assertions\" which are a key design decision for program reliability.\n",
      "\n",
      "# 1. Get Signature nuances\n",
      "sig_path = \"docs/docs/api/signatures/Signature.md\"\n",
      "content_sig = get_file_content(sig_path)\n",
      "if content_sig:\n",
      "    print(\"--- Signature Analysis ---\")\n",
      "    print(llm_query(f\"Identify specific design decisions and abstractions for Signatures (e.g., fields, types, inheritance): {content_sig[:4000]}\"))\n",
      "\n",
      "# 2. Check for Assertions/Constraints\n",
      "assertion_paths = [f for f in re.findall(r\"FILE: (.*?)\\n\", document) if \"Assert\" in f or \"Suggest\" in f]\n",
      "if assertion_paths:\n",
      "    print(\"\\n--- Assertion Analysis ---\")\n",
      "    print(llm_query(f\"What is the design role of Assertions/Suggestions? {get_file_content(assertion_paths[0])[:4000]}\"))\n",
      "else:\n",
      "    # If no files, grep document for mentions of \"Assert\" to see if it's a hidden section\n",
      "    print(\"\\n--- Grepping for Assertions ---\")\n",
      "    matches = [line for line in document.split('\\n') if \"dspy.Assert\" in line or \"dspy.Suggest\" in line]\n",
      "    print(matches[:5])\n",
      "\n",
      "# 3. Final synthesis check: Get content from the 'Program' or 'Teleprompter' sections if available\n",
      "prog_path = \"docs/docs/api/modules/Predict.md\" # Often explains the basic unit\n",
      "content_pred = get_file_content(prog_path)\n",
      "if content_pred:\n",
      "    print(\"\\n--- Predictor Analysis ---\")\n",
      "    print(llm_query(f\"How does the Predict module represent a core abstraction? {content_pred[:2000]}\"))\n",
      "```\n",
      "2026/02/08 12:38:30 INFO dspy.predict.rlm: RLM iteration 9/20\n",
      "Reasoning: I have gathered sufficient information to answer the query comprehensively.\n",
      "1. **Main Design Decisions**:\n",
      "    - **Declarative Programming**: Shifting from manual prompt engineering (string manipulation) to defining the *intent* of the program.\n",
      "    - **Separation of Logic and Implementation**: Decoupling the program structure (control flow) from the parameters (prompts/demonstrations).\n",
      "    - **PyTorch-Inspired Paradigm**: Using a modular, stateful, and hierarchical approach (Modules, Parameters, State Management) to build LLM pipelines.\n",
      "    - **Systematic Optimization (Compiling)**: Automating prompt generation and model tuning using data and metrics rather than \"vibes-based\" manual editing.\n",
      "    - **Bootstrapping**: Using teacher models to generate self-training data (traces/demonstrations) for student models.\n",
      "\n",
      "2. **Core Abstractions**:\n",
      "    - **Signatures (`dspy.Signature`)**: Declarative specifications of input/output behavior (the \"what\").\n",
      "    - **Modules (`dspy.Module`, `dspy.Predict`)**: The building blocks of programs that process signatures (the \"how\"). Includes advanced modules like `ChainOfThought` and `ReAct`.\n",
      "    - **Optimizers (formerly Teleprompters)**: Algorithms that \"compile\" programs by optimizing prompts or weights (e.g., `BootstrapFewShot`).\n",
      "    - **Data/Examples (`dspy.Example`)**: Objects representing data points used for training and testing.\n",
      "    - **Parameters**: The instructions and few-shot demonstrations that are treated as learnable weights.\n",
      "    - **LM/RM Clients**: Abstractions for Language Models and Retrieval Models, allowing for easy model swapping.\n",
      "\n",
      "I have explored the `index.md`, `Module.md`, `Predict.md`, and `BootstrapFewShot.md` files which cover these points. I will now synthesize this into the final response.\n",
      "Code:\n",
      "```python\n",
      "findings = [\n",
      "    {\"concept\": \"Declarative Programming\", \"type\": \"Design Decision\", \"description\": \"Defining the intent (what) rather than the prompt string (how).\"},\n",
      "    {\"concept\": \"Separation of Logic and Implementation\", \"type\": \"Design Decision\", \"description\": \"Separating the program's control flow from its prompts and model weights.\"},\n",
      "    {\"concept\": \"PyTorch-Inspired Paradigm\", \"type\": \"Design Decision\", \"description\": \"Structuring LLM programs as modular, hierarchical, and stateful components similar to neural networks.\"},\n",
      "    {\"concept\": \"Systematic Optimization\", \"type\": \"Design Decision\", \"description\": \"Replacing manual prompt engineering with data-driven 'compilation' via Optimizers.\"},\n",
      "    {\"concept\": \"Signatures\", \"type\": \"Core Abstraction\", \"description\": \"Declarative specifications of a task's input/output behavior (e.g., 'question -> answer').\"},\n",
      "    {\"concept\": \"Modules\", \"type\": \"Core Abstraction\", \"description\": \"Composable blocks (like dspy.Predict or dspy.ChainOfThought) that implement signatures and store state.\"},\n",
      "    {\"concept\": \"Optimizers (Teleprompters)\", \"type\": \"Core Abstraction\", \"description\": \"Algorithms that tune program parameters (prompts/examples) to maximize a metric.\"},\n",
      "    {\"concept\": \"Parameters\", \"type\": \"Core Abstraction\", \"description\": \"The tunable parts of a program, such as instructions and few-shot demonstrations, treated like model weights.\"},\n",
      "    {\"concept\": \"Examples\", \"type\": \"Core Abstraction\", \"description\": \"The data format for training, testing, and few-shot traces.\"}\n",
      "]\n",
      "\n",
      "answer = \"\"\"\n",
      "The DSPy framework is built on a fundamental shift from manual 'prompt engineering' to systematic 'language program optimization.' \n",
      "\n",
      "### Main Design Decisions\n",
      "1. **Declarative Programming**: DSPy allows developers to define the *intent* of a task using Signatures rather than writing brittle prompt strings. This abstracts away the underlying LM's specific prompting requirements.\n",
      "2. **Separation of Concerns**: The framework separates the program's logic (the Python code defining the flow) from its implementation (the specific prompts and examples). This allows the same program to be 're-compiled' for different LMs.\n",
      "3. **PyTorch-Inspired Modularity**: Programs are built using a hierarchical structure of `dspy.Module` objects. These modules encapsulate state and parameters, mirroring the mental model of neural network layers.\n",
      "4. **Data-Driven Optimization**: Instead of manual trial-and-error, DSPy uses 'Optimizers' (or Teleprompters) that use training data and metrics to automatically generate the best instructions and few-shot examples for the program.\n",
      "\n",
      "### Core Abstractions\n",
      "- **Signatures (`dspy.Signature`)**: The declarative heart of DSPy. It specifies *what* a module should do by defining input and output fields (e.g., `question -> answer`).\n",
      "- **Modules (`dspy.Module`)**: The structural units of a program. They process Signatures and can be nested. Common modules include `dspy.Predict` (basic inference) and `dspy.ChainOfThought` (reasoning).\n",
      "- **Optimizers (Teleprompters)**: Algorithms like `BootstrapFewShot` that 'compile' a DSPy program. They iterate through data to optimize the 'Parameters' of the modules.\n",
      "- **Parameters**: Unlike neural networks that use floating-point weights, DSPy parameters are the instructions, prefixes, and few-shot demonstrations that guide the LLM.\n",
      "- **Examples (`dspy.Example`)**: A unified data abstraction used to represent inputs, outputs, and intermediate traces for optimization and evaluation.\n",
      "- **LM and RM Clients**: Standardized interfaces for Language Models and Retrieval Models (vector DBs), allowing developers to swap backends with minimal code changes.\n",
      "\"\"\"\n",
      "\n",
      "SUBMIT(findings, answer, 5)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Findings (9 items):\n",
      "  1. {'concept': 'Declarative Programming', 'type': 'Design Decision', 'description': 'Defining the intent (what) rather than the prompt string (how).'}\n",
      "  2. {'concept': 'Separation of Logic and Implementation', 'type': 'Design Decision', 'description': \"Separating the program's control flow from its prompts and model weights.\"}\n",
      "  3. {'concept': 'PyTorch-Inspired Paradigm', 'type': 'Design Decision', 'description': 'Structuring LLM programs as modular, hierarchical, and stateful components similar to neural networks.'}\n",
      "  4. {'concept': 'Systematic Optimization', 'type': 'Design Decision', 'description': \"Replacing manual prompt engineering with data-driven 'compilation' via Optimizers.\"}\n",
      "  5. {'concept': 'Signatures', 'type': 'Core Abstraction', 'description': \"Declarative specifications of a task's input/output behavior (e.g., 'question -> answer').\"}\n",
      "\n",
      "Answer: \n",
      "The DSPy framework is built on a fundamental shift from manual 'prompt engineering' to systematic 'language program optimization.' \n",
      "\n",
      "### Main Design Decisions\n",
      "1. **Declarative Programming**: DSPy allows developers to define the *intent* of a task using Signatures rather than writing brittle prompt ...\n",
      "Sections examined: 5\n"
     ]
    }
   ],
   "source": [
    "# 9a. Long-Context Analysis — AnalyzeLongDocument\n",
    "\n",
    "analyze_sig = AnalyzeLongDocument\n",
    "rlm_analyze = dspy.RLM(\n",
    "    signature=analyze_sig,\n",
    "    interpreter=interp,\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = rlm_analyze(\n",
    "    document=dspy_docs[:30_000],\n",
    "    query=\"What are the main design decisions and core abstractions?\",\n",
    ")\n",
    "print(f\"\\nFindings ({len(result.findings)} items):\")\n",
    "for i, f in enumerate(result.findings[:5]):\n",
    "    print(f\"  {i + 1}. {f}\")\n",
    "print(f\"\\nAnswer: {result.answer[:300]}...\")\n",
    "print(f\"Sections examined: {result.sections_examined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8dd1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:33 INFO dspy.predict.rlm: RLM iteration 1/20\n",
      "Reasoning: I need to summarize a long document (30,000 characters) with a focus on \"DSPy optimizers and how they work\". \n",
      "First, I will inspect the document structure and content to determine how to chunk it effectively. Since the document seems to be a directory structure followed by text, I'll print the first 2000 and last 2000 characters to get a sense of the layout.\n",
      "\n",
      "Plan:\n",
      "1. Examine document structure.\n",
      "2. Split the document into manageable chunks (e.g., 5000-8000 characters).\n",
      "3. Query the LLM for each chunk to extract information related to the focus.\n",
      "4. Consolidate the findings into a summary and key points.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Document length: {len(document)}\")\n",
      "print(f\"Focus: {focus}\")\n",
      "print(\"-\" * 20)\n",
      "print(\"Document Start Preview:\")\n",
      "print(document[:2000])\n",
      "print(\"-\" * 20)\n",
      "print(\"Document End Preview:\")\n",
      "print(document[-2000:])\n",
      "```\n",
      "2026/02/08 12:38:33 INFO dspy.predict.rlm: RLM iteration 2/20\n",
      "Reasoning: The document appears to be a collection of documentation files (likely Markdown) concatenated together, indicated by \"FILE: path/to/file.md\" headers. The focus is specifically on \"DSPy optimizers\". The directory structure shows many optimizer-related files under `docs/docs/api/optimizers/`.\n",
      "\n",
      "I will split the document into chunks based on these file markers to ensure I don't break the context of a specific optimizer's documentation. Since the document is 30,000 characters, and the sub-LLM can handle ~500K characters, I could technically process it in one go, but chunking is safer for \"controllable focus\" and better detail extraction.\n",
      "\n",
      "Plan:\n",
      "1. Split the document by the \"FILE:\" markers.\n",
      "2. Filter the chunks to find those that are relevant to \"optimizers\".\n",
      "3. For each relevant chunk, ask the LLM to extract how the specific optimizer works.\n",
      "4. Synthesize the final summary.\n",
      "\n",
      "First, I'll extract the content and identify where the optimizer sections are.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Split document by the file headers\n",
      "parts = re.split(r'={10,}\\s+FILE: (.*?)\\s+={10,}', document)\n",
      "\n",
      "# parts[0] is everything before the first header\n",
      "# parts[1::2] are the filenames\n",
      "# parts[2::2] are the file contents\n",
      "\n",
      "files = []\n",
      "for i in range(1, len(parts), 2):\n",
      "    files.append({\n",
      "        \"path\": parts[i],\n",
      "        \"content\": parts[i+1]\n",
      "    })\n",
      "\n",
      "print(f\"Total files found: {len(files)}\")\n",
      "\n",
      "# Filter for optimizer related files\n",
      "optimizer_files = [f for f in files if \"optimizers\" in f[\"path\"].lower()]\n",
      "print(f\"Optimizer-related files: {len(optimizer_files)}\")\n",
      "for f in optimizer_files:\n",
      "    print(f\"- {f['path']}\")\n",
      "\n",
      "# If no specific optimizer files were found in the headers, or if the list is small, \n",
      "# I'll check the first chunk (intro) as well.\n",
      "if len(optimizer_files) == 0:\n",
      "    print(\"No specific optimizer files found in headers. Checking first 5000 chars.\")\n",
      "    print(document[:5000])\n",
      "```\n",
      "2026/02/08 12:38:33 INFO dspy.predict.rlm: RLM iteration 3/20\n",
      "Reasoning: The previous step identified 31 files within the document, but only 3 were explicitly listed in the `optimizer_files` list because the document likely got cut off or the headers for other optimizers are formatted differently. Looking at the `directory structure` in the initial preview, there are many more optimizers listed (e.g., `BootstrapFinetune.md`, `COPRO.md`, `MIPRO.md` [if present], `Ensemble.md`, etc.). \n",
      "\n",
      "The `document` length is 30,000 characters. The \"FILE:\" marker splitting logic only caught 3 because the preview showed the end of the file list. However, checking the length of the parts found (31 files) suggests that the `re.split` worked, but maybe the regex or the filtering was too strict, or the document simply ends after the first few optimizer files.\n",
      "\n",
      "Let's inspect the `files` list more closely to see what the other files are and if they contain optimizer information even if \"optimizers\" isn't in the path (though it should be). I'll also check the content of the whole document for keywords like \"optimizer\" or \"teleprompter\" (the old name for optimizers in DSPy) to ensure no information is missed.\n",
      "\n",
      "I will then gather all text related to optimizers and use `llm_query` to summarize how they work.\n",
      "Code:\n",
      "```python\n",
      "# List all files found to see what we have\n",
      "for i, f in enumerate(files):\n",
      "    print(f\"File {i}: {f['path']}\")\n",
      "\n",
      "# Let's search for \"optimizer\" or \"teleprompter\" in all file contents\n",
      "# to catch relevant info in general docs.\n",
      "relevant_content = []\n",
      "for f in files:\n",
      "    if \"optimizer\" in f['content'].lower() or \"teleprompter\" in f['content'].lower() or \"optimizers\" in f['path'].lower():\n",
      "        relevant_content.append(f\"FILE: {f['path']}\\n{f['content']}\")\n",
      "\n",
      "print(f\"\\nFound {len(relevant_content)} files with optimizer-related content.\")\n",
      "\n",
      "# Combine relevant content for a single LLM query if it's small enough\n",
      "combined_relevant_text = \"\\n\\n\".join(relevant_content)\n",
      "print(f\"Combined relevant text length: {len(combined_relevant_text)}\")\n",
      "\n",
      "# If the text is manageable, I'll process it. \n",
      "# Also check if there's significant content in the 'document' that wasn't caught by the FILE: split.\n",
      "# The split might have missed the first section if it didn't start with '== FILE:'.\n",
      "if not document.startswith(\"=\"):\n",
      "    header_index = document.find(\"==========\")\n",
      "    if header_index != -1:\n",
      "        initial_content = document[:header_index]\n",
      "        if \"optimizer\" in initial_content.lower():\n",
      "            print(\"Found optimizer info in the initial document section.\")\n",
      "            combined_relevant_text = initial_content + \"\\n\\n\" + combined_relevant_text\n",
      "\n",
      "# Prompt the LLM to summarize\n",
      "prompt = f\"\"\"\n",
      "Focus Topic: {focus}\n",
      "\n",
      "Based on the provided documentation, explain what DSPy optimizers are and how they work. \n",
      "Detail specific optimizers mentioned (like BootstrapFewShot, etc.) and their mechanisms.\n",
      "\n",
      "Documentation:\n",
      "{combined_relevant_text[:15000]} # Using first half if too long, but 30k total is fine for one query\n",
      "\"\"\"\n",
      "\n",
      "# Since I want to be thorough, I'll print the combined text first to see if I need more chunks.\n",
      "print(\"--- PREVIEW OF RELEVANT TEXT ---\")\n",
      "print(combined_relevant_text[:2000])\n",
      "```\n",
      "2026/02/08 12:38:33 INFO dspy.predict.rlm: RLM iteration 4/20\n",
      "Reasoning: The document splitting revealed that while the directory structure lists many optimizers (COPRO, MIPRO, BootstrapFewShot, etc.), the actual text content provided in the `document` variable seems to be truncated or incomplete, as only the first three optimizer files (`BetterTogether`, `BootstrapFewShot`, and `BootstrapFewShotWithRandomSearch`) have content sections at the end of the document. The total length of the document is 30,000 characters, which is relatively small for a full repository documentation.\n",
      "\n",
      "I have found:\n",
      "1. The directory structure listing various optimizers.\n",
      "2. API reference stubs for `BetterTogether`, `BootstrapFewShot`, and `BootstrapFewShotWithRandomSearch`.\n",
      "3. Some general context in the `dspy.Module` section (which I'll extract next) about how modules (and thus optimizers) manage parameters.\n",
      "\n",
      "I need to perform a semantic search/extraction across the entire document to find all mentions of \"optimizer\" or \"teleprompter\" logic to provide a comprehensive answer, even if the specific markdown files are truncated. I will also look at the `dspy.Module` documentation since optimizers work by manipulating the parameters of these modules.\n",
      "\n",
      "Plan:\n",
      "1. Extract all text sections that mention \"optimizer\", \"compile\", or \"teleprompter\".\n",
      "2. Use the LLM to synthesize how they work based on the available text.\n",
      "3. If information is sparse, I will focus on the mechanisms of the optimizers that *are* documented in the text (BootstrapFewShot family).\n",
      "Code:\n",
      "```python\n",
      "# Search for the term 'compile' and 'optimizer' across the whole document\n",
      "# to find the core logic of how DSPy programs are optimized.\n",
      "\n",
      "optimizer_context_sections = []\n",
      "\n",
      "# Sliding window or line-based search for keywords\n",
      "lines = document.split('\\n')\n",
      "for i, line in enumerate(lines):\n",
      "    if any(keyword in line.lower() for keyword in [\"optimizer\", \"teleprompter\", \"compile(\", \"bootstrap\"]):\n",
      "        # Grab surrounding lines for context\n",
      "        start = max(0, i - 10)\n",
      "        end = min(len(lines), i + 20)\n",
      "        optimizer_context_sections.append(\"\\n\".join(lines[start:end]))\n",
      "\n",
      "# Deduplicate overlapping sections\n",
      "combined_context = \"\\n---\\n\".join(list(dict.fromkeys(optimizer_context_sections)))\n",
      "\n",
      "prompt = f\"\"\"\n",
      "You are an expert on DSPy. Based ON ONLY the following documentation snippets, explain:\n",
      "1. What DSPy optimizers (formerly teleprompters) are.\n",
      "2. How they generally work (the 'compile' process).\n",
      "3. Specific details for the optimizers mentioned: BetterTogether, BootstrapFewShot, and BootstrapFewShotWithRandomSearch.\n",
      "\n",
      "If the documentation is sparse, focus on the 'compile' method and 'parameters' management.\n",
      "\n",
      "Documentation Snippets:\n",
      "{combined_context}\n",
      "\"\"\"\n",
      "\n",
      "print(llm_query(prompt))\n",
      "```\n",
      "2026/02/08 12:38:33 INFO dspy.predict.rlm: RLM iteration 5/20\n",
      "Reasoning: The previous exploration revealed that the document consists of a directory structure and multiple API reference files. While the directory structure lists many optimizers, only three (`BetterTogether`, `BootstrapFewShot`, and `BootstrapFewShotWithRandomSearch`) have specific documentation entries present in the text provided. I have also found general information about the `Module` class and how it manages parameters and predictors, which is crucial to how optimizers function in DSPy.\n",
      "\n",
      "I have enough information to construct:\n",
      "1.  **Summary**: Explaining the role of optimizers in DSPy, the significance of the `compile` method, and the mechanism of parameter management.\n",
      "2.  **Key Points**: Highlighting the specific optimizers documented and the general workflow of the optimization process.\n",
      "3.  **Coverage Percentage**: Since the document contains information on many modules but I am focusing on the \"Optimizers\" section (which appears as a sub-segment of the API reference), I will estimate the coverage of relevant content found.\n",
      "\n",
      "I will perform one final check to see if there are other optimizer names mentioned in the \"Modules\" section or the initial directory structure that have brief descriptions I missed, specifically looking for `COPRO`, `MIPRO`, or `LabeledFewShot`.\n",
      "\n",
      "Plan: \n",
      "1. Scan for any hidden descriptions of other optimizers.\n",
      "2. Formulate final response.\n",
      "Code:\n",
      "```python\n",
      "# Final check for any missed descriptions of other optimizers in the document text\n",
      "other_optimizers = [\"COPRO\", \"MIPRO\", \"LabeledFewShot\", \"Ensemble\", \"KNNFewShot\"]\n",
      "found_mentions = {}\n",
      "\n",
      "for opt in other_optimizers:\n",
      "    if opt.lower() in document.lower():\n",
      "        # Find context\n",
      "        idx = document.lower().find(opt.lower())\n",
      "        found_mentions[opt] = document[max(0, idx-100):idx+200]\n",
      "\n",
      "print(\"Mentions of other optimizers:\")\n",
      "for opt, context in found_mentions.items():\n",
      "    print(f\"--- {opt} ---\\n{context}\\n\")\n",
      "\n",
      "# Based on the output, I will proceed to SUBMIT.\n",
      "```\n",
      "2026/02/08 12:38:33 INFO dspy.predict.rlm: RLM iteration 6/20\n",
      "Reasoning: The exploration of the 30,000-character document reveals it to be a collection of API documentation files for DSPy. The directory structure lists a wide variety of optimizers (COPRO, MIPRO, KNNFewShot, etc.), but the actual text content only provides detailed API references for three: `BetterTogether`, `BootstrapFewShot`, and `BootstrapFewShotWithRandomSearch`. \n",
      "\n",
      "Key takeaways from the document regarding how optimizers work:\n",
      "1. **Core Purpose**: Optimizers (formerly called teleprompters) are algorithms that refine the \"parameters\" (prompts and weights) of a DSPy program/module.\n",
      "2. **The `compile` Method**: This is the universal interface for optimization. It takes a \"student\" module and returns an optimized version.\n",
      "3. **Parameter Management**: Optimizers rely on the `get_params` and `named_predictors` methods of DSPy Modules to identify which parts of the program are tunable.\n",
      "4. **Bootstrap Mechanism**: `BootstrapFewShot` generates few-shot examples by running the program through a \"teacher\" module and using successful outputs as training examples. `BootstrapFewShotWithRandomSearch` adds an exploration layer to find the best combination of these examples.\n",
      "5. **Coverage**: While the directory structure lists many optimizers, the substantive documentation coverage in this specific text focuses on the Bootstrapping family and the general Module-Optimizer interaction.\n",
      "\n",
      "I have sufficient information to synthesize the final summary, key points, and coverage percentage based on the document's content.\n",
      "Code:\n",
      "```python\n",
      "summary = \"\"\"DSPy optimizers (formerly known as teleprompters) are specialized algorithms designed to automate the refinement of DSPy programs. They work by tuning the parameters of a module—such as prompts and demonstration examples—to improve performance against a specific metric. \n",
      "\n",
      "The primary mechanism for optimization is the `compile` method, which takes a DSPy module (often referred to as a 'student') and returns a modified, optimized version of that module. Optimizers interact with DSPy modules through a standardized parameter management system, identifying tunable components via methods like `get_params`, `named_predictors`, and `named_parameters`. \n",
      "\n",
      "Specific optimizers documented include:\n",
      "- **BootstrapFewShot**: A foundational tool that generates few-shot examples for a program by utilizing a teacher module to 'bootstrap' successful demonstrations.\n",
      "- **BootstrapFewShotWithRandomSearch**: An extension that applies random search logic to find the most effective set of bootstrapped examples.\n",
      "- **BetterTogether**: An optimizer designed for joint optimization and model collaboration.\"\"\"\n",
      "\n",
      "key_points = [\n",
      "    \"Optimizers automate the process of tuning prompts and module parameters in DSPy programs.\",\n",
      "    \"The 'compile' method is the universal interface used to transform a standard module into an optimized one.\",\n",
      "    \"Optimizers identify tunable parts of a program by accessing a module's internal 'parameters' and 'predictors'.\",\n",
      "    \"BootstrapFewShot works by generating new few-shot training examples through a teacher-student execution flow.\",\n",
      "    \"BootstrapFewShotWithRandomSearch enhances basic bootstrapping by searching across multiple configurations to identify the highest-performing version.\",\n",
      "    \"The directory structure indicates a wide range of specialized optimizers (e.g., COPRO, KNNFewShot, Ensemble) though full details for these are outside the primary text content provided.\"\n",
      "]\n",
      "\n",
      "# The document is 30,000 chars and lists many things. \n",
      "# The focus is \"optimizers\". I've extracted info from the directory, \n",
      "# the Module API, and the 3 specific optimizer files.\n",
      "coverage_pct = 40 \n",
      "\n",
      "SUBMIT(summary, key_points, coverage_pct)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key points (6 items):\n",
      "  1. Optimizers automate the process of tuning prompts and module parameters in DSPy programs.\n",
      "  2. The 'compile' method is the universal interface used to transform a standard module into an optimized one.\n",
      "  3. Optimizers identify tunable parts of a program by accessing a module's internal 'parameters' and 'predictors'.\n",
      "  4. BootstrapFewShot works by generating new few-shot training examples through a teacher-student execution flow.\n",
      "  5. BootstrapFewShotWithRandomSearch enhances basic bootstrapping by searching across multiple configurations to identify the highest-performing version.\n",
      "\n",
      "Summary: DSPy optimizers (formerly known as teleprompters) are specialized algorithms designed to automate the refinement of DSPy programs. They work by tuning the parameters of a module—such as prompts and demonstration examples—to improve performance against a specific metric. \n",
      "\n",
      "The primary mechanism for o...\n",
      "Coverage: ~40%\n"
     ]
    }
   ],
   "source": [
    "# 9b. SummarizeLongDocument — chunk-based summarization with controllable focus\n",
    "\n",
    "with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interp:\n",
    "    rlm = dspy.RLM(\n",
    "        signature=SummarizeLongDocument,\n",
    "        interpreter=interp,\n",
    "        max_iterations=20,\n",
    "        max_llm_calls=30,\n",
    "        verbose=True,\n",
    "    )\n",
    "    result = rlm(\n",
    "        document=dspy_docs[:30_000],\n",
    "        focus=\"DSPy optimizers and how they work\",\n",
    "    )\n",
    "    print(f\"\\nKey points ({len(result.key_points)} items):\")\n",
    "    for i, kp in enumerate(result.key_points[:5]):\n",
    "        print(f\"  {i + 1}. {kp}\")\n",
    "    print(f\"\\nSummary: {result.summary[:300]}...\")\n",
    "    print(f\"Coverage: ~{result.coverage_pct}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074af4c",
   "metadata": {},
   "source": [
    "## 11. Persistent Storage with Modal Volumes V2\n",
    "\n",
    "Modal Volumes V2 provide persistent storage across sandbox sessions with better performance and consistency. This is useful for:\n",
    "- Caching large documents to avoid re-uploading\n",
    "- Storing intermediate results\n",
    "- Sharing data between multiple RLM runs\n",
    "- **Hosting knowledge files** (RLM paper, DSPy docs) directly in the sandbox filesystem\n",
    "\n",
    "### Volume Setup\n",
    "First, create a V2 volume (one-time setup):\n",
    "```bash\n",
    "modal volume create --version=2 rlm-volume-dspy\n",
    "```\n",
    "\n",
    "### Volumes V2 Key Features\n",
    "- Uses `modal.Volume.from_name(name, create_if_missing=True, version=2)`\n",
    "- Mounts via `volumes={\"/data\": volume}` in `Sandbox.create()`\n",
    "- **No file count limit** (V1 had 500K inode limit)\n",
    "- **Concurrent writes** from hundreds of containers\n",
    "- Commit via `sync /data` from inside the sandbox (V2 only)\n",
    "- Background commits persist data automatically on container shutdown\n",
    "\n",
    "### Uploading Files from Local Machine\n",
    "```python\n",
    "vol = modal.Volume.from_name(\"my-volume\", create_if_missing=True, version=2)\n",
    "with vol.batch_upload() as batch:\n",
    "    batch.put_directory(\"/local/dir\", \"/remote/dir\")\n",
    "    batch.put_file(\"local.txt\", \"/remote/file.txt\")\n",
    "```\n",
    "\n",
    "Or via `ModalInterpreter.upload_to_volume()`:\n",
    "```python\n",
    "interpreter.upload_to_volume(\n",
    "    local_dirs={\"rlm_content/dspy-knowledge\": \"/dspy-knowledge\"},\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d383c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume: '/rlm-knowledge' exists, skipping upload.\n",
      "Volume: '/dspy-knowledge' exists, skipping upload.\n",
      "✓ Uploaded knowledge directories to volume 'rlm-volume-dspy':\n",
      "  /Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/_WORLD/_RLM/fleet-rlm-dspy/rlm_content/rlm-knowledge → /data/rlm-knowledge/\n",
      "  /Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/_WORLD/_RLM/fleet-rlm-dspy/rlm_content/dspy-knowledge → /data/dspy-knowledge/\n",
      "\n",
      "Volume contents:\n",
      "  /dspy-knowledge\n",
      "    /dspy-knowledge/dspy-knowledge/dspy-RLM.md\n",
      "    /dspy-knowledge/dspy-knowledge/dspy-doc.txt\n",
      "    /dspy-knowledge/dspy-knowledge/mermaid.md\n",
      "  /rlm-knowledge\n",
      "    /rlm-knowledge/rlm-knowledge/rlm-pape.pdf\n",
      "    /rlm-knowledge/rlm-knowledge/rlm-paper.md\n",
      "  /notebook-demo.json\n",
      "  /dspy-doc-cached.txt\n"
     ]
    }
   ],
   "source": [
    "# Upload rlm-knowledge/ and dspy-knowledge/ to the Modal Volume\n",
    "# This makes files available at /data/rlm-knowledge/ and /data/dspy-knowledge/\n",
    "# inside every sandbox that mounts the volume.\n",
    "\n",
    "VOLUME_NAME = \"rlm-volume-dspy\"\n",
    "\n",
    "rlm_knowledge_dir = str(PROJECT_ROOT / \"rlm_content\" / \"rlm-knowledge\")\n",
    "dspy_knowledge_dir = str(PROJECT_ROOT / \"rlm_content\" / \"dspy-knowledge\")\n",
    "\n",
    "# Verify local directories exist\n",
    "for d in [rlm_knowledge_dir, dspy_knowledge_dir]:\n",
    "    assert os.path.isdir(d), f\"Directory not found: {d}\"\n",
    "\n",
    "# Use ModalInterpreter's upload helper\n",
    "upload_interpreter = ModalInterpreter(\n",
    "    image=SANDBOX_IMAGE,\n",
    "    app_name=MODAL_APP_NAME,\n",
    "    volume_name=VOLUME_NAME,\n",
    ")\n",
    "\n",
    "upload_interpreter.upload_to_volume(\n",
    "    local_dirs={\n",
    "        rlm_knowledge_dir: \"/rlm-knowledge\",\n",
    "        dspy_knowledge_dir: \"/dspy-knowledge\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"✓ Uploaded knowledge directories to volume '{VOLUME_NAME}':\")\n",
    "print(f\"  {rlm_knowledge_dir} → /data/rlm-knowledge/\")\n",
    "print(f\"  {dspy_knowledge_dir} → /data/dspy-knowledge/\")\n",
    "print()\n",
    "\n",
    "# List uploaded files for confirmation\n",
    "vol = modal.Volume.from_name(VOLUME_NAME, create_if_missing=True, version=2)\n",
    "print(\"Volume contents:\")\n",
    "for entry in vol.listdir(\"/\"):\n",
    "    print(f\"  /{entry.path}\")\n",
    "    if entry.type.name == \"DIRECTORY\":\n",
    "        for sub in vol.listdir(f\"/{entry.path}\"):\n",
    "            size_str = (\n",
    "                f\" ({sub.stat().size:,} bytes)\"\n",
    "                if hasattr(sub, \"stat\") and sub.stat()\n",
    "                else \"\"\n",
    "            )\n",
    "            print(f\"    /{entry.path}/{sub.path}{size_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d4596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created ModalInterpreter with Volumes V2 support\n",
      "  Volume: rlm-volume-dspy → mounted at /data/\n",
      "\n",
      "\n",
      "Sandbox result: FinalOutput({'result': 'volume knowledge files verified'})\n",
      "\n",
      "✓ Knowledge files accessible inside sandbox via volume mount.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate reading uploaded knowledge files from the volume inside a sandbox\n",
    "volume_interpreter = ModalInterpreter(\n",
    "    image=SANDBOX_IMAGE,\n",
    "    app_name=MODAL_APP_NAME,\n",
    "    volume_name=VOLUME_NAME,\n",
    "    timeout=600,\n",
    ")\n",
    "\n",
    "print(\"✓ Created ModalInterpreter with Volumes V2 support\")\n",
    "print(f\"  Volume: {VOLUME_NAME} → mounted at /data/\")\n",
    "print()\n",
    "\n",
    "# Start sandbox and verify knowledge files are accessible\n",
    "volume_interpreter.start()\n",
    "\n",
    "code = \"\"\"\n",
    "import pathlib, json\n",
    "\n",
    "data_dir = pathlib.Path(\"/data\")\n",
    "print(f\"Volume mounted: {data_dir.exists()}\")\n",
    "\n",
    "# List top-level contents\n",
    "top_level = sorted([p.name for p in data_dir.iterdir()])\n",
    "print(f\"Top-level dirs: {top_level}\")\n",
    "\n",
    "# List dspy-knowledge contents\n",
    "dspy_dir = data_dir / \"dspy-knowledge\"\n",
    "if dspy_dir.exists():\n",
    "    files = sorted(p.name for p in dspy_dir.iterdir())\n",
    "    print(f\"dspy-knowledge/: {files}\")\n",
    "    for f in dspy_dir.iterdir():\n",
    "        size = f.stat().st_size\n",
    "        print(f\"  {f.name}: {size:,} bytes\")\n",
    "\n",
    "# List rlm-knowledge contents\n",
    "rlm_dir = data_dir / \"rlm-knowledge\"\n",
    "if rlm_dir.exists():\n",
    "    files = sorted(p.name for p in rlm_dir.iterdir())\n",
    "    print(f\"rlm-knowledge/: {files}\")\n",
    "    for f in rlm_dir.iterdir():\n",
    "        size = f.stat().st_size\n",
    "        print(f\"  {f.name}: {size:,} bytes\")\n",
    "\n",
    "# Read a snippet of dspy-RLM.md to confirm content\n",
    "rlm_doc = data_dir / \"dspy-knowledge\" / \"dspy-RLM.md\"\n",
    "if rlm_doc.exists():\n",
    "    content = rlm_doc.read_text()\n",
    "    print(f\"\\\\ndspy-RLM.md preview ({len(content):,} chars):\")\n",
    "    print(content[:300])\n",
    "    print(\"...\")\n",
    "\n",
    "SUBMIT(result=\"volume knowledge files verified\")\n",
    "\"\"\"\n",
    "\n",
    "result = volume_interpreter.execute(code)\n",
    "print(f\"\\nSandbox result: {result}\")\n",
    "volume_interpreter.shutdown()\n",
    "print(\"\\n✓ Knowledge files accessible inside sandbox via volume mount.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d793f17",
   "metadata": {},
   "source": [
    "### RLM on Volume-Hosted Docs\n",
    "\n",
    "The key benefit of hosting knowledge files on a Volume: the RLM sandbox can read\n",
    "documents directly from `/data/` instead of serializing them through Python variables.\n",
    "This is the canonical RLM pattern — treat context as an **external environment**, not input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55ab5a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:39 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: I will start by reading the contents of the file `/data/dspy-knowledge/dspy-RLM.md` to understand the documentation structure and identify the relevant sections for constructor parameters, built-in tools, and key concepts.\n",
      "Code:\n",
      "```python\n",
      "import os\n",
      "\n",
      "file_path = \"/data/dspy-knowledge/dspy-RLM.md\"\n",
      "\n",
      "if os.path.exists(file_path):\n",
      "    with open(file_path, 'r') as f:\n",
      "        content = f.read()\n",
      "    print(f\"File length: {len(content)}\")\n",
      "    print(\"First 1000 characters:\")\n",
      "    print(content[:1000])\n",
      "else:\n",
      "    print(f\"File {file_path} not found.\")\n",
      "```\n",
      "2026/02/08 12:38:39 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The file is roughly 8KB, which is small enough to read entirely into memory. I'll read the full content and then use `llm_query` to extract the specific lists of constructor parameters, built-in tools, and key concepts as requested. I'll structure the prompt to ensure I get structured data for the lists.\n",
      "Code:\n",
      "```python\n",
      "file_path = \"/data/dspy-knowledge/dspy-RLM.md\"\n",
      "with open(file_path, 'r') as f:\n",
      "    full_content = f.read()\n",
      "\n",
      "# Using a single comprehensive prompt to extract all required information\n",
      "prompt = f\"\"\"\n",
      "Based on the following documentation for dspy.RLM, extract the following:\n",
      "1. Constructor parameters (list of strings or dicts describing them)\n",
      "2. Built-in tools (list of tool names or descriptions available in the RLM environment)\n",
      "3. Key concepts (a summary or list of how RLM works)\n",
      "\n",
      "Documentation:\n",
      "{full_content}\n",
      "\n",
      "Format your response as a JSON object with keys: \"parameters\", \"builtin_tools\", \"key_concepts\".\n",
      "\"\"\"\n",
      "\n",
      "result = llm_query(prompt)\n",
      "print(result)\n",
      "```\n",
      "2026/02/08 12:38:40 INFO dspy.predict.rlm: RLM iteration 3/15\n",
      "Reasoning: I have successfully extracted the constructor parameters, built-in tools, and key concepts from the provided documentation using the `llm_query` function. The output from the previous step is already in a structured format. I will now parse this JSON and call the `SUBMIT` function with the extracted information.\n",
      "Code:\n",
      "```python\n",
      "import json\n",
      "\n",
      "# The output from the previous iteration's llm_query\n",
      "llm_output = \"\"\"\n",
      "{\n",
      "  \"parameters\": [\n",
      "    \"signature: Defines inputs and outputs (e.g., 'context, query -> answer')\",\n",
      "    \"max_iterations: Maximum REPL interaction loops before fallback extraction\",\n",
      "    \"max_llm_calls: Maximum llm_query/llm_query_batched calls per execution\",\n",
      "    \"max_output_chars: Maximum characters to include from REPL output\",\n",
      "    \"verbose: Log detailed execution info\",\n",
      "    \"tools: Additional tool functions callable from interpreter code\",\n",
      "    \"sub_lm: LM for sub-queries (defaults to dspy.settings.lm)\",\n",
      "    \"interpreter: Custom interpreter (defaults to PythonInterpreter via Deno/Pyodide WASM)\"\n",
      "  ],\n",
      "  \"builtin_tools\": [\n",
      "    \"llm_query(prompt): Query a sub-LLM for semantic analysis\",\n",
      "    \"llm_query_batched(prompts): Query multiple prompts concurrently\",\n",
      "    \"print(): Required to see results in the REPL output\",\n",
      "    \"SUBMIT(...): Submit final output and end execution\",\n",
      "    \"Standard library: re, json, collections, math, etc.\"\n",
      "  ],\n",
      "  \"key_concepts\": [\n",
      "    \"Context Separation: Solves 'context rot' by separating the variable space (stored in the REPL) from the token space (processed by the LLM).\",\n",
      "    \"Iterative REPL Loop: The LLM receives metadata about a large context and writes Python code to explore, filter, or aggregate it programmatically.\",\n",
      "    \"Sandboxed Execution: Code runs in a secure Deno/Pyodide WASM environment, returning only the results of execution to the LLM.\",\n",
      "    \"Recursive Analysis: Allows the main LLM to delegate semantic analysis of context snippets to smaller, cheaper sub-LLM calls via llm_query.\",\n",
      "    \"Dynamic Decomposition: The LLM decides how to navigate and chunk the data itself rather than relying on predefined extraction logic.\"\n",
      "  ]\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "data = json.loads(llm_output)\n",
      "\n",
      "SUBMIT(\n",
      "    parameters=data[\"parameters\"],\n",
      "    builtin_tools=data[\"builtin_tools\"],\n",
      "    key_concepts=data[\"key_concepts\"]\n",
      ")\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters: [\"signature: Defines inputs and outputs (e.g., 'context, query -> answer')\", 'max_iterations: Maximum REPL interaction loops before fallback extraction', 'max_llm_calls: Maximum llm_query/llm_query_batched calls per execution', 'max_output_chars: Maximum characters to include from REPL output', 'verbose: Log detailed execution info', 'tools: Additional tool functions callable from interpreter code', 'sub_lm: LM for sub-queries (defaults to dspy.settings.lm)', 'interpreter: Custom interpreter (defaults to PythonInterpreter via Deno/Pyodide WASM)']\n",
      "Built-in tools: ['llm_query(prompt): Query a sub-LLM for semantic analysis', 'llm_query_batched(prompts): Query multiple prompts concurrently', 'print(): Required to see results in the REPL output', 'SUBMIT(...): Submit final output and end execution', 'Standard library: re, json, collections, math, etc.']\n",
      "Key concepts: [\"Context Separation: Solves 'context rot' by separating the variable space (stored in the REPL) from the token space (processed by the LLM).\", 'Iterative REPL Loop: The LLM receives metadata about a large context and writes Python code to explore, filter, or aggregate it programmatically.', 'Sandbo...\n"
     ]
    }
   ],
   "source": [
    "# RLM task that reads docs directly from the Volume filesystem\n",
    "# instead of passing them as a Python variable.\n",
    "# The sandbox reads /data/dspy-knowledge/dspy-RLM.md and extracts info.\n",
    "\n",
    "\n",
    "class ExtractRLMCapabilities(dspy.Signature):\n",
    "    \"\"\"Extract RLM capabilities from documentation stored on a Volume.\n",
    "\n",
    "    Strategy:\n",
    "    1. Read /data/dspy-knowledge/dspy-RLM.md from the volume\n",
    "    2. Search for constructor parameters, built-in tools, and usage patterns\n",
    "    3. Use llm_query() on relevant sections for semantic extraction\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = dspy.InputField(desc=\"What to extract from the RLM docs\")\n",
    "    parameters: list = dspy.OutputField(desc=\"List of RLM constructor parameters\")\n",
    "    builtin_tools: list = dspy.OutputField(desc=\"List of built-in sandbox tools\")\n",
    "    key_concepts: str = dspy.OutputField(desc=\"Summary of key RLM concepts\")\n",
    "\n",
    "\n",
    "with ModalInterpreter(\n",
    "    image=SANDBOX_IMAGE,\n",
    "    app_name=MODAL_APP_NAME,\n",
    "    volume_name=VOLUME_NAME,\n",
    "    timeout=600,\n",
    ") as vol_interp:\n",
    "    rlm = dspy.RLM(\n",
    "        signature=ExtractRLMCapabilities,\n",
    "        interpreter=vol_interp,\n",
    "        max_iterations=15,\n",
    "        max_llm_calls=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    # NOTE: We do NOT pass docs as input — the RLM reads them from /data/ in the sandbox!\n",
    "    result = rlm(\n",
    "        query=\"Read /data/dspy-knowledge/dspy-RLM.md and extract all constructor parameters, built-in tools, and key concepts about how RLM works.\"\n",
    "    )\n",
    "    print(f\"\\nParameters: {result.parameters}\")\n",
    "    print(f\"Built-in tools: {result.builtin_tools}\")\n",
    "    print(f\"Key concepts: {result.key_concepts[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "596871a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLI Commands — fleet-rlm Package\n",
      "============================================================\n",
      "\n",
      "The fleet-rlm package provides a Typer CLI with the following commands:\n",
      "\n",
      "1. Basic Code Generation:\n",
      "   $ uv run fleet-rlm run-basic \\\n",
      "       --question \"What are the first 10 Fibonacci numbers?\" \\\n",
      "       --volume-name rlm-volume-dspy\n",
      "\n",
      "2. Architecture Extraction:\n",
      "   $ uv run fleet-rlm run-architecture \\\n",
      "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\n",
      "       --query \"Extract all modules and optimizers\" \\\n",
      "       --volume-name rlm-volume-dspy\n",
      "\n",
      "3. API Endpoint Extraction:\n",
      "   $ uv run fleet-rlm run-api-endpoints \\\n",
      "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\n",
      "       --volume-name rlm-volume-dspy\n",
      "\n",
      "4. Error Pattern Analysis:\n",
      "   $ uv run fleet-rlm run-error-patterns \\\n",
      "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\n",
      "       --volume-name rlm-volume-dspy\n",
      "\n",
      "5. Execution Trajectory:\n",
      "   $ uv run fleet-rlm run-trajectory \\\n",
      "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt\n",
      "\n",
      "6. Custom Tool Demo:\n",
      "   $ uv run fleet-rlm run-custom-tool \\\n",
      "       --text \"Extract emails from [email protected] and [email protected]\"\n",
      "\n",
      "7. Long-Context Analysis (NEW):\n",
      "   $ uv run fleet-rlm run-long-context \\\n",
      "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\n",
      "       --query \"What are the main design decisions?\" \\\n",
      "       --mode analyze\n",
      "\n",
      "8. Long-Context Summarization (NEW):\n",
      "   $ uv run fleet-rlm run-long-context \\\n",
      "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\n",
      "       --query \"DSPy optimizers\" \\\n",
      "       --mode summarize\n",
      "\n",
      "9. Check Modal Secrets:\n",
      "   $ uv run fleet-rlm check-secret\n",
      "   $ uv run fleet-rlm check-secret-key --key DSPY_LLM_API_KEY\n",
      "\n",
      "Volume Setup (One-time):\n",
      "   $ uv run modal volume create rlm-volume-dspy\n",
      "\n",
      "============================================================\n",
      "All run-* Commands Support --volume-name\n",
      "============================================================\n",
      "Use --volume-name to enable persistent storage for:\n",
      "  • Document caching\n",
      "  • Intermediate results\n",
      "  • Data sharing between runs\n",
      "\n",
      "Data persists at /data/ inside the Modal sandbox\n",
      "using Modal Volumes V2.\n"
     ]
    }
   ],
   "source": [
    "# CLI Usage Examples\n",
    "print(\"=\" * 60)\n",
    "print(\"CLI Commands — fleet-rlm Package\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "The fleet-rlm package provides a Typer CLI with the following commands:\n",
    "\n",
    "1. Basic Code Generation:\n",
    "   $ uv run fleet-rlm run-basic \\\\\n",
    "       --question \"What are the first 10 Fibonacci numbers?\" \\\\\n",
    "       --volume-name rlm-volume-dspy\n",
    "\n",
    "2. Architecture Extraction:\n",
    "   $ uv run fleet-rlm run-architecture \\\\\n",
    "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\\\n",
    "       --query \"Extract all modules and optimizers\" \\\\\n",
    "       --volume-name rlm-volume-dspy\n",
    "\n",
    "3. API Endpoint Extraction:\n",
    "   $ uv run fleet-rlm run-api-endpoints \\\\\n",
    "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\\\n",
    "       --volume-name rlm-volume-dspy\n",
    "\n",
    "4. Error Pattern Analysis:\n",
    "   $ uv run fleet-rlm run-error-patterns \\\\\n",
    "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\\\n",
    "       --volume-name rlm-volume-dspy\n",
    "\n",
    "5. Execution Trajectory:\n",
    "   $ uv run fleet-rlm run-trajectory \\\\\n",
    "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt\n",
    "\n",
    "6. Custom Tool Demo:\n",
    "   $ uv run fleet-rlm run-custom-tool \\\\\n",
    "       --text \"Extract emails from [email protected] and [email protected]\"\n",
    "\n",
    "7. Long-Context Analysis (NEW):\n",
    "   $ uv run fleet-rlm run-long-context \\\\\n",
    "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\\\n",
    "       --query \"What are the main design decisions?\" \\\\\n",
    "       --mode analyze\n",
    "\n",
    "8. Long-Context Summarization (NEW):\n",
    "   $ uv run fleet-rlm run-long-context \\\\\n",
    "       --docs-path rlm_content/dspy-knowledge/dspy-doc.txt \\\\\n",
    "       --query \"DSPy optimizers\" \\\\\n",
    "       --mode summarize\n",
    "\n",
    "9. Check Modal Secrets:\n",
    "   $ uv run fleet-rlm check-secret\n",
    "   $ uv run fleet-rlm check-secret-key --key DSPY_LLM_API_KEY\n",
    "\n",
    "Volume Setup (One-time):\n",
    "   $ uv run modal volume create rlm-volume-dspy\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"All run-* Commands Support --volume-name\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use --volume-name to enable persistent storage for:\")\n",
    "print(\"  • Document caching\")\n",
    "print(\"  • Intermediate results\")\n",
    "print(\"  • Data sharing between runs\")\n",
    "print()\n",
    "print(\"Data persists at /data/ inside the Modal sandbox\")\n",
    "print(\"using Modal Volumes V2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1bb07b",
   "metadata": {},
   "source": [
    "## 12. Custom Tools\n",
    "\n",
    "RLM supports custom Python tools that the generated code can call. The `regex_extract()` tool and `ExtractWithCustomTool` signature are imported from the `fleet_rlm` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "custom-tools",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex_extract tool:\n",
      "  Module: fleet_rlm.tools\n",
      "  Doc: Extract all regex matches from text using a compiled pattern.\n",
      "\n",
      "  Local test: regex_extract('...', email_pattern) = []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 12:38:42 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: I will start by exploring the content of the `docs` variable to understand its structure. I'll print the first 2000 characters and check the total length to confirm what I'm dealing with. Then I will define regex patterns to extract markdown headers (e.g., `# Header`) and code blocks (e.g., ```python ... ```).\n",
      "Code:\n",
      "```python\n",
      "print(f\"Total length: {len(docs)}\")\n",
      "print(\"--- Sample (first 1000 chars) ---\")\n",
      "print(docs[:1000])\n",
      "print(\"--- Sample (last 500 chars) ---\")\n",
      "print(docs[-500:])\n",
      "```\n",
      "2026/02/08 12:38:42 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The document appears to be a directory structure combined with API documentation details. I need to extract headers and code blocks.\n",
      "1. **Headers**: Markdown headers typically start with one or more `#` characters at the beginning of a line. I will use `regex_extract` with a multiline approach.\n",
      "2. **Code Blocks**: Code blocks are delimited by triple backticks (```). I'll extract the content and any language identifiers.\n",
      "3. **Exploration**: I'll also look for headers in the latter half of the document to see how they are formatted, as the first part was just a file tree.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Extract all markdown headers (e.g., # Header, ## Subheader)\n",
      "# Pattern: ^#+\\s+.+ (using re.MULTILINE)\n",
      "headers = regex_extract(docs, r\"^#+\\s+.+\", re.MULTILINE)\n",
      "\n",
      "# Extract all code blocks including language identifiers\n",
      "# Pattern: ```[\\w]*\\n[\\s\\S]*?```\n",
      "code_blocks = regex_extract(docs, r\"```[\\w]*\\n[\\s\\S]*?```\")\n",
      "\n",
      "print(f\"Found {len(headers)} headers.\")\n",
      "print(f\"Found {len(code_blocks)} code blocks.\")\n",
      "\n",
      "if headers:\n",
      "    print(\"Sample headers:\", headers[:5])\n",
      "if code_blocks:\n",
      "    print(\"Sample code block (first 100 chars):\", code_blocks[0][:100])\n",
      "\n",
      "# Since the document might be long, let's also check if there are sections that look like 'module' or 'class' headers\n",
      "# which might not use markdown but list-style or indentation. \n",
      "# However, the prompt specifically asks for \"markdown headers and code blocks\".\n",
      "```\n",
      "2026/02/08 12:38:42 INFO dspy.predict.rlm: RLM iteration 3/15\n",
      "Reasoning: The previous attempt found very few headers and zero code blocks. This is surprising given the document length (10,000 chars) and the preview which showed file paths and YAML-like structures. The pattern `r\"```[\\w]*\\n[\\s\\S]*?```\"` might have failed if the code blocks don't have a newline immediately after the backticks or if the backticks are used differently.\n",
      "\n",
      "Also, I need to check for different header styles. Some documentation uses `---` or `===` underlines, though standard Markdown headers use `#`. I will refine the regex to be more flexible and scan the text more closely for why code blocks weren't detected. I'll also look for indentation-based code blocks or blocks that start without a language identifier.\n",
      "\n",
      "I'll use `llm_query` to get a high-level summary of the document's organization to better inform the \"structure_summary\" field later.\n",
      "\n",
      "Plan:\n",
      "1. Re-run regex for headers with more flexibility.\n",
      "2. Search for code blocks using a simpler pattern `r\"```[\\s\\S]*?```\"`.\n",
      "3. Use `llm_query` to understand the overall organization.\n",
      "4. Print more snippets of the document to see why code blocks weren't caught.\n",
      "Code:\n",
      "```python\n",
      "# Try a broader search for code blocks\n",
      "code_blocks_broad = regex_extract(docs, r\"```[\\s\\S]*?```\")\n",
      "\n",
      "# Try to find headers that might not be at the start of a line if the string isn't split correctly\n",
      "headers_broad = regex_extract(docs, r\"(?m)^#+\\s.*\")\n",
      "\n",
      "print(f\"Broad headers count: {len(headers_broad)}\")\n",
      "print(f\"Broad code blocks count: {len(code_blocks_broad)}\")\n",
      "\n",
      "# Look for patterns of code if triple backticks are missing (e.g., indented blocks)\n",
      "# but the prompt specifically asks for \"markdown headers and code blocks\".\n",
      "# Let's inspect a middle section of the docs to see formatting.\n",
      "print(\"--- Middle Sample (chars 4000-6000) ---\")\n",
      "print(docs[4000:6000])\n",
      "\n",
      "# Get semantic summary\n",
      "summary_prompt = \"Summarize the organization and structure of this documentation. What are the main sections and how is it laid out? \" + docs[:4000]\n",
      "structure_info = llm_query(summary_prompt)\n",
      "print(\"--- LLM Structure Info ---\")\n",
      "print(structure_info)\n",
      "```\n",
      "2026/02/08 12:38:43 INFO dspy.predict.rlm: RLM iteration 4/15\n",
      "Reasoning: The exploration reveals that `docs` is a concatenated string containing a large directory tree followed by specific file contents separated by headers like `================================================\\nFILE: filename\\n================================================`. \n",
      "\n",
      "The initial regex for headers only found 3 because most of the document is a directory structure or YAML/Python code without standard Markdown headers. I need to:\n",
      "1.  **Extract all Markdown headers**: I will use a more robust regex that scans the entire document.\n",
      "2.  **Extract code blocks**: My previous regex `r\"```[\\w]*\\n[\\s\\S]*?```\"` found 0 matches. Looking at the \"Middle Sample\", I see file contents like YAML. I need to verify if these are wrapped in triple backticks or if the \"FILE: ...\" markers are the boundaries. However, the task specifically asks for \"markdown headers and code blocks\". I will try a simpler pattern for code blocks to see if they exist at all in the latter part of the document.\n",
      "3.  **Refine Structure Summary**: Use the LLM output and my observations to summarize the organization.\n",
      "\n",
      "Wait, looking at the \"Middle Sample\" output, I see `FILE: .pre-commit-config.yaml` followed by YAML content. It doesn't use backticks there. I will check the very end of the document again to see if actual Markdown files (like `Adapter.md`) are included with their content.\n",
      "Code:\n",
      "```python\n",
      "# Check for markdown headers again across the whole doc\n",
      "headers = regex_extract(docs, r\"^#+\\s+.+\", re.MULTILINE)\n",
      "\n",
      "# Check for code blocks again with a very loose pattern\n",
      "code_blocks = regex_extract(docs, r\"```[\\s\\S]*?```\")\n",
      "\n",
      "# Let's look at the content of one of the .md files if it exists in the string\n",
      "# Searching for the content of 'Adapter.md' or similar\n",
      "markdown_content_sample = regex_extract(docs, r\"FILE: .*?\\.md\\n=+\\n([\\s\\S]{1,500})\")\n",
      "\n",
      "print(f\"Headers found: {len(headers)}\")\n",
      "print(f\"Code blocks found: {len(code_blocks)}\")\n",
      "print(f\"Markdown file content sample: {markdown_content_sample[:1]}\")\n",
      "\n",
      "# If no backtick code blocks are found, I'll check if the \"FILE\" sections \n",
      "# are intended to be treated as the code/structure.\n",
      "# But I must follow the tool requirement: \"markdown headers and code blocks\".\n",
      "\n",
      "# Let's try to find if there are any code blocks at the very end.\n",
      "print(\"--- End of docs ---\")\n",
      "print(docs[-1000:])\n",
      "```\n",
      "2026/02/08 12:38:43 INFO dspy.predict.rlm: RLM iteration 5/15\n",
      "Reasoning: The exploration of `docs` shows that the content is a concatenation of many files from a repository. It starts with a large directory structure and then lists individual file contents separated by `FILE: <path>` headers. \n",
      "\n",
      "Regarding the specific requirements:\n",
      "1. **Markdown Headers**: I have found 3 headers so far (`# API Reference`, `# dspy.Adapter`, `# dspy.ChatAdapter`). The reason there are so few is that most of the file content shown in the samples consists of `mkdocstrings` identifiers (e.g., `::: dspy.ChatAdapter`) rather than standard Markdown headers.\n",
      "2. **Code Blocks**: My previous regex `r\"```[\\s\\S]*?```\"` returned 0 results. Looking at the samples, the \"code\" is actually raw YAML or Python configuration blocks that are *not* wrapped in triple backticks within the `docs` string. However, I will do one more comprehensive search for any backtick blocks that might be hidden in other parts of the 10,000-character string.\n",
      "3. **Structure Summary**: The document is organized as a repository dump:\n",
      "   - A visual directory tree of the `stanfordnlp-dspy` project.\n",
      "   - Individual file dumps for configuration files (`.pre-commit-config.yaml`) and documentation source files (`.md`).\n",
      "   - The documentation files use `mkdocstrings` syntax to generate API references for various adapters and modules.\n",
      "\n",
      "I will now perform a final extraction to ensure I haven't missed any headers or code blocks in the middle sections, and then compile the results.\n",
      "Code:\n",
      "```python\n",
      "# Final comprehensive extraction\n",
      "all_headers = regex_extract(docs, r\"^#+\\s+.+\", re.MULTILINE)\n",
      "\n",
      "# Look for any backtick code blocks again, being very careful\n",
      "all_code_blocks = regex_extract(docs, r\"```[\\s\\S]*?```\")\n",
      "\n",
      "# Summary of the document organization\n",
      "summary_prompt = (\n",
      "    \"Based on the text provided, describe the organization and structure of the document. \"\n",
      "    \"Mention the directory tree and the individual file sections.\"\n",
      ")\n",
      "structure_summary = llm_query(summary_prompt + \"\\n\\nContent snippet: \" + docs[:2000] + \"...\" + docs[-2000:])\n",
      "\n",
      "print(f\"Headers: {all_headers}\")\n",
      "print(f\"Code blocks: {all_code_blocks}\")\n",
      "print(f\"Summary: {structure_summary}\")\n",
      "\n",
      "SUBMIT(all_headers, all_code_blocks, structure_summary)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 headers\n",
      "Found 0 code blocks\n",
      "\n",
      "Structure: Based on the provided text, the document is organized as a technical reference guide for the `stanfordnlp-dspy` repository, specifically focusing on its API documentation. The structure is broken down...\n"
     ]
    }
   ],
   "source": [
    "# regex_extract and ExtractWithCustomTool were imported from the package above.\n",
    "# Let's see how they work:\n",
    "print(\"regex_extract tool:\")\n",
    "print(f\"  Module: {regex_extract.__module__}\")\n",
    "print(f\"  Doc: {regex_extract.__doc__.strip().split(chr(10))[0]}\")\n",
    "print()\n",
    "\n",
    "# Quick local test of the tool\n",
    "\n",
    "test_matches = regex_extract(\n",
    "    \"Hello [email protected] and [email protected]\", r\"[\\w.+-]+@[\\w-]+\\.[\\w.]+\"\n",
    ")\n",
    "print(f\"  Local test: regex_extract('...', email_pattern) = {test_matches}\")\n",
    "print()\n",
    "\n",
    "# Now use it in an RLM task\n",
    "with ModalInterpreter(image=SANDBOX_IMAGE, app_name=MODAL_APP_NAME) as interpreter:\n",
    "    rlm = dspy.RLM(\n",
    "        signature=ExtractWithCustomTool,\n",
    "        interpreter=interpreter,\n",
    "        tools=[regex_extract],  # Pass custom tool here\n",
    "        max_iterations=15,\n",
    "        max_llm_calls=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    result = rlm(docs=dspy_docs[:10000])  # First 10KB for demo\n",
    "    print(f\"\\nFound {len(result.headers)} headers\")\n",
    "    print(f\"Found {len(result.code_blocks)} code blocks\")\n",
    "    print(f\"\\nStructure: {result.structure_summary[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-comparison",
   "metadata": {},
   "source": [
    "## 13. RLM vs Direct LLM Comparison\n",
    "\n",
    "| Aspect | Direct LLM | RLM |\n",
    "|--------|-----------|-----|\n",
    "| **Context size** | ~128K tokens | Virtually unlimited |\n",
    "| **Attention** | Dilutes over long context | Focused (code selects snippets) |\n",
    "| **Cost** | High (all tokens in context) | Lower (targeted sub-LLM calls) |\n",
    "| **Accuracy** | Lower on long docs | Higher (targeted analysis) |\n",
    "| **Verifiability** | Black box | Transparent (full trajectory) |\n",
    "| **Tool use** | Limited | Full Python + custom tools |\n",
    "| **Iterative refinement** | Manual (chat) | Automated (code loops) |\n",
    "| **Structured output** | Prompt-dependent | Type-enforced via Signature |\n",
    "\n",
    "### When to use RLM:\n",
    "- Documents > 50KB\n",
    "- Need structured extraction (lists, dicts, nested data)\n",
    "- Multi-step analysis (filter → extract → validate)\n",
    "- Need programmatic validation or computation\n",
    "- Repetitive analysis across many documents\n",
    "\n",
    "### When NOT to use RLM:\n",
    "- Simple Q&A on short text (< 1K tokens)\n",
    "- Creative writing or brainstorming\n",
    "- Tasks that don't benefit from code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-best-practices",
   "metadata": {},
   "source": [
    "## 14. RLM Best Practices\n",
    "\n",
    "### Signature Design\n",
    "\n",
    "1. **Describe the strategy** in the docstring:\n",
    "   ```python\n",
    "   class MySignature(dspy.Signature):\n",
    "       \"\"\"Extract X from Y.\n",
    "       \n",
    "       Strategy:\n",
    "       1. Use peek()/grep() to locate relevant sections\n",
    "       2. Use chunk_by_headers() to split the document\n",
    "       3. Use llm_query() on matching sections\n",
    "       4. Aggregate results\n",
    "       \"\"\"\n",
    "   ```\n",
    "\n",
    "2. **Use typed output fields** — `list`, `dict`, `int` guide the code:\n",
    "   ```python\n",
    "   items: list = dspy.OutputField(desc=\"List of found items\")\n",
    "   count: int = dspy.OutputField(desc=\"Total count\")\n",
    "   ```\n",
    "\n",
    "3. **Import from the package** rather than redefining inline:\n",
    "   ```python\n",
    "   from fleet_rlm.signatures import AnalyzeLongDocument, SummarizeLongDocument\n",
    "   ```\n",
    "\n",
    "### Sandbox-Side Helpers\n",
    "\n",
    "The driver injects these helpers automatically — the LLM code can call them:\n",
    "- `peek(text, start, length)` — inspect a slice without loading the whole string\n",
    "- `grep(text, pattern, context=0)` — case-insensitive line search\n",
    "- `chunk_by_size(text, size, overlap)` / `chunk_by_headers(text, pattern)` — split text\n",
    "- `add_buffer(name, value)` / `get_buffer(name)` / `clear_buffer(name)` — accumulate across iterations\n",
    "- `save_to_volume(path, content)` / `load_from_volume(path)` — persist data to `/data/`\n",
    "\n",
    "### Tuning Parameters\n",
    "\n",
    "| Parameter | Typical Range | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| `max_iterations` | 10-50 | Complex docs need more iterations |\n",
    "| `max_llm_calls` | 20-100 | Primary cost control |\n",
    "| `max_output_chars` | 10K-100K | Prevents output flooding |\n",
    "\n",
    "### Context Manager Pattern\n",
    "\n",
    "Always prefer the context manager for automatic cleanup:\n",
    "```python\n",
    "with ModalInterpreter(image=img, app_name=name) as interp:\n",
    "    rlm = dspy.RLM(signature=MySig, interpreter=interp, ...)\n",
    "    result = rlm(...)\n",
    "```\n",
    "\n",
    "### Debugging Workflow\n",
    "\n",
    "1. **Start with `verbose=True`**: See real-time reasoning and code\n",
    "2. **Inspect `result.trajectory`**: Full execution history\n",
    "3. **Test on subsets**: Use `docs[:5000]` before full runs\n",
    "4. **Check sandbox logs**: Modal shows actual execution\n",
    "5. **Validate tools**: Test custom tools independently\n",
    "6. **Use host-side chunking** (`fleet_rlm.chunking`) to pre-process before passing to RLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-conclusion",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "This notebook demonstrated the full capabilities of **dspy.RLM** with the `fleet-rlm` package:\n",
    "\n",
    "1. **Basic code generation** — LLM writes and executes Python\n",
    "2. **Long document analysis** — Process 80KB+ documents efficiently\n",
    "3. **Parallel processing** — `llm_query_batched()` for speed\n",
    "4. **Stateful reasoning** — Multi-step workflows with persistent variables\n",
    "5. **Trajectory inspection** — Full transparency into reasoning\n",
    "6. **Sandbox-side helpers** — `peek()`, `grep()`, `chunk_by_*()`, buffers, volume I/O\n",
    "7. **Host-side chunking** — `chunk_by_size`, `chunk_by_headers`, `chunk_by_timestamps`, `chunk_by_json_keys`\n",
    "8. **Long-context signatures** — `AnalyzeLongDocument`, `SummarizeLongDocument`\n",
    "9. **Persistent storage** — Modal Volumes V2 for caching and persistence\n",
    "10. **Custom tools** — Extend sandbox capabilities with user-defined functions\n",
    "11. **Context manager** — `with ModalInterpreter(...) as interp:` for safe cleanup\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- RLM treats long context as an **environment**, not input\n",
    "- Code navigates data; `llm_query()` understands semantics\n",
    "- **Sandbox helpers** (`peek`, `grep`, `chunk_*`, buffers) enable the LLM to explore data programmatically\n",
    "- The **trajectory** provides unprecedented observability\n",
    "- **Modal Volumes V2** enable persistent storage across sandbox sessions\n",
    "- **Import signatures, tools, chunking** from the `fleet_rlm` package — avoid inline redefinition\n",
    "- All capabilities are available via both notebook and CLI (`fleet-rlm`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fleet-rlm-dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
