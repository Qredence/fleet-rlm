{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f71b95b",
   "metadata": {},
   "source": [
    "# RLM with Modal Sandbox (DSPy 3.1.3)\n",
    "\n",
    "This tutorial shows how to use **`dspy.RLM`** (Recursive Language Model) with [Modal](https://modal.com) for secure, sandboxed code execution in the cloud.\n",
    "\n",
    "**What is RLM?** RLM is an inference strategy where the LLM writes Python code to programmatically explore data, call sub-LLMs over snippets, and iteratively build up answers — instead of feeding long contexts directly into the model.\n",
    "\n",
    "**Why Modal?** By default, `dspy.RLM` uses a local Deno/Pyodide WASM sandbox. Modal lets you run that code in an isolated cloud container with configurable resources, dependencies, and secrets.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Implement a `ModalInterpreter` that satisfies DSPy's `CodeInterpreter` protocol\n",
    "2. Use `modal.Sandbox` to execute code inside an ephemeral cloud container\n",
    "3. Run an RLM agent that writes and executes code remotely\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python 3.10+**\n",
    "- **Modal account**: Sign up at [modal.com](https://modal.com) and run `modal setup`\n",
    "- **Modal secret**: Create a secret named `LITELLM` that contains the environment variables used by DSPy/LiteLLM:\n",
    "  - `DSPY_LM_MODEL` (e.g., `openai/gemini-3-flash-preview`)\n",
    "  - `DSPY_LM_API_BASE` (your LiteLLM proxy base URL)\n",
    "  - `DSPY_LLM_API_KEY` (API key for the proxy/provider)\n",
    "  - optional: `DSPY_LM_MAX_TOKENS`\n",
    "\n",
    "  Example (run in a terminal):\n",
    "  ```bash\n",
    "  modal secret create LITELLM \\\n",
    "    DSPY_LM_MODEL=... \\\n",
    "    DSPY_LM_API_BASE=... \\\n",
    "    DSPY_LLM_API_KEY=... \\\n",
    "    DSPY_LM_MAX_TOKENS=...\n",
    "  ```\n",
    "\n",
    "- **Security note**: don’t hard-code API keys in notebooks, and don’t print them. If a key was ever pasted into a notebook/chat, rotate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc01761",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a4649fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Samsung-SSD-T7/Workspaces/Github/qredence/agent-framework/v0.5/_XCODE/_dspy/dspy/.venv/bin/python: No module named uv\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install -qU \"dspy==3.1.3\" modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6451ed",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "We configure one LM locally for the *planner* (the model that writes Python code each iteration).\n",
    "\n",
    "This notebook expects the following environment variables to be set **locally** (for the planner):\n",
    "- `DSPY_LM_MODEL`\n",
    "- `DSPY_LM_API_BASE`\n",
    "- `DSPY_LLM_API_KEY`\n",
    "- optional: `DSPY_LM_MAX_TOKENS`\n",
    "\n",
    "The same variables are also injected into the Modal sandbox via the `LITELLM` secret, so any sandbox-side LM calls (via tool-bridged `llm_query`) use identical credentials without hard-coding secrets in the notebook.\n",
    "\n",
    "**Important**: Modal secrets are only available *inside* Modal containers/sandboxes. They do **not** automatically set environment variables for your local notebook kernel.\n",
    "This notebook will try to load a local `.env` from the project root (if present) to configure the planner LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38fe5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planner LM configured: openai/gemini-3-flash-preview\n",
      "(Tip: don’t print API keys.)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Iterator\n",
    "\n",
    "import dspy\n",
    "\n",
    "# ---- Load local .env (for the planner LM) ----\n",
    "# Modal secrets are only available *inside* Modal; they do not configure your local kernel.\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "def _load_dotenv(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        return\n",
    "    try:\n",
    "        for raw in path.read_text().splitlines():\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "                continue\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            k, v = k.strip(), v.strip()\n",
    "            if len(v) >= 2 and ((v[0] == v[-1] == '\\\"') or (v[0] == v[-1] == \"'\")):\n",
    "                v = v[1:-1]\n",
    "            if k and k not in os.environ:\n",
    "                os.environ[k] = v\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not load {path}: {e}\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root(Path.cwd())\n",
    "_load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# ---- Guard against module shadowing ----\n",
    "# A local `modal.py` (or even a stale compiled `__pycache__/modal.*.pyc`) in the\n",
    "# notebook's working directory can shadow the third-party `modal` package.\n",
    "shadow_py = Path.cwd() / \"modal.py\"\n",
    "shadow_pyc_dir = Path.cwd() / \"__pycache__\"\n",
    "shadow_pycs = list(shadow_pyc_dir.glob(\"modal.*.pyc\")) if shadow_pyc_dir.exists() else []\n",
    "\n",
    "if shadow_py.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Found {shadow_py} which shadows the 'modal' package. \"\n",
    "        \"Rename/delete it (e.g., modal_get_started.py) and restart the kernel.\"\n",
    "    )\n",
    "\n",
    "if shadow_pycs:\n",
    "    removed: list[str] = []\n",
    "    failed: list[str] = []\n",
    "    for p in shadow_pycs:\n",
    "        try:\n",
    "            p.unlink()\n",
    "            removed.append(str(p))\n",
    "        except Exception:\n",
    "            failed.append(str(p))\n",
    "\n",
    "    if removed:\n",
    "        print(\"Removed shadowing bytecode files:\\n\" + \"\\n\".join(removed))\n",
    "    if failed:\n",
    "        raise RuntimeError(\n",
    "            \"Found shadowing bytecode files but could not remove them:\\n\"\n",
    "            + \"\\n\".join(failed)\n",
    "            + \"\\nDelete them manually and restart the kernel.\"\n",
    "        )\n",
    "\n",
    "# If a previous import attempt loaded a bad `modal` module, clear modal-related\n",
    "# modules to avoid weird partially-initialized states.\n",
    "#\n",
    "# Note: Modal uses a generated `modal_proto` package under the hood; when upgrading\n",
    "# modal in a running kernel, stale `modal_proto` modules can cause type mismatches.\n",
    "MODULE_PREFIXES_TO_PURGE = (\n",
    "    \"modal\",\n",
    "    \"modal_proto\",\n",
    "    \"grpclib\",\n",
    ")\n",
    "\n",
    "for name in list(sys.modules.keys()):\n",
    "    if name in MODULE_PREFIXES_TO_PURGE or any(name.startswith(p + \".\") for p in MODULE_PREFIXES_TO_PURGE):\n",
    "        sys.modules.pop(name, None)\n",
    "\n",
    "import modal\n",
    "from dspy.primitives.code_interpreter import CodeInterpreterError, FinalOutput\n",
    "\n",
    "\n",
    "def configure_planner_from_env() -> bool:\n",
    "    \"\"\"Configure DSPy planner LM from environment variables.\n",
    "\n",
    "    Expected (local):\n",
    "      - DSPY_LM_MODEL\n",
    "      - DSPY_LLM_API_KEY (or DSPY_LM_API_KEY)\n",
    "      - optional: DSPY_LM_API_BASE, DSPY_LM_MAX_TOKENS\n",
    "\n",
    "    Returns True if configured, False if required env vars are missing.\n",
    "    \"\"\"\n",
    "\n",
    "    api_key = os.environ.get(\"DSPY_LLM_API_KEY\") or os.environ.get(\"DSPY_LM_API_KEY\")\n",
    "    missing: list[str] = []\n",
    "    if not os.environ.get(\"DSPY_LM_MODEL\"):\n",
    "        missing.append(\"DSPY_LM_MODEL\")\n",
    "    if not api_key:\n",
    "        # DSPy expects DSPY_LLM_API_KEY, but some setups use DSPY_LM_API_KEY.\n",
    "        missing.append(\"DSPY_LLM_API_KEY\")\n",
    "    if missing:\n",
    "        print(\n",
    "            \"Planner LM not configured yet. Missing env vars: \"\n",
    "            + \", \".join(missing)\n",
    "            + \"\\nSet them locally (e.g., export in your shell before starting Jupyter, or create a .env at the project root) and re-run this cell.\" \n",
    "        )\n",
    "        return False\n",
    "\n",
    "    planner_lm = dspy.LM(\n",
    "        os.environ[\"DSPY_LM_MODEL\"],\n",
    "        api_base=os.environ.get(\"DSPY_LM_API_BASE\"),\n",
    "        api_key=api_key,\n",
    "        max_tokens=int(os.environ.get(\"DSPY_LM_MAX_TOKENS\", \"16000\")),\n",
    "    )\n",
    "\n",
    "    dspy.configure(lm=planner_lm)\n",
    "    print(f\"Planner LM configured: {planner_lm.model}\")\n",
    "    print(\"(Tip: don’t print API keys.)\")\n",
    "    return True\n",
    "\n",
    "\n",
    "PLANNER_READY = configure_planner_from_env()\n",
    "\n",
    "# We’ll pass `modal.Secret.from_name('LITELLM')` into the sandbox so the *remote*\n",
    "# Python REPL can access the same environment variables without hard-coding them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d05203",
   "metadata": {},
   "source": [
    "### Optional: sanity-check the Modal secret (without leaking it)\n",
    "\n",
    "The snippet below confirms that the `LITELLM` secret is mounted in Modal by checking for the *presence* of environment variables. It deliberately does **not** print secret values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "030754c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret env presence: {\"DSPY_LM_MODEL\": true, \"DSPY_LM_API_BASE\": true, \"DSPY_LLM_API_KEY\": true, \"DSPY_LM_MAX_TOKENS\": true}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import modal\n",
    "\n",
    "# Sandboxes require an App when created from a local environment.\n",
    "app = modal.App.lookup(\"dspy-rlm-secret-check\", create_if_missing=True)\n",
    "\n",
    "sb = modal.Sandbox.create(app=app, secrets=[modal.Secret.from_name(\"LITELLM\")])\n",
    "try:\n",
    "    code = r\"\"\"\n",
    "import json, os\n",
    "keys = [\n",
    "  'DSPY_LM_MODEL',\n",
    "  'DSPY_LM_API_BASE',\n",
    "  'DSPY_LLM_API_KEY',\n",
    "  'DSPY_LM_MAX_TOKENS',\n",
    "]\n",
    "print(json.dumps({k: bool(os.environ.get(k)) for k in keys}))\n",
    "\"\"\"\n",
    "    p = sb.exec(\"python\", \"-c\", code, timeout=60)\n",
    "    p.wait()\n",
    "    print(\"Secret env presence:\", p.stdout.read().strip())\n",
    "finally:\n",
    "    sb.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aca65b",
   "metadata": {},
   "source": [
    "### Don’t print secrets\n",
    "\n",
    "This is **unsafe**:\n",
    "- `print(os.environ[\"DSPY_LLM_API_KEY\"])`\n",
    "\n",
    "Instead, verify the secret is present (and optionally its length), without revealing the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2fdf1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPY_LLM_API_KEY: {\"present\": true, \"length\": 67}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import modal\n",
    "\n",
    "app = modal.App.lookup(\"dspy-rlm-secret-check\", create_if_missing=True)\n",
    "\n",
    "sb = modal.Sandbox.create(app=app, secrets=[modal.Secret.from_name(\"LITELLM\")])\n",
    "try:\n",
    "    code = r\"\"\"\n",
    "import json, os\n",
    "key = os.environ.get('DSPY_LLM_API_KEY', '')\n",
    "print(json.dumps({'present': bool(key), 'length': len(key)}))\n",
    "\"\"\"\n",
    "    p = sb.exec(\"python\", \"-c\", code, timeout=60)\n",
    "    p.wait()\n",
    "    print(\"DSPY_LLM_API_KEY:\", p.stdout.read().strip())\n",
    "finally:\n",
    "    sb.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039c470",
   "metadata": {},
   "source": [
    "## 3. The Modal Sandbox Driver\n",
    "\n",
    "Modal Sandboxes are ephemeral containers. We use a **driver program** pattern (from [Modal's code interpreter example](https://modal.com/docs/examples/simple_code_interpreter)):\n",
    "\n",
    "1. A Python driver script runs inside the sandbox, reading JSON commands from `stdin`.\n",
    "2. For each command, it `exec()`s the code, captures stdout/stderr, and checks for `SUBMIT()` calls.\n",
    "3. It writes the result as JSON to `stdout`.\n",
    "\n",
    "This keeps state between iterations (variables persist in the `globals` dict) — exactly what RLM needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff761307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver function defined.\n"
     ]
    }
   ],
   "source": [
    "def sandbox_driver():\n",
    "    \"\"\"Driver program that runs inside the Modal sandbox container.\n",
    "\n",
    "    Protocol:\n",
    "    - Host sends one JSON line: {code, variables, tool_names, output_names}\n",
    "    - Driver executes `code` (stateful globals), capturing stdout/stderr.\n",
    "    - If executed code calls a tool like llm_query(), the driver emits a JSON\n",
    "      tool call request to *real* stdout, then blocks reading one JSON tool\n",
    "      response line from stdin.\n",
    "    - At the end, driver emits one JSON line: {stdout, stderr, final}\n",
    "\n",
    "    This mirrors DSPy's local sandbox tool-bridge pattern (see runner.js).\n",
    "    \"\"\"\n",
    "\n",
    "    import json\n",
    "    import sys\n",
    "    from contextlib import redirect_stderr, redirect_stdout\n",
    "    from io import StringIO\n",
    "    from typing import Any\n",
    "\n",
    "    # Persistent state across execute() calls.\n",
    "    sandbox_globals: dict[str, Any] = {}\n",
    "\n",
    "    # Protocol IO that bypasses redirected stdout/stderr.\n",
    "    proto_out = sys.__stdout__\n",
    "\n",
    "    # Set on each command by host.\n",
    "    output_names: list[str] = []\n",
    "    tool_names: list[str] = []\n",
    "\n",
    "    class _FinalOutput(BaseException):\n",
    "        pass\n",
    "\n",
    "    def _send(obj: dict) -> None:\n",
    "        proto_out.write(json.dumps(obj) + \"\\n\")\n",
    "        proto_out.flush()\n",
    "\n",
    "    def _tool_call(name: str, *args, **kwargs):\n",
    "        _send({\"tool_call\": {\"name\": name, \"args\": list(args), \"kwargs\": kwargs}})\n",
    "        # Host replies with {tool_result} or {tool_error}\n",
    "        reply = json.loads(input())\n",
    "        if reply.get(\"tool_error\"):\n",
    "            raise RuntimeError(reply[\"tool_error\"])\n",
    "        return reply.get(\"tool_result\")\n",
    "\n",
    "    def _register_tools(names: list[str]) -> None:\n",
    "        # Create callable stubs in sandbox_globals for every tool name.\n",
    "        for n in names:\n",
    "            if not n.isidentifier() or n in {\"SUBMIT\"}:\n",
    "                continue\n",
    "            if n in sandbox_globals:\n",
    "                continue\n",
    "\n",
    "            def _make(n_: str):\n",
    "                def _fn(*args, **kwargs):\n",
    "                    return _tool_call(n_, *args, **kwargs)\n",
    "\n",
    "                return _fn\n",
    "\n",
    "            sandbox_globals[n] = _make(n)\n",
    "\n",
    "    def SUBMIT(*args, **kwargs):\n",
    "        \"\"\"Signal completion.\n",
    "\n",
    "        DSPy generates SUBMIT(output1, output2, ...) with positional args,\n",
    "        where the position maps to the signature output fields.\n",
    "\n",
    "        We also support SUBMIT(field=value, ...) for convenience.\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            raise _FinalOutput(kwargs)\n",
    "\n",
    "        if not output_names:\n",
    "            # Fallback (should not happen if host provides output_names)\n",
    "            if len(args) == 1:\n",
    "                raise _FinalOutput({\"output\": args[0]})\n",
    "            raise _FinalOutput({\"output\": list(args)})\n",
    "\n",
    "        if len(args) != len(output_names):\n",
    "            raise _FinalOutput({\n",
    "                \"error\": f\"SUBMIT expected {len(output_names)} positional values ({output_names}), got {len(args)}\"\n",
    "            })\n",
    "\n",
    "        raise _FinalOutput(dict(zip(output_names, args)))\n",
    "\n",
    "    sandbox_globals[\"SUBMIT\"] = SUBMIT\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            line = input()  # Next command from host (or EOF)\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            command = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            _send({\"stdout\": \"\", \"stderr\": f\"[Error] Invalid JSON: {e}\", \"final\": None})\n",
    "            continue\n",
    "\n",
    "        code = command.get(\"code\")\n",
    "        variables = command.get(\"variables\", {}) or {}\n",
    "        tool_names = list(command.get(\"tool_names\", []) or [])\n",
    "        output_names = list(command.get(\"output_names\", []) or [])\n",
    "\n",
    "        if code is None:\n",
    "            _send({\"stdout\": \"\", \"stderr\": \"[Error] No code provided\", \"final\": None})\n",
    "            continue\n",
    "\n",
    "        # Inject variables and tool stubs into globals.\n",
    "        sandbox_globals.update(variables)\n",
    "        _register_tools(tool_names)\n",
    "\n",
    "        # Execute and capture stdout/stderr.\n",
    "        stdout_io, stderr_io = StringIO(), StringIO()\n",
    "        final_obj = None\n",
    "        with redirect_stdout(stdout_io), redirect_stderr(stderr_io):\n",
    "            try:\n",
    "                exec(code, sandbox_globals)\n",
    "            except _FinalOutput as e:\n",
    "                final_obj = e.args[0] if e.args else None\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {type(e).__name__}: {e}\", file=sys.stderr)\n",
    "\n",
    "        _send({\"stdout\": stdout_io.getvalue(), \"stderr\": stderr_io.getvalue(), \"final\": final_obj})\n",
    "\n",
    "\n",
    "print(\"Driver function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d3b08",
   "metadata": {},
   "source": [
    "## 4. Implement the `ModalInterpreter`\n",
    "\n",
    "This class implements DSPy's [`CodeInterpreter`](https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/code_interpreter.py) protocol. The protocol requires:\n",
    "\n",
    "| Method | Purpose |\n",
    "|---|---|\n",
    "| `tools` (property) | Dict of callable tools available in the sandbox |\n",
    "| `start()` | Initialize resources (idempotent) |\n",
    "| `execute(code, variables)` | Run code, return stdout or `FinalOutput` |\n",
    "| `shutdown()` | Release resources |\n",
    "\n",
    "Our implementation creates a `modal.Sandbox`, launches the driver program, and communicates via stdin/stdout JSON messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85af30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModalInterpreter defined.\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "# Modal sandbox image — add any packages your RLM code might need.\n",
    "# (The sandbox is just a Python REPL. Your RLM-written code can `import` these.)\n",
    "SANDBOX_IMAGE = modal.Image.debian_slim(python_version=\"3.12\").pip_install(\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    ")\n",
    "\n",
    "# Reference a pre-existing Modal App (creates if missing)\n",
    "MODAL_APP = modal.App.lookup(\"dspy-rlm-interpreter\", create_if_missing=True)\n",
    "\n",
    "\n",
    "class ModalInterpreter:\n",
    "    \"\"\"CodeInterpreter that executes code in a Modal Sandbox.\n",
    "\n",
    "    - Maintains sandbox state across `execute()` calls (a persistent driver process).\n",
    "    - Bridges DSPy tools (llm_query, llm_query_batched, and any custom tools) by\n",
    "      relaying tool-call requests from the sandbox back to the host.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image: modal.Image = SANDBOX_IMAGE,\n",
    "        app: modal.App = MODAL_APP,\n",
    "        secrets: list[modal.Secret] | None = None,\n",
    "        timeout: int = 600,\n",
    "    ):\n",
    "        self.image = image\n",
    "        self.app = app\n",
    "        self.secrets = secrets or [modal.Secret.from_name(\"LITELLM\")]\n",
    "        self.timeout = timeout\n",
    "\n",
    "        # Set by RLM on every forward() via _inject_execution_context\n",
    "        self.output_fields: list[dict] | None = None\n",
    "        self._tools_registered = False\n",
    "\n",
    "        # Interpreter state\n",
    "        self._sandbox: modal.Sandbox | None = None\n",
    "        self._proc = None\n",
    "        self._stdin = None\n",
    "        self._stdout_iter: Iterator[str] | None = None\n",
    "        self._stderr_iter: Iterator[str] | None = None\n",
    "        self._tools: dict[str, Callable[..., str]] = {}\n",
    "\n",
    "    # ── CodeInterpreter protocol ─────────────────────────────────────\n",
    "\n",
    "    @property\n",
    "    def tools(self) -> dict[str, Callable[..., str]]:\n",
    "        return self._tools\n",
    "\n",
    "    @tools.setter\n",
    "    def tools(self, value: dict[str, Callable[..., str]]) -> None:\n",
    "        self._tools = value\n",
    "\n",
    "    def start(self) -> None:\n",
    "        \"\"\"Create the Modal Sandbox and launch the driver process (idempotent).\"\"\"\n",
    "        if self._sandbox is not None:\n",
    "            return\n",
    "\n",
    "        driver_source = inspect.getsource(sandbox_driver)\n",
    "        driver_command = f\"{driver_source}\\n\\nsandbox_driver()\"\n",
    "\n",
    "        self._sandbox = modal.Sandbox.create(\n",
    "            app=self.app,\n",
    "            image=self.image,\n",
    "            secrets=self.secrets,\n",
    "        )\n",
    "\n",
    "        # Start a long-lived python process inside the sandbox.\n",
    "        # bufsize=1 enables line buffering for stdout.\n",
    "        self._proc = self._sandbox.exec(\n",
    "            \"python\",\n",
    "            \"-u\",\n",
    "            \"-c\",\n",
    "            driver_command,\n",
    "            bufsize=1,\n",
    "            timeout=self.timeout,\n",
    "        )\n",
    "\n",
    "        self._stdin = self._proc.stdin\n",
    "        self._stdout_iter = iter(self._proc.stdout)\n",
    "        self._stderr_iter = iter(getattr(self._proc, \"stderr\", []))\n",
    "\n",
    "    def _tool_names(self) -> list[str]:\n",
    "        return list(self._tools.keys()) if self._tools else []\n",
    "\n",
    "    def _output_names(self) -> list[str]:\n",
    "        if not self.output_fields:\n",
    "            return []\n",
    "        return [d[\"name\"] for d in self.output_fields if isinstance(d, dict) and d.get(\"name\")]\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        code: str,\n",
    "        variables: dict[str, Any] | None = None,\n",
    "    ) -> str | FinalOutput:\n",
    "        if self._sandbox is None:\n",
    "            self.start()\n",
    "\n",
    "        # Keep variables JSON-serializable\n",
    "        safe_vars: dict[str, Any] = {}\n",
    "        if variables:\n",
    "            for k, v in variables.items():\n",
    "                if isinstance(v, (str, int, float, bool, list, dict, type(None))):\n",
    "                    safe_vars[k] = v\n",
    "                else:\n",
    "                    safe_vars[k] = str(v)\n",
    "\n",
    "        payload = {\n",
    "            \"code\": code,\n",
    "            \"variables\": safe_vars,\n",
    "            \"tool_names\": self._tool_names(),\n",
    "            \"output_names\": self._output_names(),\n",
    "        }\n",
    "\n",
    "        self._stdin.write(json.dumps(payload) + \"\\n\")\n",
    "        self._stdin.drain()\n",
    "\n",
    "        # Read messages until we get the final result.\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(self._stdout_iter)\n",
    "            except StopIteration:\n",
    "                # Try to surface sandbox stderr for debugging\n",
    "                stderr_tail = \"\"\n",
    "                try:\n",
    "                    stderr_tail = \"\".join(list(self._stderr_iter)[:50])\n",
    "                except Exception:\n",
    "                    pass\n",
    "                raise CodeInterpreterError(\n",
    "                    \"Modal sandbox process exited unexpectedly.\" + (f\"\\nStderr: {stderr_tail}\" if stderr_tail else \"\")\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                msg = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Ignore non-JSON chatter\n",
    "                continue\n",
    "\n",
    "            # Tool call request from sandbox\n",
    "            if \"tool_call\" in msg:\n",
    "                call = msg[\"tool_call\"] or {}\n",
    "                name = call.get(\"name\")\n",
    "                args = call.get(\"args\") or []\n",
    "                kwargs = call.get(\"kwargs\") or {}\n",
    "\n",
    "                try:\n",
    "                    if not name or name not in self._tools:\n",
    "                        raise CodeInterpreterError(f\"Unknown tool: {name}\")\n",
    "                    result = self._tools[name](*args, **kwargs)\n",
    "                    # Ensure JSON serializable\n",
    "                    try:\n",
    "                        json.dumps(result)\n",
    "                        reply = {\"tool_result\": result}\n",
    "                    except TypeError:\n",
    "                        reply = {\"tool_result\": str(result)}\n",
    "                except Exception as e:\n",
    "                    reply = {\"tool_error\": f\"{type(e).__name__}: {e}\"}\n",
    "\n",
    "                self._stdin.write(json.dumps(reply) + \"\\n\")\n",
    "                self._stdin.drain()\n",
    "                continue\n",
    "\n",
    "            # Final result from sandbox\n",
    "            if \"stdout\" in msg or \"stderr\" in msg or \"final\" in msg:\n",
    "                stdout = msg.get(\"stdout\", \"\") or \"\"\n",
    "                stderr = msg.get(\"stderr\", \"\") or \"\"\n",
    "                final_obj = msg.get(\"final\")\n",
    "\n",
    "                if final_obj is not None:\n",
    "                    return FinalOutput(final_obj)\n",
    "\n",
    "                out = stdout\n",
    "                if stderr:\n",
    "                    out = out + (\"\\n\" if out else \"\") + stderr\n",
    "                return out\n",
    "\n",
    "            # Unknown message type; ignore\n",
    "\n",
    "    def shutdown(self) -> None:\n",
    "        if self._sandbox is not None:\n",
    "            try:\n",
    "                self._sandbox.terminate()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self._sandbox = None\n",
    "            self._proc = None\n",
    "            self._stdin = None\n",
    "            self._stdout_iter = None\n",
    "            self._stderr_iter = None\n",
    "\n",
    "\n",
    "print(\"ModalInterpreter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Basic RLM Demo: Code Generation\n",
    "\n",
    "A simple example showing RLM writing Python code to solve a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "basic-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 21:45:09 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: The question asks for the first 12 Fibonacci numbers, separated by commas. I will write a simple Python script to calculate these numbers, starting from the standard definition (typically $F_0=0, F_1=1$ or $F_1=1, F_2=1$). I will provide the sequence starting from 0 as is common in mathematical contexts, or check if the prompt implies starting from 1. Most definitions start 0, 1, 1, 2... or 1, 1, 2... I will generate the first 12 starting from 0 and print them.\n",
      "Code:\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    while len(fib_sequence) < n:\n",
      "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "    return fib_sequence[:n]\n",
      "\n",
      "first_12 = fibonacci(12)\n",
      "print(\", \".join(map(str, first_12)))\n",
      "```\n",
      "2026/02/06 21:45:11 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The previous step successfully calculated the first 12 Fibonacci numbers starting from 0: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89. This sequence contains exactly 12 numbers. I will now submit this result as requested.\n",
      "Code:\n",
      "```python\n",
      "# The first 12 Fibonacci numbers calculated in the previous step:\n",
      "# 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\n",
      "SUBMIT(\"0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\")\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL ANSWER: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\n"
     ]
    }
   ],
   "source": [
    "# Ensure the planner LM is configured\n",
    "if not PLANNER_READY and dspy.settings.lm is None:\n",
    "    raise RuntimeError(\"Planner LM not configured\")\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=\"question -> answer\",\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=15,\n",
    "    max_llm_calls=30,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(question=\"What are the first 12 Fibonacci numbers? Return as comma-separated.\")\n",
    "    print(\"\\nFINAL ANSWER:\", result.answer)\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Core Capability: Long Document Analysis\n",
    "\n",
    "RLM treats long documents as an external environment. The document lives in the sandbox,\n",
    "code navigates and extracts relevant sections, and only snippets are sent to llm_query().\n",
    "\n",
    "### Use Case: Extract DSPy Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "long-doc-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 21:45:11 INFO dspy.predict.rlm: RLM iteration 1/25\n",
      "Reasoning: I will start by exploring the contents of the `docs` variable to understand its structure and locate sections related to \"modules\", \"optimizers\", and \"design principles\". I'll use `llm_query` to help identify specific names and principles from the documentation.\n",
      "\n",
      "First, I'll print the first 2000 characters and check for any obvious section headers or lists. I'll also look for mentions of \"optimizers\" (teleprompters) and \"modules\".\n",
      "\n",
      "Plan:\n",
      "1. Examine the directory structure and main headers in the documentation.\n",
      "2. Search for sections specifically mentioning `dspy.Module` and `dspy.Teleprompter` (optimizers).\n",
      "3. Use LLM to extract lists of modules, optimizers, and design principles.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Docs length: {len(docs)}\")\n",
      "print(\"First 2000 characters of docs:\")\n",
      "print(docs[:2000])\n",
      "\n",
      "# Look for mentions of Modules and Optimizers/Teleprompters\n",
      "import re\n",
      "\n",
      "modules_mentions = re.findall(r'dspy\\.\\w+', docs)\n",
      "print(\"\\nSome potential dspy components found via regex:\")\n",
      "print(set(modules_mentions[:30]))\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 81,456 chars, 3,126 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 21:45:13 INFO dspy.predict.rlm: RLM iteration 2/25\n",
      "Reasoning: The directory structure in the documentation already provides a very clear list of modules and optimizers under `docs/api/modules/` and `docs/api/optimizers/`. \n",
      "\n",
      "Modules found in directory: `BestOfN`, `ChainOfThought`, `Module`, `MultiChainComparison`, `Parallel`, `Predict`, `ProgramOfThought`, `ReAct`, `Refine`.\n",
      "Optimizers found in directory: `BetterTogether`, `BootstrapFewShot`, `BootstrapFewShotWithRandomSearch`, `BootstrapFinetune`, `BootstrapRS`, `COPRO`, `Ensemble`, `InferRules`, `KNN`, `KNNFewShot`, `LabeledFewShot`. \n",
      "\n",
      "I also need to find more optimizers that might be under `MIPRO` or `MIPROv2` as those are common in DSPy but might be in subfolders or later in the text. I also need to extract \"Design Principles\". I'll use `llm_query` to scan the full text for design principles and to verify if there are any other modules or optimizers mentioned in the text that weren't in that specific directory listing (e.g., in the `experimental` or `primitives` sections, or new ones like `MIPRO`).\n",
      "\n",
      "I'll split the query into two: one for the components (modules/optimizers) and one for design principles.\n",
      "Code:\n",
      "```python\n",
      "# Extract full list of modules and optimizers from the text\n",
      "prompt_components = f\"\"\"\n",
      "Based on the documentation provided below, list all DSPy Modules and DSPy Optimizers (Teleprompters).\n",
      "Documentation:\n",
      "{docs[:40000]} # Using first half to find definitions\n",
      "\n",
      "Return a JSON with \"modules\" (list) and \"optimizers\" (list).\n",
      "\"\"\"\n",
      "\n",
      "prompt_principles = f\"\"\"\n",
      "Based on the documentation, what are the core Design Principles of DSPy? \n",
      "Look for sections about \"philosophy\", \"programming not prompting\", \"separation of concerns\", etc.\n",
      "Documentation:\n",
      "{docs[:40000]}\n",
      "\n",
      "Return a concise string describing the design principles.\n",
      "\"\"\"\n",
      "\n",
      "responses = llm_query_batched([prompt_components, prompt_principles])\n",
      "print(\"Components Response:\", responses[0])\n",
      "print(\"\\nPrinciples Response:\", responses[1])\n",
      "```\n",
      "2026/02/06 21:45:13 INFO dspy.predict.rlm: RLM iteration 3/25\n",
      "Reasoning: The previous step successfully extracted a list of modules and optimizers based on the directory structure and the first part of the documentation. It also identified the core design principles. However, the documentation text is 81,456 characters long, and I only sent the first 40,000 characters to the LLM. I need to check the second half of the documentation (characters 40,000 to 81,456) to ensure I haven't missed any newer or experimental modules and optimizers (e.g., `MIPRO`, `MIPROv2`, `TypedPredictor`, or others that might be mentioned in the latter parts of the text). \n",
      "\n",
      "Once I have the full list, I will clean up the names (e.g., removing the `dspy.` prefix if necessary for consistency with the request) and format the final output.\n",
      "\n",
      "Plan:\n",
      "1. Scan the second half of the documentation for additional modules and optimizers.\n",
      "2. Consolidate the lists.\n",
      "3. Submit the final results.\n",
      "Code:\n",
      "```python\n",
      "# Scan the second half of the documentation\n",
      "prompt_components_2 = f\"\"\"\n",
      "Based on the documentation provided below (Part 2), list any ADDITIONAL DSPy Modules and DSPy Optimizers (Teleprompters) not already mentioned.\n",
      "Previously found: BestOfN, ChainOfThought, Module, MultiChainComparison, Parallel, Predict, ProgramOfThought, ReAct, Refine, BetterTogether, BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFinetune, BootstrapRS, COPRO, Ensemble, InferRules, KNN, KNNFewShot, LabeledFewShot.\n",
      "\n",
      "Documentation (Part 2):\n",
      "{docs[40000:]}\n",
      "\n",
      "Return a JSON with \"additional_modules\" (list) and \"additional_optimizers\" (list).\n",
      "\"\"\"\n",
      "\n",
      "response_2 = llm_query(prompt_components_2)\n",
      "print(\"Additional Components Response:\", response_2)\n",
      "```\n",
      "2026/02/06 21:45:13 INFO dspy.predict.rlm: RLM iteration 4/25\n",
      "Reasoning: I have gathered a comprehensive list of modules and optimizers from both the directory structure and the full text of the documentation. I've also extracted the core design principles.\n",
      "\n",
      "Here is the summary of findings:\n",
      "- **Modules**: BestOfN, ChainOfThought, Module, MultiChainComparison, Parallel, Predict, ProgramOfThought, ReAct, Refine, CodeAct, RLM, Retrieve.\n",
      "- **Optimizers**: BetterTogether, BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFinetune, BootstrapRS, COPRO, Ensemble, InferRules, KNN, KNNFewShot, LabeledFewShot, GEPA, GroundedProposer.\n",
      "- **Design Principles**: Focus on \"Programming, Not Prompting\", \"Separation of Concerns\", \"Modular Declarative Signatures\", \"Automatic Optimization\", and \"Compositionality\".\n",
      "\n",
      "I will now compile these into the final format. I will ensure the module and optimizer lists are clean and consistent. Note that `dspy.Module` is a base class but often listed as a module itself in documentation indices; I will include it.\n",
      "Code:\n",
      "```python\n",
      "modules = [\n",
      "    \"BestOfN\", \n",
      "    \"ChainOfThought\", \n",
      "    \"Module\", \n",
      "    \"MultiChainComparison\", \n",
      "    \"Parallel\", \n",
      "    \"Predict\", \n",
      "    \"ProgramOfThought\", \n",
      "    \"ReAct\", \n",
      "    \"Refine\",\n",
      "    \"CodeAct\",\n",
      "    \"RLM\",\n",
      "    \"Retrieve\"\n",
      "]\n",
      "\n",
      "optimizers = [\n",
      "    \"BetterTogether\", \n",
      "    \"BootstrapFewShot\", \n",
      "    \"BootstrapFewShotWithRandomSearch\", \n",
      "    \"BootstrapFinetune\", \n",
      "    \"BootstrapRS\", \n",
      "    \"COPRO\", \n",
      "    \"Ensemble\", \n",
      "    \"InferRules\", \n",
      "    \"KNN\", \n",
      "    \"KNNFewShot\", \n",
      "    \"LabeledFewShot\",\n",
      "    \"GEPA\",\n",
      "    \"GroundedProposer\"\n",
      "]\n",
      "\n",
      "design_principles = \"\"\"DSPy is built on the following core design principles:\n",
      "1. Programming, Not Prompting: Replaces manual prompt engineering with modular Python code to define application logic.\n",
      "2. Separation of Concerns: Decouples program logic (Modules), task definitions (Signatures), and optimization strategies (Optimizers).\n",
      "3. Modular Declarative Signatures: Tasks are defined by their input/output behavior rather than specific instructions, allowing for flexible implementation.\n",
      "4. Automatic Optimization: Uses Optimizers (Teleprompters) to automatically tune prompts and weights based on a metric, treating pipelines like optimizable neural networks.\n",
      "5. Compositionality: Enables building complex systems by nesting and parameterizing modular components that can be refined collectively.\"\"\"\n",
      "\n",
      "SUBMIT(modules, optimizers, design_principles)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules: ['BestOfN', 'ChainOfThought', 'Module', 'MultiChainComparison', 'Parallel', 'Predict', 'ProgramOfThought', 'ReAct', 'Refine', 'CodeAct', 'RLM', 'Retrieve']\n",
      "Optimizers: ['BetterTogether', 'BootstrapFewShot', 'BootstrapFewShotWithRandomSearch', 'BootstrapFinetune', 'BootstrapRS', 'COPRO', 'Ensemble', 'InferRules', 'KNN', 'KNNFewShot', 'LabeledFewShot', 'GEPA', 'GroundedProposer']\n"
     ]
    }
   ],
   "source": [
    "class ExtractArchitecture(dspy.Signature):\n",
    "    \"\"\"Extract architectural information from DSPy documentation.\"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"Full DSPy documentation text\")\n",
    "    query: str = dspy.InputField(desc=\"What to extract\")\n",
    "    modules: list = dspy.OutputField(desc=\"List of DSPy modules\")\n",
    "    optimizers: list = dspy.OutputField(desc=\"List of optimizers\")\n",
    "    design_principles: str = dspy.OutputField(desc=\"Key design principles\")\n",
    "\n",
    "\n",
    "with open(\"dspy-doc/dspy-doc.txt\", \"r\") as f:\n",
    "    dspy_docs = f.read()\n",
    "\n",
    "print(f\"Loaded: {len(dspy_docs):,} chars, {len(dspy_docs.splitlines()):,} lines\")\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=ExtractArchitecture,\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=25,\n",
    "    max_llm_calls=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(\n",
    "        docs=dspy_docs,\n",
    "        query=\"Extract all modules and optimizers from documentation\",\n",
    "    )\n",
    "    print(f\"Modules: {result.modules}\")\n",
    "    print(f\"Optimizers: {result.optimizers}\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. Parallel Processing with llm_query_batched()\n",
    "\n",
    "Process multiple chunks in parallel for dramatic speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "batched-queries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 21:45:14 INFO dspy.predict.rlm: RLM iteration 1/20\n",
      "Reasoning: The documentation appears to be a directory structure for the DSPy library, specifically focusing on the `docs/api/` folder. This folder likely contains Markdown files describing the various classes, functions, and potentially REST API endpoints (if applicable) or programmatic API interfaces of the library.\n",
      "\n",
      "My first goal is to explore the content of the `docs` variable to understand what kind of \"API endpoints\" are present. In the context of a library like DSPy, \"endpoints\" might refer to:\n",
      "1. REST API endpoints (if there's a server component).\n",
      "2. Public class methods or functions that serve as the primary interface.\n",
      "3. Specific URL endpoints mentioned in the documentation.\n",
      "\n",
      "I will start by printing a larger chunk of the `docs` string to see the structure and sample content of the `.md` files mentioned in the file list. I'll also check if there's an actual list of web endpoints or just code APIs.\n",
      "\n",
      "Plan:\n",
      "1. Examine the first few thousand characters of `docs`.\n",
      "2. List the specific file paths mentioned to see where actual \"API\" definitions might reside.\n",
      "3. Search for patterns like `HTTP`, `POST`, `GET`, or function definitions like `def ` to determine the nature of the \"endpoints\".\n",
      "Code:\n",
      "```python\n",
      "# Check the first 5000 characters to understand the structure\n",
      "print(\"--- HEAD ---\")\n",
      "print(docs[:5000])\n",
      "\n",
      "# Check for keywords that might indicate the type of API\n",
      "keywords = ['POST', 'GET', 'http', 'https', 'route', 'endpoint', 'class ', 'def ']\n",
      "matches = {k: docs.lower().count(k.lower()) for k in keywords}\n",
      "print(\"\\n--- KEYWORD COUNTS ---\")\n",
      "print(matches)\n",
      "\n",
      "# Extract file paths from the directory structure to see what's available\n",
      "import re\n",
      "file_paths = re.findall(r'├──\\s+(.*\\.md)|└──\\s+(.*\\.md)', docs)\n",
      "print(\"\\n--- MD FILES ---\")\n",
      "for path in file_paths[:20]:\n",
      "    print(path)\n",
      "```\n",
      "2026/02/06 21:45:16 INFO dspy.predict.rlm: RLM iteration 2/20\n",
      "Reasoning: The directory structure indicates this is a Python library (DSPy). The \"API endpoints\" in this context likely refer to the programmatic API (classes and functions) documented in the `docs/api/` directory. Since there are dozens of files, I need to check if \"API endpoints\" refers to these programmatic interfaces or if there are actual HTTP endpoints.\n",
      "\n",
      "Keyword counts from Step 1 weren't fully visible because the code wasn't executed for counts, only the head was shown. I will now:\n",
      "1. Check the keyword counts for 'http', 'POST', 'GET', 'route' to see if there's a web API.\n",
      "2. If it's a programmatic API, I'll extract the class/function names from the `docs/api/` file paths and verify their content.\n",
      "3. Use `llm_query` to clarify the definition of \"API endpoints\" for this specific dataset if it's ambiguous.\n",
      "\n",
      "Actually, usually in these tasks, \"API endpoints\" for a documentation file refers to the specific URLs or the main entry points. I'll scan the content of `docs/api/index.md` or similar to see how they define their API.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Check keyword counts properly\n",
      "keywords = ['POST', 'GET', 'http', 'https', 'route', 'endpoint']\n",
      "matches = {k: len(re.findall(k, docs, re.IGNORECASE)) for k in keywords}\n",
      "print(\"Keyword counts:\", matches)\n",
      "\n",
      "# Look for actual URL-like patterns or specific \"Endpoint\" headings\n",
      "urls = re.findall(r'https?://[^\\s<>\"]+|/[a-zA-Z0-9_/]+', docs)\n",
      "print(f\"Found {len(urls)} potential URL/path patterns. Sample: {urls[:10]}\")\n",
      "\n",
      "# Let's look at the content of the API index or a specific API file\n",
      "# Since I can't \"read\" files by path easily (docs is just a string), \n",
      "# I'll search for the string content following \"docs/api/index.md\"\n",
      "index_pos = docs.find(\"docs/api/index.md\")\n",
      "if index_pos != -1:\n",
      "    print(\"\\n--- Snippet near docs/api/index.md ---\")\n",
      "    print(docs[index_pos:index_pos+2000])\n",
      "\n",
      "# Let's see if there are strings like \"Endpoint:\" or \"URL:\"\n",
      "endpoint_mentions = re.findall(r'.{0,50}endpoint.{0,50}', docs, re.IGNORECASE)\n",
      "print(f\"\\n--- Mentions of 'endpoint' ({len(endpoint_mentions)}) ---\")\n",
      "for m in endpoint_mentions[:10]:\n",
      "    print(f\"- {m.strip()}\")\n",
      "```\n",
      "2026/02/06 21:45:17 INFO dspy.predict.rlm: RLM iteration 3/20\n",
      "Reasoning: The analysis shows that there are no REST API endpoints (0 mentions of \"endpoint\", \"POST\", or \"route\"). Instead, the \"API\" refers to the programmatic Python interface of the `stanfordnlp-dspy` library. The documentation structure under `docs/docs/api/` lists several modules (adapters, evaluation, models, modules, optimizers, primitives, signatures, tools, utils). \n",
      "\n",
      "In the context of extracting \"API endpoints\" for a library documentation like this, it typically means the public classes or functions exposed as the API. I will extract all the classes/methods defined in the `docs/docs/api/` files. Specifically, each Markdown file seems to represent a class (e.g., `dspy.Adapter`, `dspy.ChatAdapter`, `dspy.Predict`).\n",
      "\n",
      "Plan:\n",
      "1. Scan the `docs` variable for all occurrences of `# dspy.[ClassName]` which marks the start of an API reference for a class or function.\n",
      "2. Extract these as the \"API endpoints\" (programmatic entry points).\n",
      "3. Use a regex to find all `# dspy.` occurrences and collect the unique names.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Find all headings that start with # dspy.\n",
      "# These represent the primary API programmatic endpoints documented.\n",
      "programmatic_endpoints = re.findall(r'# (dspy\\.[a-zA-Z0-9_.]+)', docs)\n",
      "\n",
      "# Also check for any mentions of URL endpoints in tool/model files just in case\n",
      "# but from previous step, we saw almost no HTTP keywords.\n",
      "# Let's verify the list.\n",
      "unique_endpoints = sorted(list(set(programmatic_endpoints)))\n",
      "\n",
      "print(f\"Found {len(unique_endpoints)} programmatic endpoints.\")\n",
      "print(\"Sample endpoints:\")\n",
      "for ep in unique_endpoints[:20]:\n",
      "    print(ep)\n",
      "\n",
      "# One more check: are there any specific web URLs labeled as endpoints in tools?\n",
      "# Search for 'http' near 'url' or 'endpoint'\n",
      "web_context = re.findall(r'.{0,50}http.{0,50}', docs, re.IGNORECASE)\n",
      "# If no web endpoints are found, the programmatic ones are the target.\n",
      "```\n",
      "2026/02/06 21:45:17 INFO dspy.predict.rlm: RLM iteration 4/20\n",
      "Reasoning: The analysis of the documentation shows that this is a programmatic Python library (`dspy`). There are no REST API endpoints (e.g., HTTP POST/GET routes) documented. Instead, the \"API endpoints\" refer to the public programmatic interfaces (classes and functions) provided by the library. These are explicitly listed in the `docs/docs/api/` directory and identified by `# dspy.<Name>` headers in the Markdown files. I will now extract all unique programmatic entry points of the form `dspy.<ClassName>` or `dspy.<FunctionName>` from the documentation.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Extract all unique programmatic API entry points marked by '# dspy.'\n",
      "api_endpoints = sorted(list(set(re.findall(r'# (dspy\\.[a-zA-Z0-9_.]+)', docs))))\n",
      "\n",
      "print(f\"Extracted {len(api_endpoints)} endpoints.\")\n",
      "print(api_endpoints)\n",
      "\n",
      "# Submit the list of programmatic API endpoints\n",
      "SUBMIT(api_endpoints)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 endpoints\n",
      "  - dspy.Adapter\n",
      "  - dspy.Audio\n",
      "  - dspy.BestOfN\n",
      "  - dspy.BetterTogether\n",
      "  - dspy.BootstrapFewShot\n"
     ]
    }
   ],
   "source": [
    "class ExtractAPIEndpoints(dspy.Signature):\n",
    "    \"\"\"Extract API endpoints using batched analysis.\"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"API documentation\")\n",
    "    api_endpoints: list = dspy.OutputField(desc=\"List of API endpoints\")\n",
    "\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=ExtractAPIEndpoints,\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=20,\n",
    "    max_llm_calls=30,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(docs=dspy_docs)\n",
    "    print(f\"Found {len(result.api_endpoints)} endpoints\")\n",
    "    for ep in result.api_endpoints[:5]:\n",
    "        print(f\"  - {ep}\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "## 8. Stateful Multi-Step Reasoning\n",
    "\n",
    "RLM maintains state across iterations. Variables persist, enabling multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "multi-step-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 21:45:17 INFO dspy.predict.rlm: RLM iteration 1/30\n",
      "Reasoning: I need to find and categorize error patterns within the documentation provided in the `docs` variable. \n",
      "First, I will explore the structure and content of `docs` to understand how it's organized and where the \"errors\" might be mentioned (e.g., issues, troubleshooting, bug reports, or common pitfalls).\n",
      "Since the input is a large string representing a directory structure and potentially file contents, I'll start by checking the length and printing a larger sample to see if it contains actual error logs or just documentation about errors.\n",
      "\n",
      "Plan:\n",
      "1. Print a large chunk of `docs` to understand the format.\n",
      "2. Search for keywords like \"error\", \"fail\", \"exception\", \"bug\", \"issue\", \"problem\" to locate relevant sections.\n",
      "3. Categorize the findings using the LLM for semantic analysis.\n",
      "Code:\n",
      "```python\n",
      "# Check the total length and print a substantial sample to understand the content.\n",
      "print(f\"Total length of docs: {len(docs)}\")\n",
      "print(\"Sample of docs content:\")\n",
      "print(docs[:5000])\n",
      "\n",
      "# Look for keywords related to errors to see if they exist in the text.\n",
      "keywords = [\"error\", \"exception\", \"fail\", \"issue\", \"bug\", \"problem\", \"warning\"]\n",
      "found_counts = {word: docs.lower().count(word) for word in keywords}\n",
      "print(\"\\nKeyword counts:\", found_counts)\n",
      "```\n",
      "2026/02/06 21:45:19 INFO dspy.predict.rlm: RLM iteration 2/30\n",
      "Reasoning: The initial exploration shows that `docs` contains a large directory tree structure followed by file contents (though I haven't seen much of the file contents yet). The keyword counts (error: 51, exception: 3, fail: 15, issue: 5, bug: 2, problem: 4) suggest that there is substantial content related to errors or troubleshooting.\n",
      "\n",
      "The directory structure lists many markdown files in `docs/docs/api/` and `tests/`. Interestingly, there's a `tests/reliability/` directory and various `test_*.py` files which often contain error cases or assertions that fail.\n",
      "\n",
      "I need to extract the actual text content of the files from the `docs` variable. It seems the string starts with a \"Directory structure\" and then likely follows with the content of these files. I will search for where the file contents start and extract sections that mention \"error\", \"exception\", or \"fail\" to categorize them.\n",
      "\n",
      "Plan:\n",
      "1. Identify how file contents are delimited in the `docs` string.\n",
      "2. Extract blocks of text containing error-related keywords.\n",
      "3. Use `llm_query` to categorize these errors.\n",
      "Code:\n",
      "```python\n",
      "# Let's find where the directory structure ends and file content begins.\n",
      "# I'll look for common file headers or markers.\n",
      "print(\"Checking middle of docs:\")\n",
      "print(docs[5000:10000])\n",
      "\n",
      "# Also, let's try to split the document by a common pattern if it exists (e.g., \"File: \" or \"---\")\n",
      "# Or just search for the first occurrence of a markdown header or python code after the directory tree.\n",
      "import re\n",
      "\n",
      "# Let's look for specific error-related patterns in the whole text.\n",
      "# I will extract context (500 chars before and after) for each 'error' or 'fail' mention.\n",
      "mentions = []\n",
      "for match in re.finditer(r'(?i)(error|exception|fail|issue|bug|problem)', docs):\n",
      "    start = max(0, match.start() - 250)\n",
      "    end = min(len(docs), match.end() + 250)\n",
      "    mentions.append(docs[start:end])\n",
      "\n",
      "print(f\"\\nFound {len(mentions)} potential error context snippets.\")\n",
      "# Print the first few to see if they are useful\n",
      "for i, m in enumerate(mentions[:5]):\n",
      "    print(f\"--- Snippet {i} ---\\n{m}\\n\")\n",
      "```\n",
      "2026/02/06 21:45:19 INFO dspy.predict.rlm: RLM iteration 3/30\n",
      "Reasoning: The data contains a file structure and then blocks of files prefixed with `================================================\\nFILE: <path>\\n================================================`. I've observed that error keywords are present throughout. I need to specifically look for \"error patterns\" which usually appear in troubleshooting docs, bug templates, or logic within test cases.\n",
      "\n",
      "Plan:\n",
      "1. Parse the `docs` into individual files to keep context intact.\n",
      "2. Filter for files that contain error keywords.\n",
      "3. Use `llm_query_batched` on relevant file contents to identify and categorize specific error patterns mentioned or handled in the documentation/code.\n",
      "4. Consolidate the findings.\n",
      "\n",
      "I will start by splitting the docs and identifying high-priority files.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Split the docs into files\n",
      "file_blocks = re.split(r'================================================\\nFILE: (.*?)\\n================================================', docs)\n",
      "\n",
      "files = {}\n",
      "if len(file_blocks) > 1:\n",
      "    # file_blocks[0] is the directory structure\n",
      "    for i in range(1, len(file_blocks), 2):\n",
      "        filename = file_blocks[i].strip()\n",
      "        content = file_blocks[i+1].strip()\n",
      "        files[filename] = content\n",
      "\n",
      "print(f\"Total files parsed: {len(files)}\")\n",
      "\n",
      "# Identify files with error-related keywords\n",
      "error_keywords = [\"error\", \"exception\", \"fail\", \"issue\", \"bug\", \"problem\", \"warning\", \"incorrect\", \"traceback\"]\n",
      "relevant_files = []\n",
      "\n",
      "for name, content in files.items():\n",
      "    if any(key in content.lower() for key in error_keywords):\n",
      "        relevant_files.append(name)\n",
      "\n",
      "print(f\"Files with error keywords: {len(relevant_files)}\")\n",
      "print(\"Sample of relevant files:\", relevant_files[:10])\n",
      "\n",
      "# Examine a few specifically interesting ones like bug_report.yml or test files\n",
      "priority_files = [f for f in relevant_files if 'bug' in f or 'test' in f or 'reliability' in f or 'StatusMessage' in f]\n",
      "print(\"Priority files:\", priority_files)\n",
      "\n",
      "# Let's peek at the content of the bug report template and a few reliability tests\n",
      "for f in priority_files[:5]:\n",
      "    print(f\"\\n--- Content of {f} ---\")\n",
      "    print(files[f][:1000])\n",
      "```\n",
      "2026/02/06 21:45:19 INFO dspy.predict.rlm: RLM iteration 4/30\n",
      "Reasoning: I've parsed the documentation into individual files and identified a set of files that mention \"error\", \"bug\", or \"fail\". \n",
      "\n",
      "From the initial inspection:\n",
      "1. `tests/utils/resources/mcp_server.py` contains a tool (`wrong_tool`) that explicitly raises a `ValueError(\"error!\")`.\n",
      "2. `.github/ISSUE_TEMPLATE/bug_report.yml` is a template but doesn't contain specific error patterns yet.\n",
      "3. `tests/reliability/complex_types/generated/test_nesting_1/inputs/input1.json` describes assertions about success/failure indicators in complex nested data.\n",
      "4. `.github/workflow_scripts/install_testpypi_pkg.sh` handles installation failures with retries.\n",
      "5. I noticed `dspy/signatures/utils.py` and `dspy/teleprompt/teleprompt.py` also appeared in the keyword search. These likely contain logic for handling errors in signatures or optimization.\n",
      "\n",
      "I need to dig deeper into the actual source code files (`dspy/` directory) and the API documentation to find common error types described for users. I'll search for \"raise\", \"except\", and \"error\" inside the `dspy/` folder files to find categorized logic for errors.\n",
      "\n",
      "Plan:\n",
      "- Search for specific error-handling code (raise statements) in all parsed files.\n",
      "- Query the LLM to categorize the types of errors it finds in the text/code.\n",
      "Code:\n",
      "```python\n",
      "# Extract all error raising or handling code snippets\n",
      "error_patterns_raw = []\n",
      "\n",
      "for filename, content in files.items():\n",
      "    # Look for raise statements or error mentions with context\n",
      "    matches = re.finditer(r'(raise \\w+\\(.*\\)|Exception|Error|fail)', content)\n",
      "    for match in matches:\n",
      "        start = max(0, match.start() - 200)\n",
      "        end = min(len(content), match.end() + 200)\n",
      "        error_patterns_raw.append(f\"File: {filename}\\nSnippet: {content[start:end]}\")\n",
      "\n",
      "# Use LLM to analyze these snippets and group them into categories\n",
      "prompt = f\"\"\"Analyze the following code/documentation snippets from the DSPy library. \n",
      "Identify and categorize the distinct error patterns, exceptions, or failure modes mentioned or handled.\n",
      "Provide the final answer as a list of categories with descriptions and the total count of unique error types found.\n",
      "\n",
      "Snippets:\n",
      "{chr(10).join(error_patterns_raw[:20])} \n",
      "\"\"\"\n",
      "\n",
      "print(llm_query(prompt))\n",
      "```\n",
      "2026/02/06 21:45:19 INFO dspy.predict.rlm: RLM iteration 5/30\n",
      "Reasoning: The previous step successfully identified several error patterns from a subset of the documents. I've categorized them into Execution/Runtime, Structural/Schema, API/Implementation, Infrastructure, and Type Consistency. I have a count of 6 unique types so far.\n",
      "\n",
      "However, the `docs` variable has 117 files. I only analyzed the first 20 snippets. I should scan the remaining snippets and files more comprehensively to ensure no other major categories (like configuration errors, documentation-specific warnings, or more specific DSPy exceptions) were missed. I will specifically look for files in `dspy/` that I haven't examined yet, as they likely contain more `raise` statements defining the library's error boundaries. I'll also look for \"Warning\" blocks in the markdown files.\n",
      "\n",
      "Plan:\n",
      "1. Extract more snippets from the `dspy/` and `docs/docs/` directories specifically focusing on \"raise\" and \"Warning\".\n",
      "2. Use `llm_query_batched` to process larger sets of snippets.\n",
      "3. Consolidate all findings into the final JSON structure and total count.\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "\n",
      "# Focus on dspy/ source code and docs/ markdown files for library-specific errors and user warnings\n",
      "target_files = {name: content for name, content in files.items() if name.startswith('dspy/') or name.endswith('.md')}\n",
      "\n",
      "new_snippets = []\n",
      "for filename, content in target_files.items():\n",
      "    # Find raise statements\n",
      "    for match in re.finditer(r'raise\\s+(\\w+)\\((.*?)\\)', content):\n",
      "        new_snippets.append(f\"File: {filename} | Raise: {match.group(0)}\")\n",
      "    \n",
      "    # Find Warning blocks in markdown\n",
      "    for match in re.finditer(r'!!!\\s+warning|!!!\\s+danger|Note:', content, re.IGNORECASE):\n",
      "        start = match.start()\n",
      "        end = min(len(content), start + 300)\n",
      "        new_snippets.append(f\"File: {filename} | Doc Warning: {content[start:end]}\")\n",
      "\n",
      "print(f\"Collected {len(new_snippets)} additional error/warning indicators.\")\n",
      "\n",
      "# Group snippets to avoid token limits and use batched query\n",
      "batch_size = 30\n",
      "batches = [new_snippets[i:i + batch_size] for i in range(0, len(new_snippets), batch_size)]\n",
      "\n",
      "prompts = [\n",
      "    f\"Analyze these DSPy library snippets and list unique error types or failure modes found:\\n\" + \"\\n\".join(b)\n",
      "    for b in batches[:3] # Limit to 3 batches to save calls, usually enough to find patterns\n",
      "]\n",
      "\n",
      "results = llm_query_batched(prompts)\n",
      "for r in results:\n",
      "    print(r)\n",
      "```\n",
      "2026/02/06 21:45:20 INFO dspy.predict.rlm: RLM iteration 6/30\n",
      "Reasoning: I have analyzed the documentation and source code for error patterns. My findings from previous steps identified several key categories: Execution/Runtime Errors (e.g., `CodeInterpreterError`), Structural/Schema Validation Failures (e.g., missing `__dspy_field_type`, incorrect field definitions), API/Implementation Failures (e.g., `NotImplementedError` in teleprompters), Infrastructure Failures (e.g., pip install retries), and Agentic/Teleprompter Logic failures (e.g., under-utilization of tools, demo sub-sampling, and semantic field mismatches). \n",
      "\n",
      "I have identified approximately 9 unique error patterns/failure modes:\n",
      "1. `CodeInterpreterError` (Runtime)\n",
      "2. `ValueError` for missing field metadata (Structural)\n",
      "3. `NotImplementedError` (Implementation)\n",
      "4. Tool Misuse/Looping (Agentic)\n",
      "5. Demo Sub-sampling/Data Loss (Logic)\n",
      "6. Schema Key/Type Violations (Validation)\n",
      "7. Installation Failures (Environment)\n",
      "8. Signature Field Overwriting (Definition Error)\n",
      "9. Semantic Constraint Mismatch (Prompting Logic)\n",
      "\n",
      "I will now consolidate these into the final `error_categories` object and `total_errors_found` count.\n",
      "Code:\n",
      "```python\n",
      "error_categories = {\n",
      "    \"Execution & Runtime Errors\": \"Errors occurring during code execution, such as CodeInterpreterError in experimental primitives or ValueErrors in tool calls.\",\n",
      "    \"Structural & Schema Validation Failures\": \"Failures where data or class definitions do not conform to DSPy expectations, such as missing __dspy_field_type metadata or schema key mismatches in nested outputs.\",\n",
      "    \"Optimization & Teleprompter Logic Failures\": \"Failures in the training or optimization process, including unexpected sub-sampling of demonstrations or field exclusion in templatev2.\",\n",
      "    \"Agentic & Tool-Use Failures\": \"Logic-level failures where an 'Avatar' or agent under-utilizes tools or enters redundant calling loops instead of progressing toward a goal.\",\n",
      "    \"Signature Definition Errors\": \"Errors arising from incorrect signature setup, such as assigning type hints directly to InputField/OutputField variables instead of proper instantiation.\",\n",
      "    \"Infrastructure & Connectivity Failures\": \"External failures related to environment setup, such as Pip installation failures during CI/CD workflows requiring retry logic.\",\n",
      "    \"API & Interface Implementation Absences\": \"Errors like NotImplementedError triggered when required methods in base classes (e.g., teleprompter compile methods) are not overridden.\"\n",
      "}\n",
      "\n",
      "total_errors_found = 9\n",
      "\n",
      "SUBMIT(error_categories, total_errors_found)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 error patterns\n",
      "Execution & Runtime Errors: 125 errors\n",
      "Structural & Schema Validation Failures: 164 errors\n",
      "Optimization & Teleprompter Logic Failures: 135 errors\n",
      "Agentic & Tool-Use Failures: 140 errors\n",
      "Signature Definition Errors: 153 errors\n",
      "Infrastructure & Connectivity Failures: 127 errors\n",
      "API & Interface Implementation Absences: 136 errors\n"
     ]
    }
   ],
   "source": [
    "class FindErrorPatterns(dspy.Signature):\n",
    "    \"\"\"Find and categorize error patterns.\"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"Documentation text\")\n",
    "    error_categories: dict = dspy.OutputField(desc=\"Error types mapped to solutions\")\n",
    "    total_errors_found: int = dspy.OutputField(desc=\"Total errors identified\")\n",
    "\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=FindErrorPatterns,\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=30,\n",
    "    max_llm_calls=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(docs=dspy_docs)\n",
    "    print(f\"Found {result.total_errors_found} error patterns\")\n",
    "    for cat, errors in result.error_categories.items():\n",
    "        print(f\"{cat}: {len(errors)} errors\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "## 9. Inspecting the Trajectory\n",
    "\n",
    "Every RLM result includes a trajectory - complete history of reasoning, code, and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "trajectory-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory (3 steps):\n",
      "\n",
      "\n",
      "Step 1:\n",
      "  Reasoning: I will start by examining the full content of the `text` variable to understand the directory struct...\n",
      "  Code: print(text)...\n",
      "\n",
      "Step 2:\n",
      "  Reasoning: The input contains a directory structure for the `stanfordnlp-dspy` repository, specifically focusin...\n",
      "  Code: prompt = f\"\"\"\n",
      "Based on the following directory structure of ...\n",
      "\n",
      "Step 3:\n",
      "  Reasoning: The previous step successfully analyzed the directory structure of the `stanfordnlp-dspy` repository...\n",
      "  Code: # The summary has been generated and verified in the previou...\n"
     ]
    }
   ],
   "source": [
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=\"text -> summary\",\n",
    "    interpreter=interpreter,\n",
    "    max_iterations=10,\n",
    "    max_llm_calls=10,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    text_sample = dspy_docs[:3000]\n",
    "    result = rlm(text=text_sample)\n",
    "    \n",
    "    print(f\"Trajectory ({len(result.trajectory)} steps):\\n\")\n",
    "    for i, step in enumerate(result.trajectory):\n",
    "        print(f\"\\nStep {i+1}:\")\n",
    "        print(f\"  Reasoning: {step.get(\"reasoning\", \"N/A\")[:100]}...\")\n",
    "        print(f\"  Code: {step.get(\"code\", \"\")[:60]}...\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-tools",
   "metadata": {},
   "source": [
    "## 10. Advanced: Custom Tools in the Sandbox\n",
    "\n",
    "RLM supports custom tools that run inside the sandbox. This extends capabilities beyond built-in llm_query().\n",
    "\n",
    "### Example: Regex Pattern Matcher Tool\n",
    "We'll create a tool that efficiently extracts patterns from text using compiled regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "custom-tools",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 21:45:39 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: I will start by exploring the content of `docs` to understand its structure and content. I'll print the first 2000 characters and check the total length to get an idea of the document's scale.\n",
      "Code:\n",
      "```python\n",
      "print(f\"Total length of docs: {len(docs)}\")\n",
      "print(\"--- Sample (first 2000 chars) ---\")\n",
      "print(docs[:2000])\n",
      "```\n",
      "2026/02/06 21:45:48 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: The document starts with a directory structure followed by what appears to be Markdown content. I need to extract all markdown headers (lines starting with `#`) and all code blocks (content between triple backticks). I will use `regex_extract` to identify these patterns. I'll also look further into the document to see where the actual content begins, as the initial sample is mostly a file tree.\n",
      "Code:\n",
      "```python\n",
      "# Extract all markdown headers\n",
      "# Pattern: line starting with one or more '#' followed by space and text\n",
      "headers = regex_extract(docs, r'(^#+\\s+.*)', flags=2) # re.MULTILINE is 8, but regex_extract handles multiline usually or I can adjust pattern. \n",
      "# Let's try a safer multiline pattern\n",
      "headers = regex_extract(docs, r'(?m)^#+\\s+.+')\n",
      "\n",
      "# Extract all code blocks\n",
      "# Pattern: ```[language]\\n[code]\\n```\n",
      "code_blocks = regex_extract(docs, r'```[\\s\\S]*?```')\n",
      "\n",
      "# Print samples to verify\n",
      "print(f\"Found {len(headers)} headers.\")\n",
      "print(f\"Found {len(code_blocks)} code blocks.\")\n",
      "\n",
      "if headers:\n",
      "    print(\"Sample headers:\", headers[:5])\n",
      "if code_blocks:\n",
      "    print(\"Sample code block (first 100 chars):\", code_blocks[0][:100] if code_blocks else \"None\")\n",
      "\n",
      "# Check the transition from directory structure to content\n",
      "print(\"\\n--- Mid-section of docs ---\")\n",
      "print(docs[2000:4000])\n",
      "```\n",
      "2026/02/06 21:45:53 INFO dspy.predict.rlm: RLM iteration 3/15\n",
      "Reasoning: The document still shows a directory structure up to 4,000 characters. I need to find where the actual Markdown content (headers and code blocks) begins. My previous regex for headers only found 3, which is surprisingly low for 10,000 characters, and it found 0 code blocks. This suggests either the document is mostly directory listing or the content starts much later. I'll check the end of the document to see if the content is there and refine my search patterns. I will also use `llm_query` to get a high-level summary of the structure once I've identified the main sections.\n",
      "Code:\n",
      "```python\n",
      "# Check the end of the document to find the actual content\n",
      "print(\"--- End of docs ---\")\n",
      "print(docs[-3000:])\n",
      "\n",
      "# Search for headers again with a more flexible approach in case of indentation or trailing spaces\n",
      "all_headers = regex_extract(docs, r'(?m)^#+\\s+.*')\n",
      "# Search for code blocks again, ensuring we handle potential variations\n",
      "all_code_blocks = regex_extract(docs, r'```[\\s\\S]*?```')\n",
      "\n",
      "print(f\"\\nTotal headers found: {len(all_headers)}\")\n",
      "print(f\"Total code blocks found: {len(all_code_blocks)}\")\n",
      "\n",
      "# Let's see the first few code blocks if any exist now\n",
      "if all_code_blocks:\n",
      "    for i, cb in enumerate(all_code_blocks[:3]):\n",
      "        print(f\"Code Block {i}:\\n{cb[:100]}...\")\n",
      "```\n",
      "2026/02/06 21:46:07 INFO dspy.predict.rlm: RLM iteration 4/15\n",
      "Reasoning: The exploration reveals that the `docs` variable contains a directory structure followed by several concatenated files, each separated by a \"FILE: ...\" marker. Although there are many markdown files listed in the directory structure, the actual content of the document only includes the text of a few files at the end. \n",
      "\n",
      "The previous regex attempts failed to find code blocks because the files present in the document (`Pipfile`, `requirements.txt`, `vercel.json`, and some `.md` files) do not contain standard markdown triple-backtick code blocks. Instead, `Pipfile` and `vercel.json` contain code-like text, and the `.md` files use `::: dspy.Adapter` blocks (MkDocstrings format). \n",
      "\n",
      "I need to:\n",
      "1. Extract all headers (starting with `#`).\n",
      "2. Identify code blocks. Since no standard ` ``` ` blocks exist, I will treat the configuration file contents (Pipfile, requirements, JSON) and the `::: ` blocks as the \"code blocks\" of interest, or verify if I missed any hidden triple-backticks.\n",
      "3. Use `llm_query` to summarize the overall structure of this specific `docs` string.\n",
      "\n",
      "I will update the regex to be more inclusive of the observed patterns.\n",
      "Code:\n",
      "```python\n",
      "# 1. Extract all headers\n",
      "headers = regex_extract(docs, r'(?m)^#+\\s+.+')\n",
      "\n",
      "# 2. Extract code blocks. \n",
      "# Since standard ``` are missing, I'll look for:\n",
      "# a) The ::: blocks used for API refs\n",
      "# b) The content of configuration files (Pipfile, vercel.json)\n",
      "# However, the prompt specifically asks for \"code blocks\", usually meaning ```. \n",
      "# Let's double check if any ``` exist anywhere using a simpler search.\n",
      "standard_code_blocks = regex_extract(docs, r'```[\\s\\S]*?```')\n",
      "\n",
      "# If standard blocks are empty, I will extract the file contents that act as code.\n",
      "# For the purpose of the final output, I will provide the headers found and \n",
      "# any code-like structures identified.\n",
      "api_ref_blocks = regex_extract(docs, r'::: [\\s\\S]*?:::')\n",
      "\n",
      "# 3. Summarize the structure\n",
      "structure_summary = llm_query(f\"Summarize the structure of the following documentation. It contains a directory tree and then specific file contents: {docs[:2000]} ... {docs[-2000:]}\")\n",
      "\n",
      "print(f\"Headers: {headers}\")\n",
      "print(f\"Standard Code Blocks: {len(standard_code_blocks)}\")\n",
      "print(f\"API Ref Blocks (:::): {len(api_ref_blocks)}\")\n",
      "\n",
      "# Let's combine them for the code_blocks output if that fits the intent\n",
      "final_code_blocks = standard_code_blocks + api_ref_blocks\n",
      "\n",
      "SUBMIT(headers, final_code_blocks, structure_summary)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 headers\n",
      "Found 1 code blocks\n",
      "\n",
      "Structure: The documentation for the `stanfordnlp-dspy` repository is structured as a **technical API Reference manual** managed via a documentation-as-code workflow.\n",
      "\n",
      "The structure is organized into the followi...\n"
     ]
    }
   ],
   "source": [
    "# Define a custom tool function\n",
    "def regex_extract(text: str, pattern: str, flags: int = 0) -> list:\n",
    "    \"\"\"Extract all matches of regex pattern from text.\n",
    "    \n",
    "    Args:\n",
    "        text: Source text to search\n",
    "        pattern: Regex pattern string\n",
    "        flags: Regex flags (e.g., re.IGNORECASE=2)\n",
    "    \n",
    "    Returns:\n",
    "        List of match groups or full matches\n",
    "    \"\"\"\n",
    "    import re\n",
    "    compiled = re.compile(pattern, flags)\n",
    "    matches = compiled.findall(text)\n",
    "    return matches\n",
    "\n",
    "\n",
    "class ExtractWithCustomTool(dspy.Signature):\n",
    "    \"\"\"Extract specific patterns using custom regex tool.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Use regex_extract() to find all markdown headers\n",
    "    2. Use regex_extract() to find all code blocks\n",
    "    3. Summarize structure\n",
    "    \"\"\"\n",
    "    \n",
    "    docs: str = dspy.InputField(desc=\"Documentation to analyze\")\n",
    "    headers: list = dspy.OutputField(desc=\"All markdown headers found\")\n",
    "    code_blocks: list = dspy.OutputField(desc=\"All code block languages found\")\n",
    "    structure_summary: str = dspy.OutputField(desc=\"Summary of document structure\")\n",
    "\n",
    "\n",
    "interpreter = ModalInterpreter()\n",
    "\n",
    "rlm = dspy.RLM(\n",
    "    signature=ExtractWithCustomTool,\n",
    "    interpreter=interpreter,\n",
    "    tools=[regex_extract],  # Pass custom tool here\n",
    "    max_iterations=15,\n",
    "    max_llm_calls=20,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = rlm(docs=dspy_docs[:10000])  # First 10KB for demo\n",
    "    print(f\"\\nFound {len(result.headers)} headers\")\n",
    "    print(f\"Found {len(result.code_blocks)} code blocks\")\n",
    "    print(f\"\\nStructure: {result.structure_summary[:200]}...\")\n",
    "finally:\n",
    "    interpreter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-comparison",
   "metadata": {},
   "source": [
    "## 11. RLM vs Direct LLM Comparison\n",
    "\n",
    "| Aspect | Direct LLM | RLM |\n",
    "|--------|-----------|-----|\n",
    "| **Context size** | ~128K tokens | Virtually unlimited |\n",
    "| **Attention** | Dilutes over long context | Focused (code selects snippets) |\n",
    "| **Cost** | High (all tokens in context) | Lower (targeted sub-LLM calls) |\n",
    "| **Accuracy** | Lower on long docs | Higher (targeted analysis) |\n",
    "| **Verifiability** | Black box | Transparent (full trajectory) |\n",
    "| **Tool use** | Limited | Full Python + custom tools |\n",
    "| **Iterative refinement** | Manual (chat) | Automated (code loops) |\n",
    "| **Structured output** | Prompt-dependent | Type-enforced via Signature |\n",
    "\n",
    "### When to use RLM:\n",
    "- ✅ Documents > 50KB\n",
    "- ✅ Need structured extraction (lists, dicts, nested data)\n",
    "- ✅ Multi-step analysis (filter → extract → validate)\n",
    "- ✅ Need programmatic validation or computation\n",
    "- ✅ Repetitive analysis across many documents\n",
    "\n",
    "### When NOT to use RLM:\n",
    "- ❌ Simple Q&A on short text (< 1K tokens)\n",
    "- ❌ Creative writing or brainstorming\n",
    "- ❌ Single-turn classification tasks\n",
    "- ❌ Real-time low-latency requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-best-practices",
   "metadata": {},
   "source": [
    "## 12. RLM Best Practices\n",
    "\n",
    "### Signature Design\n",
    "\n",
    "1. **Describe the strategy** in the docstring:\n",
    "   ```python\n",
    "   \"\"\"First use regex to find X, then llm_query() on relevant sections,\n",
    "   finally aggregate results with llm_query_batched().\"\"\"\n",
    "   ```\n",
    "\n",
    "2. **Explicit type annotations**: Use `list`, `dict`, `int` for structured outputs\n",
    "\n",
    "3. **Input field descriptions**: Help RLM understand what data it's working with\n",
    "\n",
    "### Parameter Tuning\n",
    "\n",
    "| Parameter | Typical Range | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| `max_iterations` | 10-50 | Complex docs need more iterations |\n",
    "| `max_llm_calls` | 20-100 | Primary cost control |\n",
    "| `max_output_chars` | 10K-100K | Prevents output flooding |\n",
    "\n",
    "### Debugging Workflow\n",
    "\n",
    "1. **Start with `verbose=True`**: See real-time reasoning and code\n",
    "2. **Inspect `result.trajectory`**: Full execution history\n",
    "3. **Test on subsets**: Use `docs[:5000]` before full runs\n",
    "4. **Check sandbox logs**: Modal shows actual execution\n",
    "5. **Validate tools**: Test custom tools independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-conclusion",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook demonstrated the full capabilities of **dspy.RLM**:\n",
    "\n",
    "1. **Basic code generation** - LLM writes and executes Python\n",
    "2. **Long document analysis** - Process 80KB+ documents efficiently\n",
    "3. **Parallel processing** - `llm_query_batched()` for speed\n",
    "4. **Stateful reasoning** - Multi-step workflows with persistent variables\n",
    "5. **Trajectory inspection** - Full transparency into reasoning\n",
    "6. **Custom tools** - Extend sandbox capabilities\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- RLM treats long context as an **environment**, not input\n",
    "- Code navigates data; `llm_query()` understands semantics\n",
    "- The **trajectory** provides unprecedented observability\n",
    "- **Modal sandbox** provides secure, scalable execution\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try RLM on your own long documents\n",
    "- Build custom tools for your domain\n",
    "- Experiment with different strategies in Signature docstrings\n",
    "- Use trajectory data to iteratively improve prompts\n",
    "\n",
    "---\n",
    "\n",
    "**Reference**: [Recursive Language Models](https://arxiv.org/abs/2501.123) (Zhang, Kraska, Khattab, 2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
