# LiteLLM Proxy Configuration
# Proxy URL: https://litellm-proxy-gojcb5mtua-uc.a.run.app
#
# This configuration defines all 15 models available in the model hub.
# Each model is configured with its provider-specific settings.
#
# Authentication: Uses DSPY_LLM_API_KEY from .env as the master key.
# Clients must send: Authorization: Bearer $DSPY_LLM_API_KEY
#
# /v1/responses Endpoint Compatibility (Tested 2025-02-07):
# ✅ 11 models work: All Deepinfra, All Nvidia NIM, Gemini Flash/Pro (chat), Vertex DeepSeek
# ❌ 3 models have issues:
#   - gemini/gemini-2.5-computer-use-preview: 400 Bad Request (use chat/completions)
#   - gemini/gemini-3-pro-image-preview: Image generation (use images/generations)
#   - kimi-k2-thinking-maas: Times out (>60s for simple queries)

model_list:
  # ==================== Deepinfra Models ====================
  - model_name: deepinfra/deepseek-ai/DeepSeek-R1-0528
    litellm_params:
      model: deepinfra/deepseek-ai/DeepSeek-R1-0528
      api_key: os.environ/DEEPINFRA_API_KEY
    model_info:
      mode: chat
      max_input_tokens: 164000
      max_output_tokens: 164000
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.00000215
      features: []

  - model_name: deepinfra/deepseek-ai/DeepSeek-V3.2
    litellm_params:
      model: deepinfra/deepseek-ai/DeepSeek-V3.2
      api_key: os.environ/DEEPINFRA_API_KEY
    model_info:
      mode: chat
      input_cost_per_token: 0
      output_cost_per_token: 0
      features: []

  - model_name: deepinfra/nvidia/Nemotron-3-Nano-30B-A3B
    litellm_params:
      model: deepinfra/nvidia/Nemotron-3-Nano-30B-A3B
      api_key: os.environ/DEEPINFRA_API_KEY
    model_info:
      mode: chat
      input_cost_per_token: 0
      output_cost_per_token: 0
      features: []

  - model_name: moonshotai/Kimi-K2.5
    litellm_params:
      model: deepinfra/moonshotai/Kimi-K2.5
      api_key: os.environ/DEEPINFRA_API_KEY
    model_info:
      mode: chat
      input_cost_per_token: 0
      output_cost_per_token: 0
      features: []

  # ==================== Vertex AI Models ====================
  - model_name: deepseek-v3.2-maas
    litellm_params:
      model: vertex_ai/deepseek-v3.2-maas
      vertex_project: os.environ/VERTEX_PROJECT_ID
      vertex_location: os.environ/VERTEX_LOCATION
    model_info:
      mode: chat
      max_input_tokens: 164000
      max_output_tokens: 33000
      input_cost_per_token: 0.00000056
      output_cost_per_token: 0.00000168
      features:
        - reasoning

  - model_name: gemini-3-flash-preview
    litellm_params:
      model: vertex_ai/gemini-3-flash-preview
      vertex_project: os.environ/VERTEX_PROJECT_ID
      vertex_location: os.environ/VERTEX_LOCATION
    model_info:
      mode: chat
      max_input_tokens: 1049000
      max_output_tokens: 66000
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.000003
      features:
        - vision
      rpm: 60

  - model_name: kimi-k2-thinking-maas
    litellm_params:
      model: vertex_ai/kimi-k2-thinking-maas
      vertex_project: os.environ/VERTEX_PROJECT_ID
      vertex_location: os.environ/VERTEX_LOCATION
    model_info:
      mode: chat
      max_input_tokens: 256000
      max_output_tokens: 256000
      input_cost_per_token: 0.0000006
      output_cost_per_token: 0.0000025
      features:
        - web_search
        - thinking
      # NOTE: This thinking model may take longer to respond (>60s).
      # Consider increasing timeout for complex queries.

  # ==================== Gemini Models ====================
  - model_name: gemini/gemini-2.5-computer-use-preview-10-2025
    litellm_params:
      model: gemini/gemini-2.5-computer-use-preview-10-2025
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: chat
      max_input_tokens: 128000
      max_output_tokens: 64000
      input_cost_per_token: 0.00000125
      output_cost_per_token: 0.00001
      features:
        - vision
        - computer_use
      rpm: 2000
      tpm: 800000
      # NOTE: This computer-use model may have compatibility issues with
      # /v1/responses. Consider using /v1/chat/completions instead.

  - model_name: gemini/gemini-3-flash-preview
    litellm_params:
      model: gemini/gemini-3-flash-preview
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: chat
      max_input_tokens: 1049000
      max_output_tokens: 66000
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.000003
      features:
        - vision
      rpm: 2000
      tpm: 800000

  - model_name: gemini/gemini-3-pro-image-preview
    litellm_params:
      model: gemini/gemini-3-pro-image-preview
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: image_generation
      max_input_tokens: 66000
      max_output_tokens: 33000
      input_cost_per_token: 0.000002
      output_cost_per_token: 0.000012
      features:
        - vision
      rpm: 1000
      tpm: 4000000
      # NOTE: This is an image generation model. Use /v1/images/generations
      # endpoint instead of /v1/responses or /v1/chat/completions

  - model_name: gemini/gemini-3-pro-preview
    litellm_params:
      model: gemini/gemini-3-pro-preview
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: chat
      max_input_tokens: 1049000
      max_output_tokens: 66000
      input_cost_per_token: 0.000002
      output_cost_per_token: 0.000012
      features:
        - vision
      rpm: 2000
      tpm: 800000

  - model_name: gemini/gemini-embedding-001
    litellm_params:
      model: gemini/gemini-embedding-001
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: embedding
      max_input_tokens: 2000
      input_cost_per_token: 0.00000015
      output_cost_per_token: 0
      rpm: 10000
      tpm: 10000000

  # ==================== Nvidia NIM Models ====================
  - model_name: nvidia_nim/moonshotai/kimi-k2-instruct-0905
    litellm_params:
      model: nvidia_nim/moonshotai/kimi-k2-instruct-0905
      api_key: os.environ/NVIDIA_NIM_API_KEY
    model_info:
      mode: chat
      input_cost_per_token: 0
      output_cost_per_token: 0
      features: []

  - model_name: nvidia_nim/nvidia/nemotron-3-nano-30b-a3b
    litellm_params:
      model: nvidia_nim/nvidia/nemotron-3-nano-30b-a3b
      api_key: os.environ/NVIDIA_NIM_API_KEY
    model_info:
      mode: chat
      input_cost_per_token: 0
      output_cost_per_token: 0
      features: []

  - model_name: nvidia_nim/qwen/qwen3-next-80b-a3b-instruct
    litellm_params:
      model: nvidia_nim/qwen/qwen3-next-80b-a3b-instruct
      api_key: os.environ/NVIDIA_NIM_API_KEY
    model_info:
      mode: chat
      input_cost_per_token: 0
      output_cost_per_token: 0
      features: []

# ==================== RLM Settings ====================
rlm_settings:
  # Maximum iterations for RLM code execution loops
  max_iterations: 30
  
  # Maximum LLM calls per RLM task
  max_llm_calls: 50
  
  # Maximum output characters before truncation
  max_output_chars: 10000
  
  # Character threshold for stdout summarization
  stdout_summary_threshold: 10000
  
  # Length of prefix shown in stdout summaries
  stdout_summary_prefix_len: 200
  
  # Enable verbose logging for RLM execution
  verbose: false

# ==================== Router Settings ====================
router_settings:
  # Routing strategy options:
  # - "simple-shuffle": Random selection
  # - "least-busy": Route to model with lowest TPM usage
  # - "usage-based-routing": Route based on cost/usage
  # - "latency-based-routing": Route based on latency
  # - "rate-limit-aware-routing": Route considering rate limits
  routing_strategy: "simple-shuffle"

  # Enable fallbacks for failed requests
  num_retries: 3
  timeout: 600

# ==================== LiteLLM Module Settings ====================
litellm_settings:
  # Drop unsupported parameters instead of raising errors
  drop_params: True

  # Enable verbose logging for debugging
  # set_verbose: True

  # Callbacks for logging/observability
  # success_callback: ["langfuse"]
  # failure_callback: ["langfuse"]

  # Caching configuration (optional)
  # cache: True
  # cache_params:
  #   type: redis
  #   host: os.environ/REDIS_HOST
  #   port: os.environ/REDIS_PORT

# ==================== General Server Settings ====================
general_settings:
  # Master key for proxy authentication - clients must provide this in Authorization header
  # Using DSPY_LLM_API_KEY from .env file for consistency with your existing setup
  master_key: os.environ/DSPY_LLM_API_KEY

  # Database for storing logs/spend tracking (optional)
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>"
  # Alternatively set via environment: DATABASE_URL

  # Store model requests/responses in DB
  # store_model_in_db: True

  # Alerting configuration (optional)
  # alerting: ["slack"]
  # Set SLACK_WEBHOOK_URL in environment

  # Max budget per user (optional)
  # max_user_budget: 100.0

# ==================== Environment Variables ====================
# Required environment variables for this configuration:
#
# DSPY_LLM_API_KEY       - Master key for proxy authentication (from your .env)
#                          Clients must send: Authorization: Bearer $DSPY_LLM_API_KEY
#
# Provider API Keys (the proxy needs these to call underlying providers):
# DEEPINFRA_API_KEY      - API key for Deepinfra models
# GEMINI_API_KEY         - API key for Gemini models
# VERTEX_PROJECT_ID      - Google Cloud project ID for Vertex AI
# VERTEX_LOCATION        - Google Cloud location for Vertex AI (e.g., us-central1)
# NVIDIA_NIM_API_KEY     - API key for Nvidia NIM models
#
# Optional environment variables:
#
# DATABASE_URL           - PostgreSQL connection string for logging
# REDIS_HOST/REDIS_PORT  - For caching
# LANGFUSE_PUBLIC_KEY    - For Langfuse logging
# LANGFUSE_SECRET_KEY    - For Langfuse logging
# SLACK_WEBHOOK_URL      - For Slack alerts
